<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Docker 本地部署Hadoop、Spark、Zookeeper、Hive集群</title>
      <link href="/2024/01/08/Docker%20%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2Hadoop%EF%BC%8CSpark%EF%BC%8CZookeeper%EF%BC%8CHive%E9%9B%86%E7%BE%A4/"/>
      <url>/2024/01/08/Docker%20%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2Hadoop%EF%BC%8CSpark%EF%BC%8CZookeeper%EF%BC%8CHive%E9%9B%86%E7%BE%A4/</url>
      
        <content type="html"><![CDATA[<h2 id="Mysql、Redis、Tdengine-省略…-直接docker容器拉取"><a href="#Mysql、Redis、Tdengine-省略…-直接docker容器拉取" class="headerlink" title="Mysql、Redis、Tdengine 省略… 直接docker容器拉取"></a>Mysql、Redis、Tdengine 省略… 直接docker容器拉取</h2><h2 id="拉取各节点centos系统镜像"><a href="#拉取各节点centos系统镜像" class="headerlink" title="拉取各节点centos系统镜像"></a>拉取各节点centos系统镜像</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 设置docker网桥，用于分配固定IP</span></span></span><br><span class="line">docker network create --subnet=172.15.0.0/16 netgroup</span><br><span class="line"></span><br><span class="line">docker  pull  centos</span><br><span class="line"></span><br><span class="line">docker run -d --privileged -ti -v D:\WorkStation\Bigdatadevelop\BigdataTools\master:/opt --name cluster-master -h cluster-master --net netgroup --ip 172.15.0.2 centos:centos7 /usr/sbin/init</span><br><span class="line"></span><br><span class="line">docker run -d --privileged -ti -v D:\WorkStation\Bigdatadevelop\BigdataTools\slave1:/opt --name cluster-slave1-h cluster-slave1--net netgroup --ip 172.15.0.3 centos:centos7 /usr/sbin/init </span><br><span class="line"></span><br><span class="line">docker exec -ti 946556c3ae6c /bin/bash</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">改hostname</span></span><br><span class="line">vi /etc/hosts</span><br><span class="line">service network restart</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">改yum源</span></span><br><span class="line">mkdir bak</span><br><span class="line">mv * bak</span><br><span class="line">mv Centos-7.repo /etc/yum.repos.d/</span><br><span class="line">mv epel-7.repo /etc/yum.repos.d/</span><br><span class="line">yum clean all</span><br><span class="line">yum makecache</span><br><span class="line">yum repolist</span><br><span class="line"></span><br><span class="line">yum install httpd</span><br><span class="line">yum install net-tools</span><br><span class="line"></span><br><span class="line">yum -y install openssh-clients</span><br><span class="line">yum install openssh-server</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">免密配置</span></span><br><span class="line">systemctl status sshd</span><br><span class="line"></span><br><span class="line">passwd root</span><br><span class="line">ssh-keygen -t rsa</span><br><span class="line"></span><br><span class="line">ssh-copy-id  -f -i ~/.ssh/id_rsa.pub cluster-master</span><br><span class="line">ssh-copy-id  -f -i ~/.ssh/id_rsa.pub cluster-slave1</span><br><span class="line">ssh-copy-id  -f -i ~/.ssh/id_rsa.pub cluster-slave2</span><br><span class="line">ssh-copy-id  -f -i ~/.ssh/id_rsa.pub cluster-slave3</span><br><span class="line"></span><br><span class="line">Pzszh@062</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">容器保存成镜像</span></span><br><span class="line">docker commit -m &#x27;提交文字说明&#x27; -a &#x27;作者&#x27; 容器名 提交后的镜像名:提交后的镜像tag名</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">给需要推送的镜像打标签</span></span><br><span class="line">docker tag 镜像id 要推入的仓库的用户名/要推入的仓库名:新定义的tag</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">推送镜像到仓库</span></span><br><span class="line">docker push 要推入的仓库的用户名/要推入的仓库名:镜像标签</span><br></pre></td></tr></table></figure><h2 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">cd /opt</span><br><span class="line">tar -xzvf zookeeper-3.4.10.tar.gz</span><br><span class="line">mv zookeeper-3.4.10 /usr/local/zookeeper3.4.10</span><br><span class="line"> </span><br><span class="line">mkdir /usr/local/opt/zookeeper3.4.10/data        # 创建data目录</span><br><span class="line">mkdir /usr/local/opt/zookeeper3.4.10/dataLog     # 创建dataLog目录</span><br><span class="line"></span><br><span class="line">cd /opt/zookeeper3.4.10/data</span><br><span class="line">vi myid     # 输入数字1，然后保存，第二个节点输入2，第三个节点输入3</span><br><span class="line"> </span><br><span class="line">cd /opt/zookeeper3.4.10/conf</span><br><span class="line">cp zoo_sample.cfg zoo.cfg</span><br><span class="line">vi zoo.cfg             #在文件末尾添加如下内容</span><br><span class="line"></span><br><span class="line">dataDir=/opt/zookeeper3.4.10/data  </span><br><span class="line">dataLogDir=/opt/zookeeper3.4.10/dataLog  </span><br><span class="line">server.1=hadoop0:2888:3888  </span><br><span class="line">server.2=hadoop1:2888:3888  </span><br><span class="line">server.3=hadoop2:2888:3888  </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">注hadoop0，hadoop1，hadoop2为三个节点的主机名！</span></span><br><span class="line"></span><br><span class="line">cd /usr/local</span><br><span class="line">scp -r zookeeper3.4.10  root@hadoop1:/usr/local</span><br><span class="line">scp -r zookeeper3.4.10  root@hadoop2:/usr/local</span><br><span class="line"></span><br><span class="line">分别在三台服务器上运行如下命令</span><br><span class="line">zkServer.sh start</span><br></pre></td></tr></table></figure><h3 id="添加-Hadoop-用户"><a href="#添加-Hadoop-用户" class="headerlink" title="添加 Hadoop 用户"></a>添加 Hadoop 用户</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">useradd -m hadoop -s /bin/bash</span><br><span class="line">passwd hadoop</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">添加 root权限</span></span><br><span class="line">vi /etc/passwd </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">hadoop:X:0:1000::/home/hadoop:/bin/bash</span></span><br><span class="line">su - hadoop</span><br></pre></td></tr></table></figure><h2 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tar -zxf hadoop-3.1.3.tar.gz -C /usr/local/</span><br><span class="line">cd /usr/local/</span><br><span class="line">mv ./hadoop-3.1.3/ ./hadoop</span><br><span class="line">chown -R hadoop ./hadoop</span><br><span class="line"></span><br><span class="line">vi ~/.bashrc</span><br><span class="line">export PATH=$PATH:/usr/local/hadoop/bin:/usr/local/hadoop/sbin</span><br></pre></td></tr></table></figure><p>在配置集群&#x2F;分布式模式时，需要修改“&#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;etc&#x2F;hadoop”目录下的配置文件，这里仅设置正常启动所必须的设置项，包括workers 、core-site.xml、hdfs-site.xml、mapred-site.xml、yarn-site.xml共5个文件，更多设置项可查看官方说明。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">## workers</span><br><span class="line">需要把所有数据节点的主机名写入该文件，每行一个，默认为 localhost（即把本机作为数据节点），在进行分布式配置时，可以保留localhost，让Master节点同时充当名称节点和数据节点，或者也可以删掉localhost这行，让Master节点仅作为名称节点使用。</span><br><span class="line">localhost</span><br><span class="line">cluster-master</span><br><span class="line">cluster-slave1</span><br><span class="line">cluster-slave2</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># core-site.xml</span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://cluster-master:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/usr/local/hadoop/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">description</span>&gt;</span>Abase for other temporary directories.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">## hdfs-site.xml</span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>cluster-master:50090<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>13/value&gt;</span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/usr/local/hadoop/tmp/dfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/usr/local/hadoop/tmp/dfs/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">## mapred-site.xml</span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>cluster-master:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>cluster-master:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.app.mapreduce.am.env<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>HADOOP_MAPRED_HOME=/usr/local/hadoop<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.env<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>HADOOP_MAPRED_HOME=/usr/local/hadoop<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.env<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>HADOOP_MAPRED_HOME=/usr/local/hadoop<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">## yarn-site.xml</span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>cluster-master<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">master</span></span><br><span class="line">cd /usr/local</span><br><span class="line">rm -r ./hadoop/tmp # 删除 Hadoop 临时文件</span><br><span class="line">rm -r ./hadoop/logs/* # 删除日志文件</span><br><span class="line">tar -zcf ~/hadoop.master.tar.gz ./hadoop # 先压缩再复制</span><br><span class="line">cd ~</span><br><span class="line">scp ./hadoop.master.tar.gz Slave1:/home/hadoop</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">slave</span></span><br><span class="line">rm -r /usr/local/hadoop # 删掉旧的（如果存在）</span><br><span class="line">tar -zxf ~/hadoop.master.tar.gz -C /usr/local</span><br><span class="line">chown -R hadoop /usr/local/hadoop</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 切换 hadoop用户</span><br><span class="line"># 首次启动Hadoop集群时，需要先在Master节点执行名称节点的格式化（只需要执行这一次，后面再启动Hadoop时，不要再次格式化名称节点），命令如下：</span><br><span class="line">hdfs namenode -format</span><br><span class="line"></span><br><span class="line"># 启动Hadoop，启动需要在Master节点上进行，执行如下命令：</span><br><span class="line">start-dfs.sh</span><br><span class="line">start-yarn.sh </span><br><span class="line">mr-jobhistory-daemon.sh start historyserver</span><br></pre></td></tr></table></figure><h2 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tar -zxf ~/下载/spark-2.4.0-bin-without-hadoop.tgz -C /usr/local/</span><br><span class="line">mv ./spark-2.4.0-bin-without-hadoop/ ./spark</span><br><span class="line">chown -R hadoop:hadoop ./spark</span><br><span class="line"></span><br><span class="line">cd /usr/local/spark</span><br><span class="line">cp ./conf/spark-env.sh.template ./conf/spark-env.sh</span><br><span class="line"></span><br><span class="line">vi ./conf/spark-env.sh</span><br><span class="line">export SPARK_DIST_CLASSPATH=$(/usr/local/hadoop/bin/hadoop classpath)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">SPARK_LOCAL_DIRS=/usr/local/spark/</span><br><span class="line">HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop</span><br><span class="line">YARN_CONF_DIR=//usr/local/hadoop/etc/hadoop</span><br><span class="line">JAVA_HOME=/usr/local/java/jdk1.8.0_162</span><br><span class="line">export SPARK_MASTER_IP=cluster-master</span><br><span class="line">export SPARK_DAEMON_JAVA_OPTS=&quot;</span><br><span class="line">-Dspark.deploy.recoveryMode=ZOOKEEPER</span><br><span class="line">-Dspark.deploy.zookeeper.url=172.15.0.2:2181</span><br><span class="line">-Dspark.deploy.zookeeper.dir=/sparkmaster&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="slaves"><a href="#slaves" class="headerlink" title="slaves"></a>slaves</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cluster-slave1 </span><br><span class="line">cluster-slave2</span><br></pre></td></tr></table></figure><h3 id="spark-default-conf"><a href="#spark-default-conf" class="headerlink" title="spark-default.conf"></a>spark-default.conf</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">spark.eventLog.enabled          true</span><br><span class="line">spark.eventLog.dir              hdfs://jinbill/spark/eventLog</span><br><span class="line">spark.history.fs.logDirectory   hdfs://jinbill/spark/eventLog</span><br><span class="line">spark.eventLog.compress         true</span><br></pre></td></tr></table></figure><h2 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf apache-hive-3.1.2-bin.tar.gz</span><br><span class="line">mv apache-hive-3.1.2-bin /usr/local/hive</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">新建一个日志目录</span></span><br><span class="line">mkdir /usr/local/hive/iotmp</span><br></pre></td></tr></table></figure><h3 id="设置环境变量"><a href="#设置环境变量" class="headerlink" title="设置环境变量"></a>设置环境变量</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">vi ~/.bashrc</span><br><span class="line"></span><br><span class="line">export HIVE_HOME=/usr/local/hive</span><br><span class="line">export PATH=$HIVE_HOME/bin:$PATH</span><br><span class="line"></span><br><span class="line">source ~/.bashrc</span><br></pre></td></tr></table></figure><h3 id="设置Hive-HDFS文件夹"><a href="#设置Hive-HDFS文件夹" class="headerlink" title="设置Hive HDFS文件夹"></a>设置Hive HDFS文件夹</h3><h3 id="解决guava库问题"><a href="#解决guava库问题" class="headerlink" title="解决guava库问题"></a>解决guava库问题</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看hadoop下guava 版本</span></span><br><span class="line">cd /usr/local/hadoop/share/hadoop/common/lib/</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">guava-27.0-jre.jar</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看hive下guava 版本</span></span><br><span class="line">cd /usr/local/hive/lib</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">guava-19.0.jar</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">高版本替换低版本</span></span><br></pre></td></tr></table></figure><h3 id="MySQL-中-建立hive数据库"><a href="#MySQL-中-建立hive数据库" class="headerlink" title="MySQL 中 建立hive数据库"></a>MySQL 中 建立hive数据库</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> DATABASE hive;</span><br></pre></td></tr></table></figure><h3 id="jdbc依赖导入"><a href="#jdbc依赖导入" class="headerlink" title="jdbc依赖导入"></a>jdbc依赖导入</h3><h4 id="将hive的jline包替换到hadoop的yarn下"><a href="#将hive的jline包替换到hadoop的yarn下" class="headerlink" title="将hive的jline包替换到hadoop的yarn下"></a>将hive的jline包替换到hadoop的yarn下</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">`mv /opt/hive/apache-hive-3.1.2-bin/lib/jline-2.12.jar /opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/`</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">JDBC 依赖放入 /usr/local/hive/lib</span></span><br><span class="line">mv mysql-connector-java-5.1.47.jar /usr/local/hive/lib/</span><br></pre></td></tr></table></figure><h3 id="修改master节点配置文件"><a href="#修改master节点配置文件" class="headerlink" title="修改master节点配置文件"></a>修改master节点配置文件</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#### VI编辑器替换命令</span><br><span class="line">:%s/$&#123;system:java.io.tmpdir&#125;/\/opt\/hive\/iotmp/g  </span><br><span class="line">:%s/$&#123;system:user.name&#125;/huan/g</span><br></pre></td></tr></table></figure><h4 id="使用mysql替换默认的derby存放元数据"><a href="#使用mysql替换默认的derby存放元数据" class="headerlink" title="使用mysql替换默认的derby存放元数据"></a>使用mysql替换默认的derby存放元数据</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--元数据库修改为MySQL--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.db.type<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">      Expects one of [derby, oracle, mysql, mssql, postgres].</span><br><span class="line">      Type of database used by the metastore. Information schema <span class="symbol">&amp;amp;</span> JDBCStorageHandler depend on it.</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--MySQL 驱动--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Driver class name for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--MySQL URL--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://10.20.89.80:3306/hive?createDatabaseIfNotExist=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">      JDBC connect string for a JDBC metastore.</span><br><span class="line">      To use SSL to encrypt/authenticate the connection, provide database-specific SSL flag in the connection URL.</span><br><span class="line">      For example, jdbc:postgresql://myhost/db?ssl=true for postgres database.</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--MySQL 用户名--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Username to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--MySQL 密码--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>123456<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>password to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="设置解析引擎为spark"><a href="#设置解析引擎为spark" class="headerlink" title="设置解析引擎为spark"></a>设置解析引擎为spark</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.execution.engine<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>spark<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">      Expects one of [mr, tez, spark].</span><br><span class="line">      Chooses execution engine. Options are: mr (Map reduce, default), tez, spark. While MR</span><br><span class="line">      remains the default engine for historical reasons, it is itself a historical engine</span><br><span class="line">      and is deprecated in Hive 2 line. It may be removed without further warning.</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="自动初始化元数据"><a href="#自动初始化元数据" class="headerlink" title="自动初始化元数据"></a>自动初始化元数据</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>datanucleus.schema.autoCreateAll<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Auto creates necessary schema on a startup if one doesn&#x27;t exist. Set this to false, after creating it once.To enable auto create also set hive.metastore.schema.verification=false. Auto creation is not recommended for production use cases, run schematool command instead.</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="关闭校验"><a href="#关闭校验" class="headerlink" title="关闭校验"></a>关闭校验</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--听说是JDK版本使用1.8的问题。。--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.schema.verification<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">     Enforce metastore schema version consistency.</span><br><span class="line">     True: Verify that version information stored in is compatible with one from Hive jars.  Also disable automatic</span><br><span class="line">           schema migration attempt. Users are required to manually migrate schema after Hive upgrade which ensures</span><br><span class="line">           proper metastore schema migration. (Default)</span><br><span class="line">     False: Warn if the version information stored in metastore doesn&#x27;t match with one from in Hive jars.</span><br><span class="line">   <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.conf.validation<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">description</span>&gt;</span>Enables type checking for registered Hive configurations<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="删除-description-中的-8，这个解析会报错"><a href="#删除-description-中的-8，这个解析会报错" class="headerlink" title="删除 description 中的 &amp;#8，这个解析会报错"></a>删除 description 中的 <code>&amp;#8</code>，这个解析会报错</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.txn.xlock.iow<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">     Ensures commands with OVERWRITE (such as INSERT OVERWRITE) acquire Exclusive locks for<span class="symbol">&amp;#8;</span>transactional tables.  This ensures that inserts (w/o overwrite) running concurrently</span><br><span class="line">     are not hidden by the INSERT OVERWRITE.</span><br><span class="line">   <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="Spark任务提交"><a href="#Spark任务提交" class="headerlink" title="Spark任务提交"></a>Spark任务提交</h2><p>打包python依赖，镜像<br>进入到虚拟环境下，如&#x2F;home&#x2F;hadoop&#x2F;anaconda3&#x2F;envs，使用以下命令将虚拟环境进行打包：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zip -r conda_env.zip conda_env # 虚拟环境为conda_env, 打包为conda_env.zip 文件</span><br></pre></td></tr></table></figure><h4 id="spark-submit参数设置"><a href="#spark-submit参数设置" class="headerlink" title="spark-submit参数设置"></a>spark-submit参数设置</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">spark_conf = SparkConf().loadPropertiesFile(<span class="string">&quot;spark_config.properties&quot;</span>)</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder \</span><br><span class="line">    .appName(spark_conf.get(<span class="string">&quot;spark.app.name&quot;</span>)) \</span><br><span class="line">    .master(spark_conf.get(<span class="string">&quot;spark.master&quot;</span>)) \</span><br><span class="line">    .config(<span class="string">&quot;spark.executor.memory&quot;</span>, spark_conf.get(<span class="string">&quot;spark.executor.memory&quot;</span>)) \</span><br><span class="line">    .config(<span class="string">&quot;spark.driver.memory&quot;</span>, spark_conf.get(<span class="string">&quot;spark.driver.memory&quot;</span>)) \</span><br><span class="line">    .getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用SparkSession进行数据处理和分析</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$&#123;SPARK_PATH&#125;</span>/bin/spark-submit \</span><br><span class="line"> --master yarn \</span><br><span class="line"> --name <span class="string">&quot;spark_demo_lr&quot;</span> \</span><br><span class="line"> --queue <span class="variable">$&#123;YARN_QUEUE&#125;</span> \</span><br><span class="line"> --deploy-mode <span class="variable">$&#123;DEPLOY_MODE&#125;</span> \</span><br><span class="line"> --driver-memory 6g \</span><br><span class="line"> --driver-cores 4 \</span><br><span class="line"> --executor-memory 12g \</span><br><span class="line"> --executor-cores 15 \</span><br><span class="line"> --num-executors 10 \</span><br><span class="line"> --archives ./source/py27.zip<span class="comment">#python_env \</span></span><br><span class="line"> --conf spark.default.parallelism=150 \</span><br><span class="line"> --conf spark.executor.memoryOverhead=4g \</span><br><span class="line"> --conf spark.driver.memoryOverhead=2g \</span><br><span class="line"> --conf spark.yarn.maxAppAttempts=3 \</span><br><span class="line"> --conf spark.yarn.submit.waitAppCompletion=<span class="literal">true</span> \</span><br><span class="line"> --conf spark.pyspark.driver.python=./source/py27/bin/python2 \</span><br><span class="line"> --conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=./python_env/py27/bin/python2 \</span><br><span class="line"> --conf spark.pyspark.python=./python_env/py27/bin/python2 \</span><br><span class="line"> ./<span class="variable">$&#123;ModelType&#125;</span>.py <span class="variable">$input_path_train</span> <span class="variable">$input_path_test</span> <span class="variable">$output_path</span></span><br></pre></td></tr></table></figure><h4 id="pyspark-传入配置文件参数"><a href="#pyspark-传入配置文件参数" class="headerlink" title="pyspark 传入配置文件参数"></a>pyspark 传入配置文件参数</h4><p>首先，可以使用 <code>--files</code> 参数将配置文件传递给 <code>spark-submit</code> 命令。这将确保配置文件在集群中的每个节点上都可用。以下是一个示例：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-submit --master yarn --deploy-mode cluster --files /home/sys_user/ask/conf/config.ini test.py</span><br></pre></td></tr></table></figure><p>在 PySpark 脚本中，可以使用 <code>SparkFiles.get()</code> 方法来读取传入的配置文件</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkFiles</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(SparkFiles.get(<span class="string">&#x27;config.ini&#x27;</span>)) <span class="keyword">as</span> config_file:</span><br><span class="line">    <span class="built_in">print</span>(config_file.read())</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 SparkSession</span></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">&quot;ConfigExample&quot;</span>).getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载配置参数</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;config.properties&quot;</span>, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">        line = line.strip()</span><br><span class="line">        <span class="keyword">if</span> line <span class="keyword">and</span> <span class="keyword">not</span> line.startswith(<span class="string">&quot;#&quot;</span>):</span><br><span class="line">            key, value = line.split(<span class="string">&quot;=&quot;</span>)</span><br><span class="line">            spark.conf.<span class="built_in">set</span>(key, value)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印配置参数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;spark.sql.shuffle.partitions:&quot;</span>, spark.conf.get(<span class="string">&quot;spark.sql.shuffle.partitions&quot;</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;spark.sql.autoBroadcastJoinThreshold:&quot;</span>, spark.conf.get(<span class="string">&quot;spark.sql.autoBroadcastJoinThreshold&quot;</span>))</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./spark-submit --master yarn --deploy-mode cluster --files /opt/bigdata/config_batch.properties --executor-memory 2g --executor-cores 1 --num-executors 5 --driver-memory 2G  --py-files /usr/local/miniconda3/envs/spark_env.zip --jars /usr/local/spark/jars/mysql-connector-java-5.1.47.jar,/usr/local/spark/jars/taos-jdbcdriver-2.0.34.jar,/usr/local/spark/jars/spark-redis-2.4.0-jar-with-dependencies.jar,/usr/local/spark/jars/fastjson-1.2.73.jar --conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=/usr/local/miniconda3/envs/spark_env/bin/python --conf spark.pyspark.python=/usr/local/miniconda3/envs/spark_env/bin/python /opt/bigdata/HealthScoreHFL2.py</span><br></pre></td></tr></table></figure><p>        .master(“local”)<br>        .enableHiveSupport()<br>        .config(‘spark.executor.extraClassPath’, “&#x2F;usr&#x2F;local&#x2F;spark&#x2F;jars&#x2F;mysql-connector-java-5.1.47.jar”)\</p><p>        .config(‘spark.driver.extraClassPath’, “&#x2F;usr&#x2F;local&#x2F;spark&#x2F;jars&#x2F;mysql-connector-java-5.1.47.jar”)\</p><p>        .config(“spark.jars”, “&#x2F;usr&#x2F;local&#x2F;spark&#x2F;jars&#x2F;mysql-connector-java-5.1.47.jar,&#x2F;usr&#x2F;local&#x2F;spark&#x2F;jars&#x2F;taos-jdbcdriver-2.0.34.jar,&#x2F;usr&#x2F;local&#x2F;spark&#x2F;jars&#x2F;spark-redis-2.4.0-jar-with-dependencies.jar,&#x2F;usr&#x2F;local&#x2F;spark&#x2F;jars&#x2F;fastjson-1.2.73.jar”)\</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./spark-submit --master yarn --deploy-mode cluster --files /opt/bigdata/batch/config_batch.properties --executor-memory 2g --executor-cores 1 --num-executors 5 --driver-memory 2G  --py-files /opt/bigdata/batch/spark3.1_env.zip --jars /usr/local/spark-yarn/jars/mysql-connector-java-5.1.47.jar,/usr/local/spark-yarn/jars/taos-jdbcdriver-2.0.34.jar,/usr/local/spark-yarn/jars/spark-redis-2.4.0-jar-with-dependencies.jar,/usr/local/spark-yarn/jars/fastjson-1.2.73.jar --conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=/usr/local/miniconda3/bin/python --conf spark.pyspark.python=/usr/local/miniconda3/bin/python /opt/bigdata/batch/HealthScoreHFL2.py</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> bigdata </category>
          
          <category> docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> bigdata </tag>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2024/01/03/hello-world/"/>
      <url>/2024/01/03/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
