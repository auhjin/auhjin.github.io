<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Linux 调整根目录和home目录的空间大小</title>
      <link href="/2024/01/15/Linux%E4%B8%8B%E8%B0%83%E6%95%B4%E6%A0%B9%E7%9B%AE%E5%BD%95%E5%92%8Chome%E7%9B%AE%E5%BD%95%E7%9A%84%E7%A9%BA%E9%97%B4%E5%A4%A7%E5%B0%8F/"/>
      <url>/2024/01/15/Linux%E4%B8%8B%E8%B0%83%E6%95%B4%E6%A0%B9%E7%9B%AE%E5%BD%95%E5%92%8Chome%E7%9B%AE%E5%BD%95%E7%9A%84%E7%A9%BA%E9%97%B4%E5%A4%A7%E5%B0%8F/</url>
      
        <content type="html"><![CDATA[<p>当安装完 Linux 操作系统，发现磁盘分区大小错误，或者后期使用过程发现 &#x2F;home 还剩余很多空间，&#x2F; 下空间不足，需要将 &#x2F;home 下空间重新分配给 &#x2F;目录下<br>1、查看分区空间和格式</p><pre><code>root@mongodb~# df -hT  文件系统 类型 容量 已用 可用 已用% 挂载点  /dev/mapper/centos-root xfs 50G 49G 1.4G 98% /  devtmpfs devtmpfs 5.8G 0 5.8G 0% /dev  tmpfs tmpfs 5.8G 0 5.8G 0% /dev/shm  tmpfs tmpfs 5.8G 602M 5.3G 11% /run  tmpfs tmpfs 5.8G 0 5.8G 0% /sys/fs/cgroup  /dev/sda1 xfs 1014M 153M 862M 16% /boot  /dev/mapper/centos-home xfs 44G 36M 44G 1% /home  tmpfs tmpfs 1.2G 0 1.2G 0% /run/user/0</code></pre><p>这里要将 &#x2F;home 的空闲空间分给 &#x2F; 目录一部分</p><p>可以看到 &#x2F;home 分区是 xfs 格式</p><p>1）ext2&#x2F;ext3&#x2F;ext4文件系统的调整命令是resize2fs（增大和减小都支持）</p><pre><code>lvextend -L 120G /dev/mapper/centos-home //增大至120G  lvextend -L +20G /dev/mapper/centos-home //增加20G  lvreduce -L 50G /dev/mapper/centos-home //减小至50G  lvreduce -L -8G /dev/mapper/centos-home //减小8G  resize2fs /dev/mapper/centos-home //执行调整</code></pre><p>2）xfs文件系统的调整命令是xfs_growfs（只支持增大）</p><pre><code>lvextend -L 120G /dev/mapper/centos-home //增大至120G  lvextend -L +20G /dev/mapper/centos-home //增加20G  xfs_growfs /dev/mapper/centos-home //执行调整</code></pre><p>xfs文件系统只支持增大分区空间的情况，不支持减小的情况。</p><p>硬要减小的话，只能在减小后将逻辑分区重新通过 mkfs.xfs 命令重新格式化才能挂载上，这样的话这个逻辑分区上原来的数据就丢失了。如果有重要文件，禁用或移除文件</p><p>2、卸载 &#x2F;home 分区</p><pre><code>[root@mongodb-1 /]# umount /home[root@mongodb-1 /]#   文件系统 容量 已用 可用 已用% 挂载点  /dev/mapper/centos-root 50G 49G 1.4G 98% /  devtmpfs 5.8G 0 5.8G 0% /dev  tmpfs 5.8G 0 5.8G 0% /dev/shm  tmpfs 5.8G 602M 5.3G 11% /run  tmpfs 5.8G 0 5.8G 0% /sys/fs/cgroup  /dev/sda1 1014M 153M 862M 16% /boot  tmpfs 1.2G 0 1.2G 0% /run/user/0</code></pre><p>卸载成功<br>3、将 &#x2F;home 分区减小200G（根据自己实际情况设定大小） ：</p><pre><code>[root@mongodb-1 /]# lvreduce -L -40G /dev/mapper/centos-home  WARNING: Reducing active logical volume to &lt; 3.12 GiB.  THIS MAY DESTROY YOUR DATA (filesystem etc.)  Do you really want to reduce centos/home? [y/n]: y  Size of logical volume centos/home changed from &lt;43.12 GiB (11038 extents) to &lt; 3.12 GiB (798 extents).  Logical volume centos/home successfully resized.</code></pre><p>因为 xfs文件系统不能执行分区减小的调整！所以这里我们要执行格式化操作，</p><pre><code>[root@mongodb-1 /]# mkfs.xfs /dev/mapper/centos-home -f  meta-data=/dev/mapper/centos-home isize=512 agcount=4, agsize=204288 blks  = sectsz=512 attr=2, projid32bit=1  = crc=1 finobt=0, sparse=0  data = bsize=4096 blocks=817152, imaxpct=25  = sunit=0 swidth=0 blks  naming =version 2 bsize=4096 ascii-ci=0 ftype=1  log =internal log bsize=4096 blocks=2560, version=2  = sectsz=512 sunit=0 blks, lazy-count=1  realtime =none extsz=4096 blocks=0, rtextents=0</code></pre><p>重新挂载 &#x2F;home 分区：</p><pre><code>mount /dev/mapper/centos-home /home/</code></pre><p>5、将上面空余的 200G 分到 &#x2F; 分区下</p><pre><code>[root@mongodb-1 /]# lvextend -L +40G /dev/mapper/centos-root  Size of logical volume centos/root changed from 50.00 GiB (12800 extents) to 90.00 GiB (23040 extents).  Logical volume centos/root successfully resized.  [root@mongodb-1 /]# xfs_growfs /dev/mapper/centos-root  meta-data=/dev/mapper/centos-root isize=512 agcount=4, agsize=3276800 blks  = sectsz=512 attr=2, projid32bit=1  = crc=1 finobt=0 spinodes=0  data = bsize=4096 blocks=13107200, imaxpct=25  = sunit=0 swidth=0 blks  naming =version 2 bsize=4096 ascii-ci=0 ftype=1  log =internal bsize=4096 blocks=6400, version=2  = sectsz=512 sunit=0 blks, lazy-count=1  realtime =none extsz=4096 blocks=0, rtextents=0  data blocks changed from 13107200 to 23592960</code></pre><pre><code>[root@mongodb-1 /]# df -h  文件系统 容量 已用 可用 已用% 挂载点  /dev/mapper/centos-root 90G 49G 42G 55% /  devtmpfs 5.8G 0 5.8G 0% /dev  tmpfs 5.8G 0 5.8G 0% /dev/shm  tmpfs 5.8G 602M 5.3G 11% /run  tmpfs 5.8G 0 5.8G 0% /sys/fs/cgroup  /dev/sda1 1014M 153M 862M 16% /boot  tmpfs 1.2G 0 1.2G 0% /run/user/0  /dev/mapper/centos-home 3.2G 33M 3.1G 2% /home</code></pre>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux 搭建离线yum源</title>
      <link href="/2024/01/15/%E6%90%AD%E5%BB%BA%E7%A6%BB%E7%BA%BFyum%E6%BA%90/"/>
      <url>/2024/01/15/%E6%90%AD%E5%BB%BA%E7%A6%BB%E7%BA%BFyum%E6%BA%90/</url>
      
        <content type="html"><![CDATA[<p>工作中，虚拟机通常都是部署在内网环境中，无法连接互联网，因此无法使用互联网上的YUM源。<br>经常会遇到系统ISO镜像中软件包缺失，系统软件补丁无法升级，第三方软件包无法安装等情况。</p><p>本文通过搭建离线YUM源，解决了上述问题。</p><h4 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h4><p>准备一台可以联网的linux机器<br>安装创建YUM仓库的软件工具</p><pre><code class="shell">[root@localhost yum.repos.d]# yum install createrepo yum-utils -y</code></pre><p>![[Pasted image 20230630202201.png]]<br>移除初始的其他镜像源</p><pre><code class="shell">[root@localhost ~]# cd /etc/yum.repos.d/  [root@localhost yum.repos.d]# mkdir bak  [root@localhost yum.repos.d]# mv * bak</code></pre><p>![[Pasted image 20230630202240.png]]<br>下载阿里云的repo文件</p><pre><code class="bash">[root@localhost ~]# wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo[root@localhost ~]# wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo</code></pre><pre><code>什么是yum源repo文件，repo文件是yum源（软件仓库）的配置文件，通常一个repo文件定义了一个或者多个软件仓库的配置内容，例如将从哪里下载需要安装或者升级的软件包，repo文件中的设置内容将被yum读取和应用。</code></pre><p>![[Pasted image 20230630203607.png]]<br>![[Pasted image 20230630202353.png]]<br>构建YUM源缓存</p><pre><code class="shell">[root@localhost ~]# yum clean all   #清除当前的yum源缓存(base) [root@localhost yum.repos.d]# yum install -y epel-release[root@localhost ~]#  yum makecache  #重新生成缓存[root@localhost ~]#  yum repolist   #查看当前可用的YUM源</code></pre><p>![[Pasted image 20230630202516.png]]<br>![[Pasted image 20230630205239.png]]<br>安装并启用httpd服务</p><pre><code class="shell">[root@localhost /]# yum install httpd[root@localhost /]# systemctl start httpd[root@localhost /]# systemctl status httpd● httpd.service - The Apache HTTP ServerLoaded: loaded (/usr/lib/systemd/system/httpd.service; disabled; vendor preset: disabled)Active: active (running) since Wed 2021-06-30 00:49:06 CST; 5s ago....output omitted....</code></pre><p>![[Pasted image 20230630203747.png]]<br>![[Pasted image 20230630203826.png]]</p><h4 id="同步阿里云的YUM源到本地"><a href="#同步阿里云的YUM源到本地" class="headerlink" title="同步阿里云的YUM源到本地"></a>同步阿里云的YUM源到本地</h4><pre><code class="shell"> #reposync根据之前下载的repo文件下载rpm包到指定文件夹,  #这个服务器home文件夹下的空间比较多，同步下载到 /home路径下，mkdir /phm/yum[root@localhost /]# cd /home/phm/yum[root@localhost yum]# reposync -r base [root@localhost yum]# reposync -r extras[root@localhost yum]# reposync -r updates[root@localhost yum]# reposync -r epel</code></pre><p>![[Pasted image 20230701115938.png]]<br>![[Pasted image 20230701120738.png]]</p><h4 id="创建本地YUM仓库"><a href="#创建本地YUM仓库" class="headerlink" title="创建本地YUM仓库"></a>创建本地YUM仓库</h4><pre><code class="shell"># 为本地yum仓库，生成新的repo文件[root@localhost yum]# cd /home/phm/yum/base[root@localhost yum]# createrepo ./[root@localhost yum]# cd /home/phm/yum/extras[root@localhost yum]# createrepo ./[root@localhost yum]# cd /home/phm/yum/updates[root@localhost yum]# createrepo ./[root@localhost yum]# cd /home/phm/yum/epel[root@localhost yum]# createrepo ./</code></pre><p>![[Pasted image 20230702202732.png]]<br>![[Pasted image 20230702202807.png]]</p><h4 id="更新YUM仓库"><a href="#更新YUM仓库" class="headerlink" title="更新YUM仓库"></a>更新YUM仓库</h4><p>到此本地YUM仓库搭建完成，<strong>如果在对应的仓库中加入新的软件包时</strong>，需要更新仓库。本文中不涉及。</p><pre><code class="shell">[root@localhost ]#  createrepo --update /var/www/html/yum/epel</code></pre><h4 id="离线YUM仓库的使用"><a href="#离线YUM仓库的使用" class="headerlink" title="离线YUM仓库的使用"></a>离线YUM仓库的使用</h4><p>将本机&#x2F;var&#x2F;www&#x2F;html&#x2F;yum目录通过sftp等客户端工具保存到移动硬盘，之后再上传到现有的yum源主机对应目录中。更新客户端主机的yum源配置，指向新的YUM仓库目录，即可正常使用该YUM仓库。配置维护域ip地址，确保与其他主机网络正常通信。配置客户端主机的yum源配置文件，将YUM仓库指向该主机，即可正常使用该YUM仓库。客户端yum源配置文件如下：</p><pre><code># 3种文件获取方式# 1. HTTPbaseurl=http://10.16.9.34/yum/base# 2. 本地文件获取baseurl=file:///home/yum/base# 3. FTPbaseurl=ftp://10.16.9.34/yum/base</code></pre><pre><code class="shell">[root@client yum.repos.d]# cd /etc/yum.repos.d[root@client yum.repos.d]# mkdir bak ; mv * bak[root@client yum.repos.d]# vi http.repo[base]name=RHEL- - Base - httpbaseurl=http://10.16.9.34/centos/yum/baseenabled=1gpgcheck=0[updates]name=RHEL- - updates - httpbaseurl=http://10.16.9.34/centos/yum/updatesenabled=1gpgcheck=0[epel]name=RHEL- - epel - httpbaseurl=http://10.16.9.34/centos/yum/epelenabled=1gpgcheck=0[extras]name=RHEL- - extras - httpbaseurl=http://10.16.9.34/centos/yum/extrasenabled=1gpgcheck=0[root@client yum.repos.d]# yum clean all[root@client yum.repos.d]# yum makecache[root@client yum.repos.d]# yum list | grep kernelkernel.x86_64                            3.10.0-1160.el7               @anacondakernel-tools.x86_64                      3.10.0-1160.el7               @anacondaabrt-addon-kerneloops.x86_64             2.1.11-60.el7.centos          basekernel-debug-devel.x86_64                3.10.0-1160.31.1.el7          updateskernel-devel.x86_64                      3.10.0-1160.31.1.el7          updates[root@client yum.repos.d]# yum install kernelLoaded plugins: fastestmirrorLoading mirror speeds from cached hostfileResolving Dependencies--&gt; Running transaction check---&gt; Package kernel.x86_64 0:3.10.0-1160.31.1.el7 will be installed--&gt; Finished Dependency Resolution.....output ommitted.........Transaction test succeededRunning transaction  Installing : kernel-3.10.0-1160.31.1.el7.x86_64                                       Complete!客户端测试安装kernel软件包，成功安装。</code></pre><p>![[Pasted image 20230712131018.png]]</p><h3 id="超融合主机磁盘I-O"><a href="#超融合主机磁盘I-O" class="headerlink" title="超融合主机磁盘I&#x2F;O"></a>超融合主机磁盘I&#x2F;O</h3><p>60G 的yum文件，从9:20 – 12:55<br>![[Pasted image 20230712131354.png]]</p>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python环境离线安装</title>
      <link href="/2024/01/10/Python%20%E7%8E%AF%E5%A2%83%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85/"/>
      <url>/2024/01/10/Python%20%E7%8E%AF%E5%A2%83%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85/</url>
      
        <content type="html"><![CDATA[<h1 id="pip-通过-requirements-文件，批量下载python包，批量离线安装python包"><a href="#pip-通过-requirements-文件，批量下载python包，批量离线安装python包" class="headerlink" title="pip 通过 requirements 文件，批量下载python包，批量离线安装python包"></a>pip 通过 requirements 文件，批量下载python包，批量离线安装python包</h1><h4 id="1、首先，在开发项目环境中分析出所有依赖的库"><a href="#1、首先，在开发项目环境中分析出所有依赖的库" class="headerlink" title="1、首先，在开发项目环境中分析出所有依赖的库"></a>1、首先，在开发项目环境中分析出所有依赖的库</h4><pre><code class="shell">pip freeze &gt; requirements.txt # 该方法仅可以使用在虚拟环境中，会将python 解释器下的所有包都导出pipreqs ./ --encoding=utf-8 --force # 表示覆盖该原有requirements.txt</code></pre><h4 id="2-在有网络的目标环境中，将所有包下载到DIR这个目录中"><a href="#2-在有网络的目标环境中，将所有包下载到DIR这个目录中" class="headerlink" title="2. 在有网络的目标环境中，将所有包下载到DIR这个目录中"></a>2. 在有网络的目标环境中，将所有包下载到DIR这个目录中</h4><p>切记，不要在 windows 下载包，然后放到 Linux 上进行安装，这样基本装不上</p><pre><code class="shell">pip download -d DIR -r requirements.txtpip wheel -w DIR -r requirements.txt</code></pre><ul><li>这两条命令的区别在于wheel 方式下载会将下载的包放入wheel 缓存，但缺点是wheel 不可以下载源码包</li><li>download 命令会查看wheel缓存，然后再去PyPI下载库，但download命令下载的包不会进入wheel缓存，download 的优点是可以下载源码包</li><li>需要注意，使用wheel 方式安装可能会报错，因为有些包是源码包，不能被打包成wheel 格式</li><li>download 方法下载的包，不会对依赖性进行检查，这意味着，如果下载 Flask-PyMongo 这个包，只会下载该包，而不会下载 pymongo，经试验发现，download 适合补充wheel不可下载的包，两者搭配使用，才能将requirements文件的库完整的下载</li></ul><h4 id="3-将文件打包后放到离线服务器上，并进行解压缩"><a href="#3-将文件打包后放到离线服务器上，并进行解压缩" class="headerlink" title="3. 将文件打包后放到离线服务器上，并进行解压缩"></a>3. 将文件打包后放到离线服务器上，并进行解压缩</h4><pre><code>pip install --no-index --find-links=DIR -r requirements.txt</code></pre><p>命令说明</p><ul><li>freeze 将依赖关系分析出来并 使用管道符导入到该文件中</li><li>download 分析 requirements 文件，将所有包进行下载，通过 d 选项导入 DIR 文件夹</li><li>wheel 分析requirements 文件，并将所有包及其依赖包下载为 wheel 格式，通过 w 选项导入 DIR 文件夹中</li><li>–find-links 指定离线安装的文件夹DIR，也就是你下载好的包</li><li>注意: –no-index 必须搭配 –find-links 使用</li></ul><h2 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h2><ol><li>安装时报错，不能找到相应的包，打开文件夹后却发现有这个库<ol><li>原因可能是该库是3.10 版本，而服务器是3.9 版本，导致安装不上</li><li>使用以下命令选择合适的版本，注意 <code>--no-deps</code> 不可缺少，</li></ol><pre><code>pip download  --no-deps--platform linux_x86_64--python-version 36--implementation cp  --abi cp36m     -r requirements.txt -d pk</code></pre></li></ol><pre><code></code></pre>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Docker 本地部署Hadoop、Spark、Zookeeper、Hive集群</title>
      <link href="/2024/01/08/Docker%20%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2Hadoop%EF%BC%8CSpark%EF%BC%8CZookeeper%EF%BC%8CHive%E9%9B%86%E7%BE%A4/"/>
      <url>/2024/01/08/Docker%20%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2Hadoop%EF%BC%8CSpark%EF%BC%8CZookeeper%EF%BC%8CHive%E9%9B%86%E7%BE%A4/</url>
      
        <content type="html"><![CDATA[<h2 id="Mysql、Redis、Tdengine-省略…-直接docker容器拉取"><a href="#Mysql、Redis、Tdengine-省略…-直接docker容器拉取" class="headerlink" title="Mysql、Redis、Tdengine 省略… 直接docker容器拉取"></a>Mysql、Redis、Tdengine 省略… 直接docker容器拉取</h2><h2 id="拉取各节点centos系统镜像"><a href="#拉取各节点centos系统镜像" class="headerlink" title="拉取各节点centos系统镜像"></a>拉取各节点centos系统镜像</h2><pre><code class="shell">## 设置docker网桥，用于分配固定IPdocker network create --subnet=172.15.0.0/16 netgroupdocker  pull  centosdocker run -d --privileged -ti -v D:\WorkStation\Bigdatadevelop\BigdataTools\master:/opt --name cluster-master -h cluster-master --net netgroup --ip 172.15.0.2 centos:centos7 /usr/sbin/initdocker run -d --privileged -ti -v D:\WorkStation\Bigdatadevelop\BigdataTools\slave1:/opt --name cluster-slave1-h cluster-slave1--net netgroup --ip 172.15.0.3 centos:centos7 /usr/sbin/init docker exec -ti 946556c3ae6c /bin/bash#改hostnamevi /etc/hostsservice network restart# 改yum源mkdir bakmv * bakmv Centos-7.repo /etc/yum.repos.d/mv epel-7.repo /etc/yum.repos.d/yum clean allyum makecacheyum repolistyum install httpdyum install net-toolsyum -y install openssh-clientsyum install openssh-server#免密配置systemctl status sshdpasswd rootssh-keygen -t rsassh-copy-id  -f -i ~/.ssh/id_rsa.pub cluster-masterssh-copy-id  -f -i ~/.ssh/id_rsa.pub cluster-slave1ssh-copy-id  -f -i ~/.ssh/id_rsa.pub cluster-slave2ssh-copy-id  -f -i ~/.ssh/id_rsa.pub cluster-slave3Pzszh@062# 容器保存成镜像docker commit -m &#39;提交文字说明&#39; -a &#39;作者&#39; 容器名 提交后的镜像名:提交后的镜像tag名# 给需要推送的镜像打标签docker tag 镜像id 要推入的仓库的用户名/要推入的仓库名:新定义的tag# 推送镜像到仓库docker push 要推入的仓库的用户名/要推入的仓库名:镜像标签</code></pre><h2 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h2><pre><code class="shell">cd /opttar -xzvf zookeeper-3.4.10.tar.gzmv zookeeper-3.4.10 /usr/local/zookeeper3.4.10 mkdir /usr/local/opt/zookeeper3.4.10/data        # 创建data目录mkdir /usr/local/opt/zookeeper3.4.10/dataLog     # 创建dataLog目录cd /opt/zookeeper3.4.10/datavi myid     # 输入数字1，然后保存，第二个节点输入2，第三个节点输入3 cd /opt/zookeeper3.4.10/confcp zoo_sample.cfg zoo.cfgvi zoo.cfg             #在文件末尾添加如下内容dataDir=/opt/zookeeper3.4.10/data  dataLogDir=/opt/zookeeper3.4.10/dataLog  server.1=hadoop0:2888:3888  server.2=hadoop1:2888:3888  server.3=hadoop2:2888:3888  # 注hadoop0，hadoop1，hadoop2为三个节点的主机名！cd /usr/localscp -r zookeeper3.4.10  root@hadoop1:/usr/localscp -r zookeeper3.4.10  root@hadoop2:/usr/local分别在三台服务器上运行如下命令zkServer.sh start</code></pre><h3 id="添加-Hadoop-用户"><a href="#添加-Hadoop-用户" class="headerlink" title="添加 Hadoop 用户"></a>添加 Hadoop 用户</h3><pre><code class="shell">useradd -m hadoop -s /bin/bashpasswd hadoop# 添加 root权限vi /etc/passwd # hadoop:X:0:1000::/home/hadoop:/bin/bashsu - hadoop</code></pre><h2 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h2><pre><code class="shell">tar -zxf hadoop-3.1.3.tar.gz -C /usr/local/cd /usr/local/mv ./hadoop-3.1.3/ ./hadoopchown -R hadoop ./hadoopvi ~/.bashrcexport PATH=$PATH:/usr/local/hadoop/bin:/usr/local/hadoop/sbin</code></pre><p>在配置集群&#x2F;分布式模式时，需要修改“&#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;etc&#x2F;hadoop”目录下的配置文件，这里仅设置正常启动所必须的设置项，包括workers 、core-site.xml、hdfs-site.xml、mapred-site.xml、yarn-site.xml共5个文件，更多设置项可查看官方说明。</p><pre><code>## workers需要把所有数据节点的主机名写入该文件，每行一个，默认为 localhost（即把本机作为数据节点），在进行分布式配置时，可以保留localhost，让Master节点同时充当名称节点和数据节点，或者也可以删掉localhost这行，让Master节点仅作为名称节点使用。localhostcluster-mastercluster-slave1cluster-slave2</code></pre><pre><code class="xml"># core-site.xml&lt;configuration&gt;        &lt;property&gt;                &lt;name&gt;fs.defaultFS&lt;/name&gt;                &lt;value&gt;hdfs://cluster-master:9000&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;                &lt;value&gt;file:/usr/local/hadoop/tmp&lt;/value&gt;                &lt;description&gt;Abase for other temporary directories.&lt;/description&gt;        &lt;/property&gt;&lt;/configuration&gt;</code></pre><pre><code class="xml">## hdfs-site.xml&lt;configuration&gt;        &lt;property&gt;                &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;                &lt;value&gt;cluster-master:50090&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;dfs.replication&lt;/name&gt;                &lt;value&gt;13/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;                &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/name&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;                &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/data&lt;/value&gt;        &lt;/property&gt;&lt;/configuration&gt;</code></pre><pre><code class="xml">## mapred-site.xml&lt;configuration&gt;        &lt;property&gt;                &lt;name&gt;mapreduce.framework.name&lt;/name&gt;                &lt;value&gt;yarn&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;                &lt;value&gt;cluster-master:10020&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;                &lt;value&gt;cluster-master:19888&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt;                &lt;value&gt;HADOOP_MAPRED_HOME=/usr/local/hadoop&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;mapreduce.map.env&lt;/name&gt;                &lt;value&gt;HADOOP_MAPRED_HOME=/usr/local/hadoop&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;mapreduce.reduce.env&lt;/name&gt;                &lt;value&gt;HADOOP_MAPRED_HOME=/usr/local/hadoop&lt;/value&gt;        &lt;/property&gt; &lt;/configuration&gt;</code></pre><pre><code class="xml">## yarn-site.xml&lt;configuration&gt;        &lt;property&gt;                &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;                &lt;value&gt;cluster-master&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;                &lt;value&gt;mapreduce_shuffle&lt;/value&gt;        &lt;/property&gt;&lt;/configuration&gt;</code></pre><pre><code class="shell"># mastercd /usr/localrm -r ./hadoop/tmp # 删除 Hadoop 临时文件rm -r ./hadoop/logs/* # 删除日志文件tar -zcf ~/hadoop.master.tar.gz ./hadoop # 先压缩再复制cd ~scp ./hadoop.master.tar.gz Slave1:/home/hadoop</code></pre><pre><code class="shell"># slaverm -r /usr/local/hadoop # 删掉旧的（如果存在）tar -zxf ~/hadoop.master.tar.gz -C /usr/localchown -R hadoop /usr/local/hadoop</code></pre><pre><code># 切换 hadoop用户# 首次启动Hadoop集群时，需要先在Master节点执行名称节点的格式化（只需要执行这一次，后面再启动Hadoop时，不要再次格式化名称节点），命令如下：hdfs namenode -format# 启动Hadoop，启动需要在Master节点上进行，执行如下命令：start-dfs.shstart-yarn.sh mr-jobhistory-daemon.sh start historyserver</code></pre><h2 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h2><pre><code class="shell">tar -zxf ~/下载/spark-2.4.0-bin-without-hadoop.tgz -C /usr/local/mv ./spark-2.4.0-bin-without-hadoop/ ./sparkchown -R hadoop:hadoop ./sparkcd /usr/local/sparkcp ./conf/spark-env.sh.template ./conf/spark-env.shvi ./conf/spark-env.shexport SPARK_DIST_CLASSPATH=$(/usr/local/hadoop/bin/hadoop classpath)</code></pre><pre><code>SPARK_LOCAL_DIRS=/usr/local/spark/HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoopYARN_CONF_DIR=//usr/local/hadoop/etc/hadoopJAVA_HOME=/usr/local/java/jdk1.8.0_162export SPARK_MASTER_IP=cluster-masterexport SPARK_DAEMON_JAVA_OPTS=&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER-Dspark.deploy.zookeeper.url=172.15.0.2:2181-Dspark.deploy.zookeeper.dir=/sparkmaster&quot;</code></pre><h3 id="slaves"><a href="#slaves" class="headerlink" title="slaves"></a>slaves</h3><pre><code>cluster-slave1 cluster-slave2</code></pre><h3 id="spark-default-conf"><a href="#spark-default-conf" class="headerlink" title="spark-default.conf"></a>spark-default.conf</h3><pre><code>spark.eventLog.enabled          truespark.eventLog.dir              hdfs://jinbill/spark/eventLogspark.history.fs.logDirectory   hdfs://jinbill/spark/eventLogspark.eventLog.compress         true</code></pre><h2 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h2><pre><code class="shell">tar -zxvf apache-hive-3.1.2-bin.tar.gzmv apache-hive-3.1.2-bin /usr/local/hive# 新建一个日志目录mkdir /usr/local/hive/iotmp</code></pre><h3 id="设置环境变量"><a href="#设置环境变量" class="headerlink" title="设置环境变量"></a>设置环境变量</h3><pre><code class="shell">vi ~/.bashrcexport HIVE_HOME=/usr/local/hiveexport PATH=$HIVE_HOME/bin:$PATHsource ~/.bashrc</code></pre><h3 id="设置Hive-HDFS文件夹"><a href="#设置Hive-HDFS文件夹" class="headerlink" title="设置Hive HDFS文件夹"></a>设置Hive HDFS文件夹</h3><h3 id="解决guava库问题"><a href="#解决guava库问题" class="headerlink" title="解决guava库问题"></a>解决guava库问题</h3><pre><code class="shell"># 查看hadoop下guava 版本cd /usr/local/hadoop/share/hadoop/common/lib/# guava-27.0-jre.jar# 查看hive下guava 版本cd /usr/local/hive/lib# guava-19.0.jar# 高版本替换低版本</code></pre><h3 id="MySQL-中-建立hive数据库"><a href="#MySQL-中-建立hive数据库" class="headerlink" title="MySQL 中 建立hive数据库"></a>MySQL 中 建立hive数据库</h3><pre><code class="sql">CREATE DATABASE hive;</code></pre><h3 id="jdbc依赖导入"><a href="#jdbc依赖导入" class="headerlink" title="jdbc依赖导入"></a>jdbc依赖导入</h3><h4 id="将hive的jline包替换到hadoop的yarn下"><a href="#将hive的jline包替换到hadoop的yarn下" class="headerlink" title="将hive的jline包替换到hadoop的yarn下"></a>将hive的jline包替换到hadoop的yarn下</h4><pre><code class="shell">`mv /opt/hive/apache-hive-3.1.2-bin/lib/jline-2.12.jar /opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/`# JDBC 依赖放入 /usr/local/hive/libmv mysql-connector-java-5.1.47.jar /usr/local/hive/lib/</code></pre><h3 id="修改master节点配置文件"><a href="#修改master节点配置文件" class="headerlink" title="修改master节点配置文件"></a>修改master节点配置文件</h3><pre><code>#### VI编辑器替换命令:%s/$&#123;system:java.io.tmpdir&#125;/\/opt\/hive\/iotmp/g  :%s/$&#123;system:user.name&#125;/huan/g</code></pre><h4 id="使用mysql替换默认的derby存放元数据"><a href="#使用mysql替换默认的derby存放元数据" class="headerlink" title="使用mysql替换默认的derby存放元数据"></a>使用mysql替换默认的derby存放元数据</h4><pre><code class="xml">&lt;!--元数据库修改为MySQL--&gt;&lt;property&gt;    &lt;name&gt;hive.metastore.db.type&lt;/name&gt;    &lt;value&gt;mysql&lt;/value&gt;    &lt;description&gt;      Expects one of [derby, oracle, mysql, mssql, postgres].      Type of database used by the metastore. Information schema &amp;amp; JDBCStorageHandler depend on it.    &lt;/description&gt;&lt;/property&gt;&lt;!--MySQL 驱动--&gt;&lt;property&gt;    &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;    &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;    &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;&lt;/property&gt;&lt;!--MySQL URL--&gt;&lt;property&gt;    &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;    &lt;value&gt;jdbc:mysql://10.20.89.80:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt;    &lt;description&gt;      JDBC connect string for a JDBC metastore.      To use SSL to encrypt/authenticate the connection, provide database-specific SSL flag in the connection URL.      For example, jdbc:postgresql://myhost/db?ssl=true for postgres database.    &lt;/description&gt;&lt;/property&gt;&lt;!--MySQL 用户名--&gt;&lt;property&gt;  &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;  &lt;value&gt;root&lt;/value&gt;  &lt;description&gt;Username to use against metastore database&lt;/description&gt;&lt;/property&gt;&lt;!--MySQL 密码--&gt;&lt;property&gt;    &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;    &lt;value&gt;123456&lt;/value&gt;    &lt;description&gt;password to use against metastore database&lt;/description&gt;&lt;/property&gt;</code></pre><h4 id="设置解析引擎为spark"><a href="#设置解析引擎为spark" class="headerlink" title="设置解析引擎为spark"></a>设置解析引擎为spark</h4><pre><code class="xml">&lt;property&gt;    &lt;name&gt;hive.execution.engine&lt;/name&gt;    &lt;value&gt;spark&lt;/value&gt;    &lt;description&gt;      Expects one of [mr, tez, spark].      Chooses execution engine. Options are: mr (Map reduce, default), tez, spark. While MR      remains the default engine for historical reasons, it is itself a historical engine      and is deprecated in Hive 2 line. It may be removed without further warning.    &lt;/description&gt;&lt;/property&gt;</code></pre><h4 id="自动初始化元数据"><a href="#自动初始化元数据" class="headerlink" title="自动初始化元数据"></a>自动初始化元数据</h4><pre><code class="xml">&lt;property&gt;    &lt;name&gt;datanucleus.schema.autoCreateAll&lt;/name&gt;    &lt;value&gt;true&lt;/value&gt;    &lt;description&gt;Auto creates necessary schema on a startup if one doesn&#39;t exist. Set this to false, after creating it once.To enable auto create also set hive.metastore.schema.verification=false. Auto creation is not recommended for production use cases, run schematool command instead.    &lt;/description&gt;&lt;/property&gt;</code></pre><h4 id="关闭校验"><a href="#关闭校验" class="headerlink" title="关闭校验"></a>关闭校验</h4><pre><code class="xml">&lt;!--听说是JDK版本使用1.8的问题。。--&gt;&lt;property&gt;   &lt;name&gt;hive.metastore.schema.verification&lt;/name&gt;   &lt;value&gt;false&lt;/value&gt;   &lt;description&gt;     Enforce metastore schema version consistency.     True: Verify that version information stored in is compatible with one from Hive jars.  Also disable automatic           schema migration attempt. Users are required to manually migrate schema after Hive upgrade which ensures           proper metastore schema migration. (Default)     False: Warn if the version information stored in metastore doesn&#39;t match with one from in Hive jars.   &lt;/description&gt;&lt;/property&gt;&lt;property&gt;   &lt;name&gt;hive.conf.validation&lt;/name&gt;   &lt;value&gt;false&lt;/value&gt;   &lt;description&gt;Enables type checking for registered Hive configurations&lt;/description&gt; &lt;/property&gt;</code></pre><h4 id="删除-description-中的-8，这个解析会报错"><a href="#删除-description-中的-8，这个解析会报错" class="headerlink" title="删除 description 中的 &amp;#8，这个解析会报错"></a>删除 description 中的 <code>&amp;#8</code>，这个解析会报错</h4><pre><code class="xml">&lt;property&gt;   &lt;name&gt;hive.txn.xlock.iow&lt;/name&gt;   &lt;value&gt;true&lt;/value&gt;   &lt;description&gt;     Ensures commands with OVERWRITE (such as INSERT OVERWRITE) acquire Exclusive locks for&amp;#8;transactional tables.  This ensures that inserts (w/o overwrite) running concurrently     are not hidden by the INSERT OVERWRITE.   &lt;/description&gt;&lt;/property&gt;</code></pre><h2 id="Spark任务提交"><a href="#Spark任务提交" class="headerlink" title="Spark任务提交"></a>Spark任务提交</h2><p>打包python依赖，镜像<br>进入到虚拟环境下，如&#x2F;home&#x2F;hadoop&#x2F;anaconda3&#x2F;envs，使用以下命令将虚拟环境进行打包：</p><pre><code class="shell">zip -r conda_env.zip conda_env # 虚拟环境为conda_env, 打包为conda_env.zip 文件</code></pre><h4 id="spark-submit参数设置"><a href="#spark-submit参数设置" class="headerlink" title="spark-submit参数设置"></a>spark-submit参数设置</h4><pre><code class="python">from pyspark.sql import SparkSessionspark_conf = SparkConf().loadPropertiesFile(&quot;spark_config.properties&quot;)spark = SparkSession.builder \    .appName(spark_conf.get(&quot;spark.app.name&quot;)) \    .master(spark_conf.get(&quot;spark.master&quot;)) \    .config(&quot;spark.executor.memory&quot;, spark_conf.get(&quot;spark.executor.memory&quot;)) \    .config(&quot;spark.driver.memory&quot;, spark_conf.get(&quot;spark.driver.memory&quot;)) \    .getOrCreate()# 使用SparkSession进行数据处理和分析</code></pre><pre><code class="bash">$&#123;SPARK_PATH&#125;/bin/spark-submit \ --master yarn \ --name &quot;spark_demo_lr&quot; \ --queue $&#123;YARN_QUEUE&#125; \ --deploy-mode $&#123;DEPLOY_MODE&#125; \ --driver-memory 6g \ --driver-cores 4 \ --executor-memory 12g \ --executor-cores 15 \ --num-executors 10 \ --archives ./source/py27.zip#python_env \ --conf spark.default.parallelism=150 \ --conf spark.executor.memoryOverhead=4g \ --conf spark.driver.memoryOverhead=2g \ --conf spark.yarn.maxAppAttempts=3 \ --conf spark.yarn.submit.waitAppCompletion=true \ --conf spark.pyspark.driver.python=./source/py27/bin/python2 \ --conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=./python_env/py27/bin/python2 \ --conf spark.pyspark.python=./python_env/py27/bin/python2 \ ./$&#123;ModelType&#125;.py $input_path_train $input_path_test $output_path</code></pre><h4 id="pyspark-传入配置文件参数"><a href="#pyspark-传入配置文件参数" class="headerlink" title="pyspark 传入配置文件参数"></a>pyspark 传入配置文件参数</h4><p>首先，可以使用 <code>--files</code> 参数将配置文件传递给 <code>spark-submit</code> 命令。这将确保配置文件在集群中的每个节点上都可用。以下是一个示例：</p><pre><code class="shell">spark-submit --master yarn --deploy-mode cluster --files /home/sys_user/ask/conf/config.ini test.py</code></pre><p>在 PySpark 脚本中，可以使用 <code>SparkFiles.get()</code> 方法来读取传入的配置文件</p><pre><code class="python">from pyspark import SparkFileswith open(SparkFiles.get(&#39;config.ini&#39;)) as config_file:    print(config_file.read())</code></pre><pre><code class="python">from pyspark.sql import SparkSession# 创建 SparkSessionspark = SparkSession.builder.appName(&quot;ConfigExample&quot;).getOrCreate()# 加载配置参数with open(&quot;config.properties&quot;, &quot;r&quot;) as f:    for line in f:        line = line.strip()        if line and not line.startswith(&quot;#&quot;):            key, value = line.split(&quot;=&quot;)            spark.conf.set(key, value)# 打印配置参数print(&quot;spark.sql.shuffle.partitions:&quot;, spark.conf.get(&quot;spark.sql.shuffle.partitions&quot;))print(&quot;spark.sql.autoBroadcastJoinThreshold:&quot;, spark.conf.get(&quot;spark.sql.autoBroadcastJoinThreshold&quot;))</code></pre><pre><code class="shell">./spark-submit --master yarn --deploy-mode cluster --files /opt/bigdata/config_batch.properties --executor-memory 2g --executor-cores 1 --num-executors 5 --driver-memory 2G  --py-files /usr/local/miniconda3/envs/spark_env.zip --jars /usr/local/spark/jars/mysql-connector-java-5.1.47.jar,/usr/local/spark/jars/taos-jdbcdriver-2.0.34.jar,/usr/local/spark/jars/spark-redis-2.4.0-jar-with-dependencies.jar,/usr/local/spark/jars/fastjson-1.2.73.jar --conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=/usr/local/miniconda3/envs/spark_env/bin/python --conf spark.pyspark.python=/usr/local/miniconda3/envs/spark_env/bin/python /opt/bigdata/HealthScoreHFL2.py</code></pre><p>        .master(“local”)<br>        .enableHiveSupport()<br>        .config(‘spark.executor.extraClassPath’, “&#x2F;usr&#x2F;local&#x2F;spark&#x2F;jars&#x2F;mysql-connector-java-5.1.47.jar”)\</p><p>        .config(‘spark.driver.extraClassPath’, “&#x2F;usr&#x2F;local&#x2F;spark&#x2F;jars&#x2F;mysql-connector-java-5.1.47.jar”)\</p><p>        .config(“spark.jars”, “&#x2F;usr&#x2F;local&#x2F;spark&#x2F;jars&#x2F;mysql-connector-java-5.1.47.jar,&#x2F;usr&#x2F;local&#x2F;spark&#x2F;jars&#x2F;taos-jdbcdriver-2.0.34.jar,&#x2F;usr&#x2F;local&#x2F;spark&#x2F;jars&#x2F;spark-redis-2.4.0-jar-with-dependencies.jar,&#x2F;usr&#x2F;local&#x2F;spark&#x2F;jars&#x2F;fastjson-1.2.73.jar”)\</p><pre><code class="shell">./spark-submit --master yarn --deploy-mode cluster --files /opt/bigdata/batch/config_batch.properties --executor-memory 2g --executor-cores 1 --num-executors 5 --driver-memory 2G  --py-files /opt/bigdata/batch/spark3.1_env.zip --jars /usr/local/spark-yarn/jars/mysql-connector-java-5.1.47.jar,/usr/local/spark-yarn/jars/taos-jdbcdriver-2.0.34.jar,/usr/local/spark-yarn/jars/spark-redis-2.4.0-jar-with-dependencies.jar,/usr/local/spark-yarn/jars/fastjson-1.2.73.jar --conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=/usr/local/miniconda3/bin/python --conf spark.pyspark.python=/usr/local/miniconda3/bin/python /opt/bigdata/batch/HealthScoreHFL2.py</code></pre>]]></content>
      
      
      <categories>
          
          <category> bigdata </category>
          
          <category> docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> bigdata </tag>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2024/01/03/hello-world/"/>
      <url>/2024/01/03/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre><code class="bash">$ hexo new &quot;My New Post&quot;</code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre><code class="bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre><code class="bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre><code class="bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
