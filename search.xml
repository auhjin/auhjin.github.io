<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>网络工程师 软考中高级 备考</title>
      <link href="/2024/02/25/%E7%BD%91%E7%BB%9C%E5%B7%A5%E7%A8%8B%E5%B8%88%20%E8%BD%AF%E8%80%83%E4%B8%AD%E9%AB%98%E7%BA%A7(%E4%B8%80)/"/>
      <url>/2024/02/25/%E7%BD%91%E7%BB%9C%E5%B7%A5%E7%A8%8B%E5%B8%88%20%E8%BD%AF%E8%80%83%E4%B8%AD%E9%AB%98%E7%BA%A7(%E4%B8%80)/</url>
      
        <content type="html"><![CDATA[<h1 id="一、计算机网络概论"><a href="#一、计算机网络概论" class="headerlink" title="一、计算机网络概论"></a>一、计算机网络概论</h1><p>1、计算机网络是通过通信线路和通信设备连接的许多的分散独立工作的计算机系统，遵从一定的协议用软件实现资源共享的系统。</p><p>2、组成：硬件、软件、协议</p><p>3、协议分为 国际标准OSI&#x2F;RM  （7 层）和公认标注 TCP&#x2F;IP （4层）</p><p>4、分布范围：局域、城域、广域网</p><p>​  拓扑结构： 星型、环形、树型</p><pre><code>  通信网ISP和信息网ICP、校园网和企业网、骨干网和接入网、公用网和专用网</code></pre><p>5、OSI&#x2F;RM 开发系统互联&#x2F;参考模型  英文&#x2F;&#x2F;&#x2F;</p><p>应用       用户应用程序和协议</p><p>表示       数据加密、压缩，语法表示、连接管理</p><p>会话       链接恢复与释放、对会话进行分段、同步</p><p>传输       端到端 可靠透明的数据传输， 分段重组、差错控制、流量空值</p><p>网络       路径选择，网络连接的多路复用、差错检测与恢复、排序流量控制</p><p>数据链路    不可靠信道 变为可靠信道，把比特组织成帧，在链路上点到点的帧传输，差错控制、流量控制</p><p>物理            提供物理通路、二进制比特流、定义电气&#x2F;机械接口</p><p>6 TCP&#x2F;IP  应用、传输、 互联网、网络接口层</p><p>网络传输的过程 数据解封</p><p>各层首部 + 应用程序数据+ 链路层尾部</p><h1 id="二、数据通信基础"><a href="#二、数据通信基础" class="headerlink" title="二、数据通信基础"></a>二、数据通信基础</h1><p>1、信源、信道、信宿          噪声</p><p>2、模拟信号 的带宽 ； W &#x3D; f2 - f1   高频-低频    信号传输的频率范围</p><p>3、数字信号 带宽</p><p>（1）无噪声 奈奎斯特定理</p><p>​码元速率B &#x3D; 2W      波特率                     提高速率  多拉快跑</p><p>​    数据速率    R &#x3D; Blog2 N &#x3D; 2Wlog2 N          N 码元种类</p><p>（2）有噪声 香农定理</p><p>​数据速率C &#x3D; W log2 （1+ S&#x2F;N）     W 信道带宽</p><p>​S 信号平均功率， N 噪声平均功率， </p><p>​S&#x2F;N 信噪比   分贝dB表示   dB &#x3D; 10lg(S&#x2F;N)</p><p>（3）单位换算</p><p>通信换算 进率  1000   1G &#x3D; 1000M        bit  比特</p><p>存储换算 进率   1024  1G &#x3D; 1024M        Byte 字节</p><p>网速 100M    是bit            下载文件         1字节byte  &#x3D; 8 bit</p><p>信道延迟  电缆信道延迟    200m&#x2F;us （200km&#x2F;ms、200000km&#x2F;s）</p><p>1 s  &#x3D; 1000 ms  &#x3D; 1000 000 us</p><p>卫星信道延迟 270ms</p><p>3、传输介质</p><p>（1）双绞线：</p><p>​非屏蔽双绞线        三15Mbps、五100Mbps、超五、六250Mbps、超六（七）</p><p>​屏蔽层双绞线STP           三、五、超五、六</p><p>​        T568A     白绿、绿、白橙、蓝、白蓝、橙、白棕、棕</p><p>​        T568B     白橙、橙、白绿、蓝、白蓝、绿、白棕、棕</p><p>​         A – B   交叉        A–A   B–B   直通</p><p>​1236   有用  、4578 排除线间干扰</p><p>（2）同轴电缆  粗同轴电缆、细同轴电缆</p><p>传送距离长、信号稳定。   常用电视、监控设备</p><p>（3）光纤  单模光纤、多模光纤    单：高贵细远</p><p>（4）无线信道： 无线电波 、红外光波</p><p>无线电波：长波、中波、短波、超短波、微波（地面、卫星）</p><p>红外光波： 近红外线、中红外线、远红外线</p><p>4、模拟信道调制： 调幅ASK、调频FSK、调相PSK、<br>正交调幅QAM （幅度相同，相位相差90度）</p><p>（1）数字信道编码：采样（2倍）、量化、编码</p><p>DPSK4相键控、QPSK 正交相移键控   码元种类 4   比特位  2      提高传输速率 </p><p>（2）极性编码： 单极性码、极性码、双极性码</p><p>归零性编码： 归零码、不归零码、双相码</p><p>曼彻斯特编码： 降0升1</p><p>差分曼彻斯特： 变1不变0、 折0平1</p><p>（3）各种编码效率：4B&#x2F;5B 效率&#x3D;80%     百兆以太网</p><p>​                                    8B&#x2F;10B     80%         千兆</p><p>​64B&#x2F;66B  97%         万兆</p><p>曼彻斯特&#x2F;差分曼彻斯特         50%         以太网 10兆</p><p>c  &#x3D; 2W log2 （1+1000） &#x3D; 10 *4000</p><h1 id="三、-数据通信基础（2）"><a href="#三、-数据通信基础（2）" class="headerlink" title="三、 数据通信基础（2）"></a>三、 数据通信基础（2）</h1><p>1、数据通信方式</p><p>单工、全双工、半双工</p><p>传输方式： 异步传输（远距离、数据小）、同步传输（近距离、数据大）</p><p>2、数据交换方式</p><p>（1）电路交换</p><p>优： 独占性、实时性、适合传输大量的数据</p><p>缺：需要建立一条物理连接、利用率低。早期的电话系统</p><p>（2）报文交换</p><p>优： 不需要专用通道，线路利用率高，存储转发节点可校验纠错。</p><p>缺：有通信时延。    类似物流包裹</p><p>（3）分组交换（数据报和虚电路）</p><p>优: 利用率更高、可选路径、数据率转换、支持优先级。</p><p>缺：时延、开销大（每个分组都有单独的源地址、目标地址数据帧）。</p><p>（4）分组数据包： 单向传送、无连接</p><p>​分组虚电路： 交互式、逻辑链接</p><p>3、多路复用技术</p><p>（1）频分复用FDM： 不同的频率，子信道隔离频带放串扰。 CATV，WIFI</p><p>（2）时分复用TDM：不同的时间，轮流占用。</p><p>​分为： 同步时分T1、E1； 统计时分ATM</p><p>（3）波分复用WDM：不同的波长     只用于光纤</p><p>4、数字传输标准</p><p>（1）T1标准： 1.544Mbps    125us &#x3D; 8000次</p><p>​1.544Mbps &#x3D; [24 * (7+1)+1] * 8000</p><p>​T2 &#x3D; 4T1   T3 &#x3D; 7T2  T4 &#x3D; 6T3</p><p>   (2)  E1标准： 2.048Mbps     125us &#x3D; 8000次</p><p>​2.048Mbps &#x3D; [32 * (7+1)] * 8000</p><p>​第一CH0 和 第十七 CH16 为控制信令  ，30个话音数据</p><p>​E2 &#x3D; 4E1  E3 &#x3D; 4E2  E4 &#x3D; 4E3 </p><p>   (3) SONET标准 和 SDH标准        用于光纤网络</p><p>​155.520</p><p>5、数据检错纠错</p><p>（1）检错码： 奇偶校验    看1的个数是奇数&#x2F;偶数。只能检错不能纠错</p><p>（2）海明码： 在数据位m后面增加冗余校验位k，组成 m+k</p><p>​m+k &lt; 2^k - 1   可纠正一位错误        能检错纠错</p><p>​码距d 两个码字之间不同的最小的位数</p><p>（3）CRC码： 冗余循环校验码        通过循环位移，实现检错，硬件容易实现</p><p>​只检测不纠错，广泛用于局域网</p><p>​多项式除法，不进位加法。</p><p>​1、写除数 2、写被除数 3、 模2计算     </p><h1 id="四、广域通信网"><a href="#四、广域通信网" class="headerlink" title="四、广域通信网"></a>四、广域通信网</h1><p>1、广域网： 长距离、跨地区的各种局域网、计算机、终端互联在一起，组成一个资源共享的通信网络。</p><p>传统：公共交换电话网PSTN，公共数据网X.25，帧中继网FR，综合业务数据网ISDN，异步传输模式ATM，虚拟专用网VPN</p><p>现代：数据数据网DDN，同步数据传输网SDH，多业务传送平台MSTP，光纤接入网FTTX，无源光网络PON，无线网WiMAX、4G</p><p>2、PSTN公共交换电话网：利用电话线上网，早期拨号上网</p><p>机械特性： RS-232-C</p><p>电气特性：采用V.28标准，信号源产生3-15V信号，±3V之间是信号电平过度区，另外两种常用的电气特性标准V.10和V.11</p><p>功能特性：对接口连线给出明确的定义。RS-232-C采用标准V.24。为DTE&#x2F;DCE接口定义了44条连线，为DTE&#x2F;ACE定义了12条连线。ACE为自动呼叫设备。按照V.24的命名方法，DTE&#x2F;DCE连线用‘’1‘’开头的三位数字命名；DTE&#x2F;ACE连线用‘’2‘’开头的三位数字命名。</p><p>过程特性：规定了使用接口线实现数据传输的操作过程</p><p>5、X.25公共数据网 使用分组交换 分三层：物理层、链路层、网络层</p><p>对应于OSI的低三层。 采用虚拟电路、面向连接的。 后退N帧ARQ、滑动窗口默认2</p><p>6、流量控制技术：协调收发端流量</p><p>停等协议： 发送一帧、等到应答，再发送。如果不应答，一直等。</p><p>滑动窗口协议：连续发送多个帧而无需应答</p><p>7、差错控制技术</p><p>自动请求重发 ARQ</p><ol><li>肯定应答</li><li>否定应答重发</li><li>超时重发</li></ol><p>停等ARQ： 停等流控和ARQ相结合</p><p>选择重发ARQ：滑动窗口和ARQ的结合，之重新发送出错位</p><p>后退N帧ARQ：滑动窗口和ARQ结合，从出错开始后退到出错位开始发送</p><p>8、流控和差错利用率计算公式</p><ol><li>常数a值的计算： a &#x3D; （d&#x2F;v）&#x2F;（L&#x2F;R） &#x3D; （RT）&#x2F;L &#x3D; （Rd&#x2F;v）&#x2F;L</li></ol><p>9、 高级数据链路控制协议HDLC  面向比特的</p><p>通常使用CRC-16、CRC-32校验。帧边界01111110</p><p>​    帧头- 地址 - 控制 -数据 - 校验 -帧尾</p><p>​       F        A        C     INFO    FCS     F</p><p>​      8         8        8   可变长  16&#x2F;32    8 </p><p>10、帧中继网FR   是根据X.25演变改进的</p><ul><li><p>工作在OSI的低两层，物理层和链路层</p></li><li><p>在第二层建立虚电路，与X.25一样，支持永久虚电路PVC和交换虚电路SVC，承载数据业务。也是分组交换。</p></li><li><p>FR只做检错，不再重传，没有流控，只有拥塞控制，检错交高层</p></li><li><p>11、帧中继的优点：</p></li><li><p>基于分组交换的透明传输，可提供面向连接的服务</p></li><li><p>帧长可变，长度可达1600~4096字节，可以承载各种局域网数据帧</p></li><li><p>数据速率可达2~45Mbps</p></li><li><p>可以接需要提供带宽，也可以应付突发的数据传输</p></li><li><p>没有流控和重传机制，开销很少</p></li></ul><p>12、综合业务数据网ISDN：基于电路交换，把数据、声音、视频信号三合一传输</p><p>ISDN两种速率： </p><p>基本（N-ISDN）速率BRI（2B+D）</p><p>基群（B-ISDN）速率PRI（30B+D）</p><p>13、异步传输模式ATM：最早是B-ISDN标准的一部分</p><p>分四层模型： 物理层、ATM层、ATM适配层、</p><p>采用53字节信元分组交换，使用统计时分TDM。采用双绞线或光纤，数据速率155M</p><p>面向连接，使用虚电路</p><h1 id="五、局域网和城域网"><a href="#五、局域网和城域网" class="headerlink" title="五、局域网和城域网"></a>五、局域网和城域网</h1><p>1、 局域网互联设备</p><p>2层网桥，</p><p>3层交换机、路由器              </p><p>2、生成树网桥&#x2F;透明网桥， IEEE 802.1d， 生成树算法</p><p>在网桥之间传递BPDU，比较参数，根据STP打开好端口，阻塞差端口，沿着好的端口建立路径。边走边拐弯。用于以太网</p><p>步骤：1、确定根桥2、</p><p>SW确定根桥id ：优先级+MAC地址，都选最小，优先级0-65535 默认32768（±4096）</p><p>Post确定根端口 ：优先级+编号， 都选最小，优先级0-255，默认128</p><p>三小原则：优先级、MAC地址、通路费用 </p><p>3、生成树端口的四种状态     插上网线后正常要经过50s，端口才能通</p><ol><li>Blocking阻塞 接受BPDU，不学习MAC地址，不转发数据帧             20S</li><li>Listening侦听 接受BPDU，不转发MAC地址，不转发数据帧。参与选举根端口或指定端口     15S</li><li>Learning 学习 接受BPDU，学习MAC地址，不转发数据帧  15S</li><li>Forwarding转发  正常转发数据帧  15S</li></ol><p>4、生成树种类和标准</p><p>多生成树    MSTP        IEEE802.1s</p><p>快速生成树RSTP         IEEE802.1w</p><p>生成树       STP            IEEE802.1d</p><p>端口认证   基于用户    IEEE802.1x</p><p>6、源路由网桥   802.5</p><p>发送探测帧到目的节点，返回路径以后沿着路径再传送。</p><p>探寻路径，不在路径上就广播，查询路径，选择最优路径再传送，选好路径再走</p><p>7 城域网</p><p>提供分组传输的数据、语音、视频的等多媒体业务  更大的传输容量更高的传输效率</p><p>无线城域网标准： WiMAX 802.16d固定 802.16e移动， WiMAXII 802.16m  4G  LTE</p><p>Q-in-Q   运营商网桥协议 PBP 802.1ad</p><p>MAC-in-MAC  运营商主干网桥 PBB  802.1ah</p><h1 id="无线通信网"><a href="#无线通信网" class="headerlink" title="无线通信网"></a>无线通信网</h1>]]></content>
      
      
      <categories>
          
          <category> Network </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Network </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PySpark yarn优化以及使用</title>
      <link href="/2024/02/22/Spark%E9%9B%86%E7%BE%A4%E4%B8%8A%E8%BF%90%E8%A1%8CPySpark%20&amp;%20yarn%E4%BC%98%E5%8C%96%E5%8F%8A%E4%BD%BF%E7%94%A8/"/>
      <url>/2024/02/22/Spark%E9%9B%86%E7%BE%A4%E4%B8%8A%E8%BF%90%E8%A1%8CPySpark%20&amp;%20yarn%E4%BC%98%E5%8C%96%E5%8F%8A%E4%BD%BF%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<h2 id="0、引言"><a href="#0、引言" class="headerlink" title="0、引言"></a>0、引言</h2><p>本文内容涉及PySpark的依赖打包和python虚拟环境的使用。<br>以下均经过实验验证其正确性。</p><h2 id="1、在Spark集群上运行PySpark-Application"><a href="#1、在Spark集群上运行PySpark-Application" class="headerlink" title="1、在Spark集群上运行PySpark Application"></a>1、在Spark集群上运行PySpark Application</h2><p>大致分为如下3种情况：</p><h3 id="1-1-YARN集群配置Python环境"><a href="#1-1-YARN集群配置Python环境" class="headerlink" title="1.1 YARN集群配置Python环境"></a>1.1 YARN集群配置Python环境</h3><pre><code>如果是初始安装YARN、Spark集群，并考虑到了当前应用场景需要支持Python程序运行在Spark集群之上，这时可以准备好对应Python软件包、依赖模块，在YARN集群中的每个节点上进行安装。这样，YARN集群的每个NodeManager上都具有Python环境，可以编写PySpark Application并在集群上运行。目前比较流行的是直接安装Python虚拟环境，使用Anaconda/Miniconda等软件，可以极大地简化Python环境的管理工作。这种方式的缺点是，如果后续使用Python编写Spark Application，需要增加新的依赖模块，那么就需要在YARN集群的每个节点上都进行该新增模块的安装。而且，如果依赖Python的版本，可能还需要管理不同版本Python环境。因为提交PySpark Application运行，具体在哪些NodeManager上运行该Application，是由YARN的调度器决定的，必须保证每个NodeManager上都具有Python环境（基础环境+依赖模块）。</code></pre><h3 id="1-2-YARN集群不配置Python环境"><a href="#1-2-YARN集群不配置Python环境" class="headerlink" title="1.2 YARN集群不配置Python环境"></a>1.2 YARN集群不配置Python环境</h3><pre><code>这种情况，更适合企业已经安装了规模较大的YARN集群，并在开始使用时并未考虑到后续会使用基于Python来编写Spark Application，并且不想在YARN集群的NodeManager上安装Python基础环境及其依赖模块。我们参考了Benjamin Zaitlen的博文（详见后面参考链接），并基于Anaconda软件环境进行了实践和验证，具体实现思路如下所示：在任意一个Linux OS的节点上，安装Anaconda软件通过Anaconda创建虚拟Python环境在创建好的Python环境中下载安装依赖的Python模块将整个Python环境打成zip包提交PySpark Application时，并通过--archives选项指定zip包路径</code></pre><p>python环境打包首先参考链接[[Python 环境离线安装]]<br>[Python 环境离线安装](<a href="https://auhjin.github.io/2024/01/10/Python">https://auhjin.github.io/2024/01/10/Python</a> 环境离线安装&#x2F;)<br>打包conda镜像</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/miniconda3/envs</span><br><span class="line">zip -r spark_env.zip spark_env</span><br><span class="line">cp spark_env.zip /opt/bigdata</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">通过使用 archives 参数上传pysaprk环境所需的python依赖</span></span><br><span class="line">spark-submit --archives /opt/bigdata/spark_env.zip</span><br></pre></td></tr></table></figure><h3 id="1-3-基于混合编程语言环境"><a href="#1-3-基于混合编程语言环境" class="headerlink" title="1.3 基于混合编程语言环境"></a>1.3 基于混合编程语言环境</h3><pre><code>假如我们还是希望使用Spark on YARN模式来运行PySpark Application，但并不将Python程序提交到YARN集群上运行。这时，我们可以考虑使用混合编程语言的方式，来处理数据任务。比如，机器学习Application具有迭代计算的特性，更适合在一个高配的节点上运行；而普通的ETL数据处理具有多机并行处理的特点，适合放到集群上进行分布式处理。一个完整的机器学习Application的设计与构建，可以将算法部分和数据准备部分分离出来，使用Scala/Java进行数据预处理，输出一个机器学习算法所需要（更便于迭代、寻优计算）的输入数据格式，这会极大地压缩算法输入数据的规模，从而使算法迭代计算充分利用单机本地的资源（内存、CPU、网络），这可能会比直接放到集群中计算要快得多。因此，我们在对机器学习Application准备数据时，使用原生的Scala编程语言实现Spark Application来处理数据，包括转换、统计、压缩等等，将满足算法输入格式的数据输出到HDFS指定目录中。在性能方面，对数据规模较大的情况下，在Spark集群上处理数据，Scala/Java实现的Spark Application运行性能要好一些。然后，算法迭代部分，基于丰富、高性能的Python科学计算模块，使用Python语言实现，其实直接使用PySpark API实现一个机器学习PySpark Application，运行模式为YARN client模式。这时，就需要在算法运行的节点上安装好Python环境及其依赖模块（而不需要在YARN集群的节点上安装），Driver程序从HDFS中读取输入数据（缓存到本地），然后在本地进行算法的迭代计算，最后输出模型。</code></pre><p>总结</p><pre><code>对于重度使用PySpark的情况，比如偏向机器学习，可以考虑在整个集群中都安装好Python环境，并根据不同的需要进行依赖模块的统一管理，能够极大地方便PySpark Application的运行。不在YARN集群上安装Python环境的方案，会使提交的Python环境zip包在YARN集群中传输带来一定开销，而且每次提交一个PySpark Application都需要打包一个环境zip文件，如果有大量的Python实现的PySpark Application需要在Spark集群上运行，开销会越来越大。另外，如果PySpark应用程序修改，可能需要重新打包环境。但是这样做确实不在需要考虑YARN集群集群节点上的Python环境了，任何版本Python编写的PySpark Application都可以使用集群资源运行。</code></pre><h2 id="2、使用-spark-submit-下-yarn-的常见模式以及区别"><a href="#2、使用-spark-submit-下-yarn-的常见模式以及区别" class="headerlink" title="2、使用 spark-submit 下 yarn 的常见模式以及区别"></a>2、使用 spark-submit 下 yarn 的常见模式以及区别</h2><h3 id="2-1-yarn-模式"><a href="#2-1-yarn-模式" class="headerlink" title="2.1 yarn 模式"></a>2.1 yarn 模式</h3><ul><li>client 模式<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">提交方式： `–master yarn –deploy-mode client`</span><br><span class="line">特点：  </span><br><span class="line">1，driver运行在提交作业的机器上(可以看到程序打印日志)；</span><br></pre></td></tr></table></figure></li><li>cluster 模式<figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">提交方式： `–<span class="keyword">master</span> <span class="title">yarn</span> –deploy-mode cluster`</span><br><span class="line">特点：  </span><br><span class="line"><span class="number">1</span>，driver运行在集群上某个机器上（看不到日志，只可以看到running状态）,Driver在AppMaster执行;</span><br></pre></td></tr></table></figure></li></ul><h3 id="2-2-使用场景"><a href="#2-2-使用场景" class="headerlink" title="2.2 使用场景"></a>2.2 使用场景</h3><ul><li>如果在<code>appmanager</code>和<code>nodemanger</code>不在同一网段，采用<code>spark-cluster</code>模式，减少网络延迟。</li><li>如果节点数很多，采用<code>spark-cluster</code>，这样可以资源均衡。</li><li>如果节点数少，又在同一网段，<code>client模式</code>和<code>cluster模式</code>，都是可以的。</li><li>在刚上线时也要采用<code>spark-client模式</code>，这样可以获取信息，都没有问题了，稳定了，再切换到<code>spark-cluster</code>模式。</li></ul><h2 id="3、yarn模式下的依赖包提交"><a href="#3、yarn模式下的依赖包提交" class="headerlink" title="3、yarn模式下的依赖包提交"></a>3、yarn模式下的依赖包提交</h2><p>spark-submit ， 使用–py-files 参数选项，作用是声明Drive依赖模块的路径，Driver把路径下的文件拷贝到HDFS对应的application文件下。<br>运行时，打印的日志如下：</p><figure class="highlight autohotkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">```</span><br><span class="line">如果需要上传文件资源，则使用`--files`【作用：把文件放置在每个executor工作目录的文件列表】</span><br></pre></td></tr></table></figure><ol><li><p><code>--master &lt;master-url&gt;</code>：指定集群的master URL，可以是local、yarn、mesos、standalone等。</p></li><li><p><code>--deploy-mode &lt;deploy-mode&gt;</code>：指定应用程序的部署模式，可以是cluster或client。cluster模式表示驱动程序运行在集群中，client模式表示驱动程序运行在提交应用程序的机器上。</p></li><li><p><code>--conf &lt;key=value&gt;</code>：用于设置Spark应用程序的配置选项，例如设置内存、并行度等。</p></li><li><p><code>--driver-memory &lt;value&gt;</code>：指定驱动程序的内存大小，例如<code>--driver-memory 4g</code>表示4GB内存。</p></li><li><p><code>--executor-memory &lt;value&gt;</code>：指定每个执行器的内存大小，例如<code>--executor-memory 2g</code>表示2GB内存。</p></li><li><p><code>--executor-cores &lt;number of cores&gt;</code>：指定每个执行器的CPU核心数。</p></li><li><p><code>--py-files &lt;comma separated dependencies&gt;</code>：指定要与应用程序一起分发的Python文件，可以是.py、.zip或.egg文件。</p></li><li><p><code>--jars &lt;comma separated dependencies&gt;</code>：指定要与应用程序一起使用的依赖项的JAR包。</p></li><li><p><code>--files &lt;comma separated files&gt;</code>：指定要与应用程序一起使用的文件</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure><p>.&#x2F;spark-submit –master yarn –deploy-mode cluster –files &#x2F;opt&#x2F;bigdata&#x2F;batch&#x2F;config_batch.properties –executor-memory 2g –executor-cores 1 –num-executors 5 –driver-memory 2G  –archives&#x3D;&#x2F;opt&#x2F;bigdata&#x2F;batch&#x2F;spark3.1_env.zip –jars &#x2F;usr&#x2F;local&#x2F;spark-yarn&#x2F;jars&#x2F;mysql-connector-java-5.1.47.jar,&#x2F;usr&#x2F;local&#x2F;spark-yarn&#x2F;jars&#x2F;taos-jdbcdriver-2.0.34.jar,&#x2F;usr&#x2F;local&#x2F;spark-yarn&#x2F;jars&#x2F;spark-redis-2.4.0-jar-with-dependencies.jar,&#x2F;usr&#x2F;local&#x2F;spark-yarn&#x2F;jars&#x2F;fastjson-1.2.73.jar –conf spark.yarn.appMasterEnv.PYSPARK_PYTHON&#x3D;&#x2F;usr&#x2F;local&#x2F;miniconda3&#x2F;bin&#x2F;python &#x2F;opt&#x2F;bigdata&#x2F;batch&#x2F;HealthScoreHFL2.py</p></li></ol><pre><code></code></pre>]]></content>
      
      
      <categories>
          
          <category> bigdata </category>
          
          <category> spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> bigdata </tag>
            
            <tag> spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于注意力网络的多元时序分类模型(TapNet)</title>
      <link href="/2024/02/01/%E5%9F%BA%E4%BA%8E%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%8E%9F%E5%9E%8B%E7%BD%91%E7%BB%9C%E7%9A%84%E5%A4%9A%E5%85%83%E6%97%B6%E5%BA%8F%E5%88%86%E7%B1%BB(TapNet)/"/>
      <url>/2024/02/01/%E5%9F%BA%E4%BA%8E%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%8E%9F%E5%9E%8B%E7%BD%91%E7%BB%9C%E7%9A%84%E5%A4%9A%E5%85%83%E6%97%B6%E5%BA%8F%E5%88%86%E7%B1%BB(TapNet)/</url>
      
        <content type="html"><![CDATA[<h3 id="论文名称：TapNet-Multivariate-Time-Series-Classification-with-Attentional-Prototypical-Network"><a href="#论文名称：TapNet-Multivariate-Time-Series-Classification-with-Attentional-Prototypical-Network" class="headerlink" title="论文名称：TapNet: Multivariate Time Series Classification with Attentional Prototypical Network"></a>论文名称：TapNet: Multivariate Time Series Classification with Attentional Prototypical Network</h3><h3 id="作者：Xuchao-Zhang-Yifeng-Gao-Jessica-Lin-etl"><a href="#作者：Xuchao-Zhang-Yifeng-Gao-Jessica-Lin-etl" class="headerlink" title="作者：Xuchao Zhang, Yifeng Gao, Jessica Lin etl."></a>作者：Xuchao Zhang, Yifeng Gao, Jessica Lin etl.</h3><h3 id="摘要："><a href="#摘要：" class="headerlink" title="摘要："></a>摘要：</h3><p>随着传感器技术的进步，多元时序分类（MTSC）问题成为时间序列数据挖掘领域最重要的问题之一，近些年来不断受到广泛关注 <strong>···特别是在智能制造、工业互联网领域，有着丰富的应用场景和数据分析需求···</strong><br>传统的基于时间序列分类方法 主要是基于 bag-of-patterns 或者 times series shapelet ，它们难以处理高维多元数据中生成的大量候选特征， 只能在训练集较小的情况下具有良好的性能 。<br>相比之下，基于深度学的方法可以有效地学习低维特征，<strong>但缺乏标记数据</strong></p><p>在 TapNet的论文中，提出了一种带有注意力原型网络的新型MTSC模型，以发挥传统方法和基于深度学习方法的优点。 Specifically， 设计了一种结合多层卷积网络的随机组排列算方法，从多元时序数据中学习地位特征。为了解决训练标签 有限的问题。提出了一种新的注意力原型网络，根据与数据标签不足的类圆形的距离来训练特征表示。 另外，通过利用为标记数据将模型扩展到半监督设置。</p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>时间序列是一组按时间顺序排列的实际观测值；多元时间序列是一组共同演化的时间序列，通常由一组传感器随着时间的推移共同记录。近十年随着传感器技术和大数据技术的不断进步，多元时序分类问题 The Multivariate Time Series Classification (MTSC)：识别多变量时间序列数据记录的标签，开始受到广泛的关注和研究。<br>由于时间序列数据是一种流行的数据类型，存在于广泛的研究领域和应用中，因此多元时间序列分类模型已被用于许多不同的现实应用中，例如人类活动识别Human Activity Recognition，脑电图&#x2F;心电图识别分析EEG&#x2F;ECG data analysis。</p>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 多元时序 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Centos7 CDH&amp;CM 企业内网离线部署</title>
      <link href="/2024/01/18/CentOS%207%20CDH%20&amp;%20CM%20%E4%BC%81%E4%B8%9A%E7%BA%A7%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B2/"/>
      <url>/2024/01/18/CentOS%207%20CDH%20&amp;%20CM%20%E4%BC%81%E4%B8%9A%E7%BA%A7%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B2/</url>
      
        <content type="html"><![CDATA[<h1 id="一、CentOS-7-CDH-CM-企业级离线部署"><a href="#一、CentOS-7-CDH-CM-企业级离线部署" class="headerlink" title="一、CentOS 7 CDH &amp; CM 企业级离线部署"></a>一、CentOS 7 CDH &amp; CM 企业级离线部署</h1><p>在文中将有以下提示符对重点进行标注说明，请注意文中提示。</p><p>️ - 文中出现此标记，代表重要提示，指需要格外注意的地方</p><ul><li>-文中出现此标记，代表可选配置，建议配置，但不必要</li></ul><p>✅ - 文中出现此标记，代表检查项目，需要检查对应的配置文件<br>❗️    - 文中出现此标记，代表该操作不可随意修改，如更改此步操作，请预先测试</p><h2 id="Cloudera’s-Distribution-Including-Apache-Hadoop"><a href="#Cloudera’s-Distribution-Including-Apache-Hadoop" class="headerlink" title="Cloudera’s Distribution Including Apache Hadoop"></a>Cloudera’s Distribution Including Apache Hadoop</h2><p>CDH（Cloudera’s Distribution Including Apache Hadoop）是Apache Hadoop和相关项目的最完整，经过测试和最流行的发行版。CDH提供Hadoop的核心要素–可扩展的存储和分布式计算–以及基于Web的用户界面和重要的企业功能。CDH是Apache许可的开源软件，并且是唯一提供统一批处理，交互式SQL和交互式搜索以及基于角色的访问控制的Hadoop解决方案。 一句话概括CDH就是集成多种技术组件的一个框架。</p><p> CDH提供<br>    - 灵活性-存储任何类型的数据并使用各种不同的计算框架进行处理，包括批处理，交互式SQL，自由文本搜索，机器学习和统计计算。<br>    - 集成-在可与广泛的硬件和软件解决方案一起使用的完整Hadoop平台上快速启动并运行。<br>    - 安全性-处理和控制敏感数据。<br>    - 可扩展性-启用广泛的应用程序并进行扩展，并扩展它们以满足您的要求。<br>    - 高可用性-自信地执行关键任务业务任务。<br>    - 兼容性-利用您现有的IT基础架构和投资。</p><p>基于 Web 的用户界面，支持大多数 Hadoop 组件，包括 HDFS、MapReduce、Hive、Pig、Hbase、Zookeeper、Sqoop，简化了大数据平台的安装和使用难度。<br>除此 Apache Hadoop 发行版本之外，还有如下发行版：</p><ul><li>Cloudera’s Distribution Including Apache Hadoop（CDH）「本文使用」</li><li>Hortonworks Data Platform (HDP)</li><li>MapR</li><li>EMR</li></ul><h5 id="Hadoop生态构成"><a href="#Hadoop生态构成" class="headerlink" title="Hadoop生态构成"></a>Hadoop生态构成</h5><ul><li><strong>HDFS:分布式文件系统</strong><ul><li>ZKFC：为实现NameNode高可用，在NameNode和Zookeeper之间传递信息，选举主节点工具。</li><li>NameNode：存储文件元数据</li><li>DateNode：存储具体数据</li><li>JournalNode：同步主NameNode节点数据到从节点NameNode</li></ul></li><li><strong>MapReduce:开源的分布式批处理计算框架</strong></li><li><strong>Spark：分布式基于内存的批处理框架</strong></li><li><strong>Zookeeper:分布式协调管理</strong></li><li><strong>Yarn:调度资源管理器</strong></li><li>HBase：基于HDFS的NoSql列式数据库</li><li><strong>Hive：将SQL转换为MapReduce进行计算</strong></li><li><strong>Hue：是CDH的一个UI框架</strong></li><li><strong>Impala：是Cloudra公司开发的一个查询系统，类似于Hive，可以通过SQL执行任务，但是它不基于MapReduce算法，而是直接执行分布式计算，这样就提高了效率。</strong></li><li>oozie:是一个工作流调度引擎，负责将多个任务组合在一起按序执行。</li><li>kudu：Apache Kudu是转为hadoop平台开发的列式存储管理器。和impala结合使用，可以进行增删改查。</li><li>Sqoop：将hadoop和关系型数据库互相转移的工具。</li><li>Flume：采集日志</li><li>还有一些其它的<br>![[Pasted image 20230702211449.png]]</li></ul><h2 id="Cloudera-Manager"><a href="#Cloudera-Manager" class="headerlink" title="Cloudera Manager"></a>Cloudera Manager</h2><p>Cloudra Manager简称CM，它是一个web操作平台，可以借助安装CDH然后安装多种Hadoop框架。是用于管理 CDH 集群的端到端应用程序，统一管理和安装。CDH 除了可以通过 CM 安装也可以通过 YUM、TAR、RPM 安装。 CloudraManager技术构成：</p><ul><li>Management Server：Cloudera Manager 的核心。主要用于管理 web server 和应用逻辑。它用于安装软件，配置，开始和停止服务，以及管理服务运行的集群。</li><li>API：通过API和ClouderaManagement和服务器进行交互</li><li>agent：分布在多台服务器，它负责启动和停止进程，部署配置，触发安装和监控主机。</li><li>Database：存储配置和监控信息。通常可以在一个或多个数据库服务器上运行的多个逻辑数据库。例如，所述的 Cloudera 管理器服务和监视，后台程序使用不同的逻辑数据库。</li><li>Parcel（Cloudera Repository）：由 Cloudera 提供的软件分发库。</li><li>Clients：客户端，提供了一个与 Server 交互的接口，通过web页面和ClouderaManager和服务器进行交互。<br>结构图如下：<br>![[Pasted image 20230702212900.png]]</li></ul><h2 id="1、部署前准备工作："><a href="#1、部署前准备工作：" class="headerlink" title="1、部署前准备工作："></a>1、部署前准备工作：</h2><h3 id="服务器节点"><a href="#服务器节点" class="headerlink" title="服务器节点"></a>服务器节点</h3><table><thead><tr><th>NAME</th><th>NUMBER</th><th>描述</th></tr></thead><tbody><tr><td>Server</td><td>1</td><td>CM 中 Manager 节点</td></tr><tr><td>Agent</td><td>1+N</td><td>CM 中 其余 Node 节点</td></tr></tbody></table><h3 id="软件包准备"><a href="#软件包准备" class="headerlink" title="软件包准备"></a>软件包准备</h3><p>各服务器节点需要：</p><ul><li>JDK 1.8.0_161「1.8 即可，小版本皆可」</li><li>jdk-8u161-linux-x64.tar.gz<br>Server节点需要安装：</li><li>MySQL 5.7.25</li><li>mysql-5.7.25-1.el7.x86_64.rpm-bundle.tar</li><li>MySQL驱动</li><li>mysql-connector-java-5.1.47.jar</li></ul><p>CDH 安装需准备：</p><ul><li>cm6.3.1-redhat7.tar.gz</li><li>cloudera-manager-server-6.3.1-1466458.el7.x86_64.rpm</li><li>cloudera-manager-agent-6.3.1-1466458.el7.x86_64.rpm</li><li>cloudera-manager-daemons-6.3.1-1466458.el7.x86_64.rpm</li><li>allkeys.asc</li><li>parcel包</li><li>CDH-6.3.1-1.cdh6.3.1.p0.1470567-el7.parcel</li><li>CDH-6.3.2-1.cdh6.3.2.p0.1605554-el7.parcel.sha1</li><li>manifest.json</li></ul><h2 id="2、所有节点服务器初始配置"><a href="#2、所有节点服务器初始配置" class="headerlink" title="2、所有节点服务器初始配置"></a>2、所有节点服务器初始配置</h2><ul><li>❗️关闭并禁止开机自启如下服务：系统防火墙（<code>Firewalld</code>、<code>iptables</code>）、<code>NetworkManager</code><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl stop iptables</span><br><span class="line">systemctl stop NetworkManager</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">disable</span> firewalld</span><br><span class="line">systemctl <span class="built_in">disable</span> iptables</span><br><span class="line">systemctl <span class="built_in">disable</span> NetworkManager</span><br></pre></td></tr></table></figure></li><li>❗️关闭并禁用 <code>SELinux</code><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -i <span class="string">&#x27;s#SELINUX=enforcing#SELINUX=disabled#g&#x27;</span> /etc/selinux/config</span><br></pre></td></tr></table></figure></li><li>❗️永久修改各节点的 Hostname 必须防止变回默认<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hostnamectl set-hostname <span class="variable">$HOSTNAME</span></span><br></pre></td></tr></table></figure></li><li>❗️添加服务器之间本地域名解析 <code>/etc/hosts</code><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/hosts</span><br><span class="line"><span class="comment"># incloud itself</span></span><br><span class="line"><span class="comment"># 192.168.3.9 PHM01</span></span><br><span class="line"><span class="comment"># 192.168.3.10 PHM02</span></span><br><span class="line"><span class="comment"># 192.168.3.11 PHM03</span></span><br><span class="line"></span><br><span class="line">service network restart</span><br></pre></td></tr></table></figure></li><li>❗️配置仅使用物理内存，所有主机都需要 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">&quot;vm.swappiness=0&quot;</span> &gt;&gt;/etc/sysctl.conf &amp;&amp; sysctl -p</span><br></pre></td></tr></table></figure></li><li>❗️禁用透明页压缩，所有主机都需要<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">&quot;never&quot;</span> &gt;/sys/kernel/mm/transparent_hugepage/defrag</span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;never&quot;</span> &gt; /sys/kernel/mm/transparent_hugepage/enabled</span><br><span class="line"><span class="built_in">chmod</span> +x /etc/rc.local</span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;never&quot;</span> &gt;/sys/kernel/mm/transparent_hugepage/defrag<span class="string">&quot; &gt;&gt;/etc/rc.local</span></span><br><span class="line"><span class="string">echo &quot;</span>never<span class="string">&quot; &gt;/sys/kernel/mm/transparent_hugepage/enabled&quot;</span> &gt;&gt;/etc/rc.local</span><br><span class="line"><span class="built_in">tail</span> ‐2 /sys/kernel/mm/transparent_hugepage/defrag</span><br><span class="line"><span class="built_in">tail</span> ‐2 /sys/kernel/mm/transparent_hugepage/enabled</span><br><span class="line"><span class="built_in">tail</span> ‐2 /etc/rc.local</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">test</span> -f /sys/kernel/mm/transparent_hugepage/enabled; <span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> never &gt; /sys/kernel/mm/transparent_hugepage/enabled</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">test</span> -f /sys/kernel/mm/transparent_hugepage/defrag; <span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> never &gt; /sys/kernel/mm/transparent_hugepage/defrag</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 即时生效 </span></span><br><span class="line"><span class="built_in">echo</span> never &gt; /sys/kernel/mm/transparent_hugepage/defrag </span><br><span class="line"><span class="built_in">echo</span> never &gt; /sys/kernel/mm/transparent_hugepage/enabled </span><br><span class="line"><span class="comment"># 永久生效 </span></span><br><span class="line"><span class="comment"># /etc/rc.d/rc.local 中增加下列内容 </span></span><br><span class="line"><span class="built_in">echo</span> never &gt; /sys/kernel/mm/transparent_hugepage/defrag </span><br><span class="line"><span class="built_in">echo</span> never &gt; /sys/kernel/mm/transparent_hugepage/enabled </span><br><span class="line"><span class="comment"># 赋予rc.local文件执行权限 </span></span><br><span class="line"><span class="built_in">chmod</span> +x /etc/rc.d/rc.local </span><br><span class="line"><span class="comment"># 验证 </span></span><br><span class="line"><span class="built_in">cat</span> /sys/kernel/mm/transparent_hugepage/defrag </span><br><span class="line"><span class="built_in">cat</span> /sys/kernel/mm/transparent_hugepage/enabled</span><br></pre></td></tr></table></figure></li><li>❗️️ 配置个节点时间同步，确保各个节点时间误差最大不得高于 2s <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install ntp</span><br></pre></td></tr></table></figure><strong>server</strong><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 管理节点使用本地时钟源,执行以下语句即可</span></span><br><span class="line"><span class="built_in">cat</span> &lt;&lt;<span class="string">EOF&gt;/etc/ntp.conf</span></span><br><span class="line"><span class="string">driftfile /var/lib/ntp/drift</span></span><br><span class="line"><span class="string">restrict default kod nomodify notrap nopeer noquery</span></span><br><span class="line"><span class="string">restrict -6 default kod nomodify notrap nopeer noquery</span></span><br><span class="line"><span class="string">restrict 127.0.0.1</span></span><br><span class="line"><span class="string">restrict -6 ::1</span></span><br><span class="line"><span class="string">server 127.127.1.0 iburst</span></span><br><span class="line"><span class="string">includefile /etc/ntp/crypto/pw</span></span><br><span class="line"><span class="string">keys /etc/ntp/keys</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure><strong>agent</strong><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 其他节点的时钟源为管理节点，执行以下语句即可</span></span><br><span class="line"><span class="built_in">cat</span> &lt;&lt;<span class="string">EOF&gt;/etc/ntp.conf</span></span><br><span class="line"><span class="string">driftfile /var/lib/ntp/drift</span></span><br><span class="line"><span class="string">restrict default kod nomodify notrap nopeer noquery</span></span><br><span class="line"><span class="string">restrict -6 default kod nomodify notrap nopeer noquery</span></span><br><span class="line"><span class="string">restrict 127.0.0.1</span></span><br><span class="line"><span class="string">restrict -6 ::1</span></span><br><span class="line"><span class="string">server node1  iburst</span></span><br><span class="line"><span class="string">includefile /etc/ntp/crypto/pw</span></span><br><span class="line"><span class="string">keys /etc/ntp/keys</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure>强制设置时间，写入硬件时钟<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改各节点时间,时间为当前时间</span></span><br><span class="line"><span class="built_in">date</span> -s <span class="string">&quot;2023-06-30 13:11:07&quot;</span></span><br><span class="line"><span class="comment"># 写入硬件时钟</span></span><br><span class="line">hwclock -w</span><br></pre></td></tr></table></figure>启动服务，并设置开机自启动<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 各节点启动服务</span></span><br><span class="line">systemctl start ntpd &amp;&amp; systemctl <span class="built_in">enable</span> ntpd</span><br></pre></td></tr></table></figure></li><li>配置 Java 运行环境，建议安装 JDK 1.8.0_161（亦可选择安装 CDH 时，选择安装 CDH 自带的 Oracle JDK）<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># JDK 安装路径</span></span><br><span class="line"><span class="built_in">mkdir</span> -p /usr/local/java</span><br></pre></td></tr></table></figure>若操作系统安装有 OpenJDK 则移除系统原有的 JDK<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 各节点都需要进行</span></span><br><span class="line"><span class="comment"># 查找JDK</span></span><br><span class="line">rpm -aq|grep java</span><br><span class="line">rpm -aq|grep jdk</span><br><span class="line"><span class="comment"># 卸载JDK</span></span><br><span class="line">yum -y remove [上述查找结果的包名]</span><br></pre></td></tr></table></figure>安装  JDK 1.8<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在各节点上进行安装</span></span><br><span class="line"><span class="comment"># 创建java目录</span></span><br><span class="line"><span class="built_in">mkdir</span> /usr/java/</span><br><span class="line"><span class="comment"># 上传jdk目录下jdk-8u162-linux-x64.tar.gz到/usr/java目录 并解压</span></span><br><span class="line">tar -zxvf jdk-8u162-linux-x64.tar.gz -C /usr/java</span><br><span class="line"><span class="comment"># ️ 修改 jdk 所属用户用户组</span></span><br><span class="line"><span class="built_in">chown</span> root:root jdk-8u161-linux-x64</span><br><span class="line"><span class="comment"># 配置环境变量</span></span><br><span class="line">vim ~/.bashrc</span><br><span class="line"><span class="comment"># 在后面追加下面三行</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/java/jdk1.8.0_162</span><br><span class="line"><span class="built_in">export</span> JRE_HOME=<span class="variable">$&#123;JAVA_HOME&#125;</span>/jre</span><br><span class="line"><span class="built_in">export</span> CLASSPATH=.:<span class="variable">$&#123;JAVA_HOME&#125;</span>/lib:<span class="variable">$&#123;JRE_HOME&#125;</span>/lib</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$&#123;JAVA_HOME&#125;</span>/bin:<span class="variable">$PATH</span></span><br><span class="line"><span class="comment"># 最后刷新环境变量</span></span><br><span class="line"><span class="built_in">source</span> ~/.bashrc</span><br><span class="line"><span class="comment"># 查看 Java 是否安装成功</span></span><br><span class="line">java -version</span><br></pre></td></tr></table></figure></li></ul><h2 id="3、Server-节点"><a href="#3、Server-节点" class="headerlink" title="3、Server 节点"></a>3、Server 节点</h2><p>若操作系统安装有 mariadb 则手工移除mariadb数据库<br>亦可不移除，在安装 mysql 是会自动处理</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">  rpm -qa | grep mariadb</span><br><span class="line"><span class="comment"># 结果应为 mariadb-libs-5.5.56-2.el7.x86_64</span></span><br><span class="line"><span class="comment"># 卸载</span></span><br><span class="line">  rpm -e --nodeps mariadb-libs-5.5.56-2.el7.x86_64</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在 server 上安装 mysql 服务</span></span><br><span class="line"><span class="comment"># 上传repo目录下mysql-5.7.25-1.el7.x86_64.rpm-bundle.tar安装包到 /home/cdh/ 目录 并解压</span></span><br><span class="line">  tar -xvf mysql-5.7.25-1.el7.x86_64.rpm-bundle.tar</span><br><span class="line"><span class="comment"># 进入解压目录 安装 mysql</span></span><br><span class="line">  yum install net-tools</span><br><span class="line">  rpm -ivh mysql-community-server-5.7.25-1.el7.x86_64.rpm mysql-community-client-5.7.25-1.el7.x86_64.rpm mysql-community-common-5.7.25-1.el7.x86_64.rpm mysql-community-libs-5.7.25-1.el7.x86_64.rpm mysql-community-libs-compat-5.7.25-1.el7.x86_64.rpm</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在 server上启动 mysql 服务,并设置自启动</span></span><br><span class="line">  systemctl start mysqld</span><br><span class="line">  systemctl <span class="built_in">enable</span> mysqld</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在node1 上初始化mysql</span></span><br><span class="line"><span class="comment"># 获得初始密码</span></span><br><span class="line">  grep <span class="string">&#x27;temporary password&#x27;</span> /var/log/mysqld.log</span><br><span class="line"><span class="comment"># 使用初始密码登录</span></span><br><span class="line">  mysql -u root -p</span><br><span class="line"><span class="comment"># 修改初始密码</span></span><br><span class="line">  <span class="comment"># show variables like &#x27;validate_password%&#x27;;# 查看密码验证策略</span></span><br><span class="line">  <span class="built_in">set</span> global validate_password_policy=0;<span class="comment"># 设置密码验证策略为低</span></span><br><span class="line">  <span class="built_in">set</span> global validate_password_mixed_case_count=0;<span class="comment"># 设置密码至少要包含的大小写字母个数</span></span><br><span class="line">  <span class="built_in">set</span> global validate_password_number_count=0;<span class="comment"># 设置密码至少要包含的数字个数</span></span><br><span class="line">  <span class="built_in">set</span> global validate_password_special_char_count=0;<span class="comment"># 设置密码至少要包含的特殊字符个数</span></span><br><span class="line">  <span class="built_in">set</span> global validate_password_length=3; <span class="comment"># 设置密码最小长度为3</span></span><br><span class="line">  ALTER USER <span class="string">&#x27;root&#x27;</span>@<span class="string">&#x27;localhost&#x27;</span> IDENTIFIED BY <span class="string">&#x27;123456&#x27;</span>;</span><br><span class="line">  grant all privileges on *.* to <span class="string">&#x27;root&#x27;</span>@<span class="string">&#x27;localhost&#x27;</span> identified by <span class="string">&#x27;123456&#x27;</span> with grant option;</span><br><span class="line">  grant all privileges on *.* to <span class="string">&#x27;root&#x27;</span>@<span class="string">&#x27;%&#x27;</span> identified by <span class="string">&#x27;123456&#x27;</span> with grant option;</span><br><span class="line">  flush privileges;</span><br><span class="line">  quit;</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在各节点上执行</span></span><br><span class="line"><span class="comment"># 上传 repo 目录下 mysql 连接 mysql-connector-java-5.1.47.jar 到 /usr/share/java</span></span><br><span class="line">  ️ <span class="built_in">mkdir</span> -p /usr/share/java</span><br><span class="line"><span class="comment"># 重命名</span></span><br><span class="line">  ️ <span class="built_in">mv</span> mysql-connector-java-5.1.47.jar mysql-connector-java.jar</span><br><span class="line"><span class="comment"># 授权</span></span><br><span class="line">  ️ <span class="built_in">chmod</span> 777 mysql-connector-java.jar</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install cloudera-manager-agent cloudera-manager-daemons cloudera-manager-server</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#执行数据库脚本</span></span></span><br><span class="line">cd /opt/cloudera/cm/schema</span><br><span class="line">./scm_prepare_database.sh mysql -uroot -p scm scm scm</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/cloudera-scm-agent/config.ini</span><br><span class="line">server_host=<span class="variable">$SERVERHOSTNAME</span></span><br></pre></td></tr></table></figure><h2 id="4、Agent-节点"><a href="#4、Agent-节点" class="headerlink" title="4、Agent 节点"></a>4、Agent 节点</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install cloudera-manager-agent cloudera-manager-daemo</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/cloudera-scm-agent/config.ini</span><br><span class="line">server_host=<span class="variable">$SERVERHOSTNAME</span></span><br></pre></td></tr></table></figure><h2 id="5、开始启动服务"><a href="#5、开始启动服务" class="headerlink" title="5、开始启动服务"></a>5、开始启动服务</h2><h3 id="server"><a href="#server" class="headerlink" title="server"></a><strong>server</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">systemctl start cloudera-scm-server</span><br><span class="line">systemctl <span class="built_in">enable</span> cloudera-scm-server</span><br><span class="line">systemctl start cloudera-scm-agent</span><br><span class="line">systemctl <span class="built_in">enable</span> cloudera-scm-agent</span><br></pre></td></tr></table></figure><h3 id="agent"><a href="#agent" class="headerlink" title="agent"></a><strong>agent</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl start cloudera-scm-agent</span><br><span class="line">systemctl <span class="built_in">enable</span> cloudera-scm-agent</span><br></pre></td></tr></table></figure><h2 id="6、Web操作"><a href="#6、Web操作" class="headerlink" title="6、Web操作"></a>6、Web操作</h2><p><a href="http://192.168.3.11:7180/">http://192.168.3.11:7180</a><br>默认用户名为 admin<br>默认密码为 admin<br>![[Pasted image 20230705102314.png]]</p><h2 id="附录1-组件建表语句"><a href="#附录1-组件建表语句" class="headerlink" title="附录1 组件建表语句"></a>附录1 组件建表语句</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">mysql -uroot -p123456</span><br><span class="line"></span><br><span class="line">  <span class="comment">##给scm授权</span></span><br><span class="line">  grant all privileges on *.* to <span class="string">&#x27;scm&#x27;</span>@<span class="string">&#x27;localhost&#x27;</span> identified by <span class="string">&#x27;scm&#x27;</span> with grant option;</span><br><span class="line">  grant all privileges on *.* to <span class="string">&#x27;scm&#x27;</span>@<span class="string">&#x27;%&#x27;</span> identified by <span class="string">&#x27;scm&#x27;</span> with grant option;</span><br><span class="line"></span><br><span class="line">  <span class="comment">##创建hive数据库</span></span><br><span class="line">  create database hive DEFAULT CHARSET utf8 COLLATE utf8_general_ci;</span><br><span class="line"></span><br><span class="line">  <span class="comment">##给hive授权</span></span><br><span class="line">  grant all privileges on *.* to <span class="string">&#x27;hive&#x27;</span>@<span class="string">&#x27;localhost&#x27;</span> identified by <span class="string">&#x27;hive&#x27;</span> with grant option;</span><br><span class="line">  grant all privileges on *.* to <span class="string">&#x27;hive&#x27;</span>@<span class="string">&#x27;%&#x27;</span> identified by <span class="string">&#x27;hive&#x27;</span> with grant option;</span><br><span class="line"></span><br><span class="line">  <span class="comment">##创建oozie数据库</span></span><br><span class="line">  create database oozie DEFAULT CHARSET utf8 COLLATE utf8_general_ci;</span><br><span class="line"></span><br><span class="line">  <span class="comment">##给oozie授权</span></span><br><span class="line">  grant all privileges on *.* to <span class="string">&#x27;oozie&#x27;</span>@<span class="string">&#x27;localhost&#x27;</span> identified by <span class="string">&#x27;oozie&#x27;</span> with grant option;</span><br><span class="line">  grant all privileges on *.* to <span class="string">&#x27;oozie&#x27;</span>@<span class="string">&#x27;%&#x27;</span> identified by <span class="string">&#x27;oozie&#x27;</span> with grant option;</span><br><span class="line"></span><br><span class="line">  <span class="comment">##创建hue数据库</span></span><br><span class="line">  create database hue DEFAULT CHARSET utf8 COLLATE utf8_general_ci;</span><br><span class="line"></span><br><span class="line">  <span class="comment">##给hub授权</span></span><br><span class="line">  grant all privileges on *.* to <span class="string">&#x27;hue&#x27;</span>@<span class="string">&#x27;localhost&#x27;</span> identified by <span class="string">&#x27;hue&#x27;</span> with grant option;</span><br><span class="line">  grant all privileges on *.* to <span class="string">&#x27;hue&#x27;</span>@<span class="string">&#x27;%&#x27;</span> identified by <span class="string">&#x27;hue&#x27;</span> with grant option;</span><br><span class="line"></span><br><span class="line">  <span class="comment">##刷新权限</span></span><br><span class="line">  flush privileges;</span><br><span class="line"></span><br><span class="line">  <span class="comment">##退出</span></span><br><span class="line">  quit;</span><br></pre></td></tr></table></figure><h2 id="附录2-CDH各组件详细信息"><a href="#附录2-CDH各组件详细信息" class="headerlink" title="附录2 CDH各组件详细信息"></a>附录2 CDH各组件详细信息</h2><table><thead><tr><th></th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td>Cloudera Manager Agent</td><td>6.3.1</td><td>所有主机</td><td>1466458.el7</td><td>不适用</td></tr><tr><td>Cloudera Manager Management Daemon</td><td>6.3.1</td><td>所有主机</td><td>1466458.el7</td><td>不适用</td></tr><tr><td>Flume NG</td><td>1.9.0+cdh6.3.2</td><td>所有主机</td><td>1605554</td><td>CDH 6</td></tr><tr><td>Hadoop</td><td>3.0.0+cdh6.3.2</td><td>所有主机</td><td>1605554</td><td>CDH 6</td></tr><tr><td>HDFS</td><td>3.0.0+cdh6.3.2</td><td>所有主机</td><td>1605554</td><td>CDH 6</td></tr><tr><td>HttpFS</td><td>3.0.0+cdh6.3.2</td><td>所有主机</td><td>1605554</td><td>CDH 6</td></tr><tr><td>hadoop-kms</td><td>3.0.0+cdh6.3.2</td><td>所有主机</td><td>1605554</td><td>CDH 6</td></tr><tr><td>MapReduce 2</td><td>3.0.0+cdh6.3.2</td><td>所有主机</td><td>1605554</td><td>CDH 6</td></tr><tr><td>YARN</td><td>3.0.0+cdh6.3.2</td><td>所有主机</td><td>1605554</td><td>CDH 6</td></tr><tr><td>HBase</td><td>2.1.0+cdh6.3.2</td><td>所有主机</td><td>1605554</td><td>CDH 6</td></tr><tr><td>Lily HBase Indexer</td><td>1.5+cdh6.3.2</td><td>所有主机</td><td>1605554</td><td>CDH 6</td></tr><tr><td>Hive</td><td>2.1.1+cdh6.3.2</td><td>所有主机</td><td>1605554</td><td>CDH 6</td></tr><tr><td>HCatalog</td><td>2.1.1+cdh6.3.2</td><td>所有主机</td><td>1605554</td><td>CDH 6</td></tr><tr><td>Hue</td><td>4.2.0+cdh6.3.2</td><td>所有主机</td><td>1605554</td><td>CDH 6</td></tr><tr><td>Impala</td><td>3.2.0+cdh6.3.2</td><td>所有主机</td><td>1605554</td><td>CDH 6</td></tr><tr><td>Java 8</td><td>1.8.0_162</td><td>所有主机</td><td>不可用</td><td>不适用</td></tr><tr><td>Kafka</td><td>2.2.1+cdh6.3.2</td><td>所有主机</td><td>1605554</td><td>CDH 6</td></tr><tr><td>Kite（仅限 CDH 5 ）</td><td>1.0.0+cdh6.3.2</td><td>所有主机</td><td>1605554</td><td>CDH 6</td></tr><tr><td>kudu</td><td>1.10.0+cdh6.3.2</td><td>所有主机</td><td>1605554</td><td>CDH 6</td></tr><tr><td>Oozie</td><td>5.1.0+cdh6.3.2</td><td>所有主机</td><td>1605554</td><td>CDH 6</td></tr><tr><td>Parquet</td><td>1.9.0+cdh6.3.2</td><td>所有主机</td><td>1605554</td><td>CDH 6</td></tr><tr><td>Pig</td><td>0.17.0+cdh6.3.2</td><td>所有主机</td><td>1605554</td><td>CDH 6</td></tr><tr><td>sentry</td><td>2.1.0+cdh6.3.2</td><td>所有主机</td><td>1605554</td><td>CDH 6</td></tr><tr><td>Solr</td><td>7.4.0+cdh6.3.2</td><td>所有主机</td><td>1605554</td><td>CDH 6</td></tr><tr><td>spark</td><td>2.4.0+cdh6.3.2</td><td>所有主机</td><td>1605554</td><td>CDH 6</td></tr><tr><td>Sqoop</td><td>1.4.7+cdh6.3.2</td><td>所有主机</td><td>1605554</td><td>CDH 6</td></tr><tr><td>ZooKeeper</td><td>3.4.5+cdh6.3.2</td><td>所有主机</td><td>1605554</td><td>CDH 6</td></tr></tbody></table><h1 id="二、"><a href="#二、" class="headerlink" title="二、"></a>二、</h1>]]></content>
      
      
      <categories>
          
          <category> bigdata </category>
          
      </categories>
      
      
        <tags>
            
            <tag> bigdata </tag>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux 调整根目录和home目录的空间大小</title>
      <link href="/2024/01/15/Linux%E4%B8%8B%E8%B0%83%E6%95%B4%E6%A0%B9%E7%9B%AE%E5%BD%95%E5%92%8Chome%E7%9B%AE%E5%BD%95%E7%9A%84%E7%A9%BA%E9%97%B4%E5%A4%A7%E5%B0%8F/"/>
      <url>/2024/01/15/Linux%E4%B8%8B%E8%B0%83%E6%95%B4%E6%A0%B9%E7%9B%AE%E5%BD%95%E5%92%8Chome%E7%9B%AE%E5%BD%95%E7%9A%84%E7%A9%BA%E9%97%B4%E5%A4%A7%E5%B0%8F/</url>
      
        <content type="html"><![CDATA[<p>当安装完 Linux 操作系统，发现磁盘分区大小错误，或者后期使用过程发现 &#x2F;home 还剩余很多空间，&#x2F; 下空间不足，需要将 &#x2F;home 下空间重新分配给 &#x2F;目录下<br>1、查看分区空间和格式</p><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">root<span class="variable">@mongodb</span>~<span class="comment"># df -hT  </span></span><br><span class="line">文件系统 类型 容量 已用 可用 已用% 挂载点  </span><br><span class="line"><span class="regexp">/dev/mapper</span><span class="regexp">/centos-root xfs 50G 49G 1.4G 98% /</span>  </span><br><span class="line">devtmpfs devtmpfs <span class="number">5.8</span>G <span class="number">0</span> <span class="number">5.8</span>G <span class="number">0</span>% <span class="regexp">/dev  </span></span><br><span class="line"><span class="regexp">tmpfs tmpfs 5.8G 0 5.8G 0% /dev</span><span class="regexp">/shm  </span></span><br><span class="line"><span class="regexp">tmpfs tmpfs 5.8G 602M 5.3G 11% /run</span>  </span><br><span class="line">tmpfs tmpfs <span class="number">5.8</span>G <span class="number">0</span> <span class="number">5.8</span>G <span class="number">0</span>% <span class="regexp">/sys/fs</span><span class="regexp">/cgroup  </span></span><br><span class="line"><span class="regexp">/dev</span><span class="regexp">/sda1 xfs 1014M 153M 862M 16% /boot</span>  </span><br><span class="line"><span class="regexp">/dev/mapper</span><span class="regexp">/centos-home xfs 44G 36M 44G 1% /home</span>  </span><br><span class="line">tmpfs tmpfs <span class="number">1.2</span>G <span class="number">0</span> <span class="number">1.2</span>G <span class="number">0</span>% <span class="regexp">/run/user</span><span class="regexp">/0</span></span><br></pre></td></tr></table></figure><p>这里要将 &#x2F;home 的空闲空间分给 &#x2F; 目录一部分</p><p>可以看到 &#x2F;home 分区是 xfs 格式</p><p>1）ext2&#x2F;ext3&#x2F;ext4文件系统的调整命令是resize2fs（增大和减小都支持）</p><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">lvextend -L <span class="number">120</span>G /dev/mapper/centos-home <span class="comment">//增大至120G  </span></span><br><span class="line">lvextend -L +<span class="number">20</span>G /dev/mapper/centos-home <span class="comment">//增加20G  </span></span><br><span class="line">lvreduce -L <span class="number">50</span>G /dev/mapper/centos-home <span class="comment">//减小至50G  </span></span><br><span class="line">lvreduce -L <span class="number">-8</span>G /dev/mapper/centos-home <span class="comment">//减小8G  </span></span><br><span class="line">resize2fs /dev/mapper/centos-home <span class="comment">//执行调整</span></span><br></pre></td></tr></table></figure><p>2）xfs文件系统的调整命令是xfs_growfs（只支持增大）</p><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">lvextend -L <span class="number">120</span>G /dev/mapper/centos-home <span class="comment">//增大至120G  </span></span><br><span class="line">lvextend -L +<span class="number">20</span>G /dev/mapper/centos-home <span class="comment">//增加20G  </span></span><br><span class="line">xfs_growfs /dev/mapper/centos-home <span class="comment">//执行调整</span></span><br></pre></td></tr></table></figure><p>xfs文件系统只支持增大分区空间的情况，不支持减小的情况。</p><p>硬要减小的话，只能在减小后将逻辑分区重新通过 mkfs.xfs 命令重新格式化才能挂载上，这样的话这个逻辑分区上原来的数据就丢失了。如果有重要文件，禁用或移除文件</p><p>2、卸载 &#x2F;home 分区</p><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root<span class="variable">@mongodb</span>-<span class="number">1</span> /]<span class="comment"># umount /home</span></span><br><span class="line"></span><br><span class="line">[root<span class="variable">@mongodb</span>-<span class="number">1</span> /]<span class="comment">#   </span></span><br><span class="line">文件系统 容量 已用 可用 已用% 挂载点  </span><br><span class="line"><span class="regexp">/dev/mapper</span><span class="regexp">/centos-root 50G 49G 1.4G 98% /</span>  </span><br><span class="line">devtmpfs <span class="number">5.8</span>G <span class="number">0</span> <span class="number">5.8</span>G <span class="number">0</span>% <span class="regexp">/dev  </span></span><br><span class="line"><span class="regexp">tmpfs 5.8G 0 5.8G 0% /dev</span><span class="regexp">/shm  </span></span><br><span class="line"><span class="regexp">tmpfs 5.8G 602M 5.3G 11% /run</span>  </span><br><span class="line">tmpfs <span class="number">5.8</span>G <span class="number">0</span> <span class="number">5.8</span>G <span class="number">0</span>% <span class="regexp">/sys/fs</span><span class="regexp">/cgroup  </span></span><br><span class="line"><span class="regexp">/dev</span><span class="regexp">/sda1 1014M 153M 862M 16% /boot</span>  </span><br><span class="line">tmpfs <span class="number">1.2</span>G <span class="number">0</span> <span class="number">1.2</span>G <span class="number">0</span>% <span class="regexp">/run/user</span><span class="regexp">/0</span></span><br></pre></td></tr></table></figure><p>卸载成功<br>3、将 &#x2F;home 分区减小200G（根据自己实际情况设定大小） ：</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@mongodb-1 /]# lvreduce -L -40G /dev/mapper/centos-home  </span><br><span class="line"><span class="symbol">WARNING: </span>Reducing active logical volume to &lt; 3.12 GiB.  </span><br><span class="line">THIS MAY DESTROY YOUR DATA (filesystem etc.)  </span><br><span class="line">Do you really want to reduce centos/home? [y/n]: y  </span><br><span class="line">Size of logical volume centos/home changed from &lt;43.12 GiB (11038 extents) to &lt; 3.12 GiB (798 extents).  </span><br><span class="line">Logical volume centos/home successfully resized.</span><br></pre></td></tr></table></figure><p>因为 xfs文件系统不能执行分区减小的调整！所以这里我们要执行格式化操作，</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@mongodb-1 /]# mkfs.xfs /dev/mapper/centos-home -f  </span><br><span class="line"><span class="attribute">meta-data</span>=/dev/mapper/centos-home <span class="attribute">isize</span>=512 <span class="attribute">agcount</span>=4, <span class="attribute">agsize</span>=204288 blks  </span><br><span class="line">= <span class="attribute">sectsz</span>=512 <span class="attribute">attr</span>=2, <span class="attribute">projid32bit</span>=1  </span><br><span class="line">= <span class="attribute">crc</span>=1 <span class="attribute">finobt</span>=0, <span class="attribute">sparse</span>=0  </span><br><span class="line">data = <span class="attribute">bsize</span>=4096 <span class="attribute">blocks</span>=817152, <span class="attribute">imaxpct</span>=25  </span><br><span class="line">= <span class="attribute">sunit</span>=0 <span class="attribute">swidth</span>=0 blks  </span><br><span class="line">naming =version 2 <span class="attribute">bsize</span>=4096 <span class="attribute">ascii-ci</span>=0 <span class="attribute">ftype</span>=1  </span><br><span class="line">log =internal log <span class="attribute">bsize</span>=4096 <span class="attribute">blocks</span>=2560, <span class="attribute">version</span>=2  </span><br><span class="line">= <span class="attribute">sectsz</span>=512 <span class="attribute">sunit</span>=0 blks, <span class="attribute">lazy-count</span>=1  </span><br><span class="line">realtime =none <span class="attribute">extsz</span>=4096 <span class="attribute">blocks</span>=0, <span class="attribute">rtextents</span>=0</span><br></pre></td></tr></table></figure><p>重新挂载 &#x2F;home 分区：</p><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mount /dev/mapper/centos-home /home/</span><br></pre></td></tr></table></figure><p>5、将上面空余的 200G 分到 &#x2F; 分区下</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@mongodb-1 /]# lvextend -L +40G /dev/mapper/centos-root  </span><br><span class="line">Size of logical volume centos/root changed <span class="keyword">from</span> 50.00 GiB (12800 extents) <span class="keyword">to</span> 90.00 GiB (23040 extents).  </span><br><span class="line">Logical volume centos/root successfully resized.  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@mongodb-1 /]# xfs_growfs /dev/mapper/centos-root  </span><br><span class="line"><span class="attribute">meta-data</span>=/dev/mapper/centos-root <span class="attribute">isize</span>=512 <span class="attribute">agcount</span>=4, <span class="attribute">agsize</span>=3276800 blks  </span><br><span class="line">= <span class="attribute">sectsz</span>=512 <span class="attribute">attr</span>=2, <span class="attribute">projid32bit</span>=1  </span><br><span class="line">= <span class="attribute">crc</span>=1 <span class="attribute">finobt</span>=0 <span class="attribute">spinodes</span>=0  </span><br><span class="line">data = <span class="attribute">bsize</span>=4096 <span class="attribute">blocks</span>=13107200, <span class="attribute">imaxpct</span>=25  </span><br><span class="line">= <span class="attribute">sunit</span>=0 <span class="attribute">swidth</span>=0 blks  </span><br><span class="line">naming =version 2 <span class="attribute">bsize</span>=4096 <span class="attribute">ascii-ci</span>=0 <span class="attribute">ftype</span>=1  </span><br><span class="line">log =internal <span class="attribute">bsize</span>=4096 <span class="attribute">blocks</span>=6400, <span class="attribute">version</span>=2  </span><br><span class="line">= <span class="attribute">sectsz</span>=512 <span class="attribute">sunit</span>=0 blks, <span class="attribute">lazy-count</span>=1  </span><br><span class="line">realtime =none <span class="attribute">extsz</span>=4096 <span class="attribute">blocks</span>=0, <span class="attribute">rtextents</span>=0  </span><br><span class="line">data blocks changed <span class="keyword">from</span> 13107200 <span class="keyword">to</span> 23592960</span><br></pre></td></tr></table></figure><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root<span class="variable">@mongodb</span>-<span class="number">1</span> /]<span class="comment"># df -h  </span></span><br><span class="line">文件系统 容量 已用 可用 已用% 挂载点  </span><br><span class="line"><span class="regexp">/dev/mapper</span><span class="regexp">/centos-root 90G 49G 42G 55% /</span>  </span><br><span class="line">devtmpfs <span class="number">5.8</span>G <span class="number">0</span> <span class="number">5.8</span>G <span class="number">0</span>% <span class="regexp">/dev  </span></span><br><span class="line"><span class="regexp">tmpfs 5.8G 0 5.8G 0% /dev</span><span class="regexp">/shm  </span></span><br><span class="line"><span class="regexp">tmpfs 5.8G 602M 5.3G 11% /run</span>  </span><br><span class="line">tmpfs <span class="number">5.8</span>G <span class="number">0</span> <span class="number">5.8</span>G <span class="number">0</span>% <span class="regexp">/sys/fs</span><span class="regexp">/cgroup  </span></span><br><span class="line"><span class="regexp">/dev</span><span class="regexp">/sda1 1014M 153M 862M 16% /boot</span>  </span><br><span class="line">tmpfs <span class="number">1.2</span>G <span class="number">0</span> <span class="number">1.2</span>G <span class="number">0</span>% <span class="regexp">/run/user</span><span class="regexp">/0  </span></span><br><span class="line"><span class="regexp">/dev</span><span class="regexp">/mapper/centos</span>-home <span class="number">3.2</span>G <span class="number">33</span>M <span class="number">3.1</span>G <span class="number">2</span>% <span class="regexp">/home</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux 搭建离线yum源</title>
      <link href="/2024/01/15/%E6%90%AD%E5%BB%BA%E7%A6%BB%E7%BA%BFyum%E6%BA%90/"/>
      <url>/2024/01/15/%E6%90%AD%E5%BB%BA%E7%A6%BB%E7%BA%BFyum%E6%BA%90/</url>
      
        <content type="html"><![CDATA[<p>工作中，虚拟机通常都是部署在内网环境中，无法连接互联网，因此无法使用互联网上的YUM源。<br>经常会遇到系统ISO镜像中软件包缺失，系统软件补丁无法升级，第三方软件包无法安装等情况。</p><p>本文通过搭建离线YUM源，解决了上述问题。</p><h4 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h4><p>准备一台可以联网的linux机器<br>安装创建YUM仓库的软件工具</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost yum.repos.d]# yum install createrepo yum-utils -y</span><br></pre></td></tr></table></figure><p>![[Pasted image 20230630202201.png]]<br>移除初始的其他镜像源</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# cd /etc/yum.repos.d/  </span><br><span class="line">[root@localhost yum.repos.d]# mkdir bak  </span><br><span class="line">[root@localhost yum.repos.d]# mv * bak</span><br></pre></td></tr></table></figure><p>![[Pasted image 20230630202240.png]]<br>下载阿里云的repo文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]<span class="comment"># wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo</span></span><br><span class="line">[root@localhost ~]<span class="comment"># wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo</span></span><br></pre></td></tr></table></figure><pre><code>什么是yum源repo文件，repo文件是yum源（软件仓库）的配置文件，通常一个repo文件定义了一个或者多个软件仓库的配置内容，例如将从哪里下载需要安装或者升级的软件包，repo文件中的设置内容将被yum读取和应用。</code></pre><p>![[Pasted image 20230630203607.png]]<br>![[Pasted image 20230630202353.png]]<br>构建YUM源缓存</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# yum clean all   #清除当前的yum源缓存</span><br><span class="line">(base) [root@localhost yum.repos.d]# yum install -y epel-release</span><br><span class="line">[root@localhost ~]#  yum makecache  #重新生成缓存</span><br><span class="line">[root@localhost ~]#  yum repolist   #查看当前可用的YUM源</span><br></pre></td></tr></table></figure><p>![[Pasted image 20230630202516.png]]<br>![[Pasted image 20230630205239.png]]<br>安装并启用httpd服务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost /]# yum install httpd</span><br><span class="line">[root@localhost /]# systemctl start httpd</span><br><span class="line">[root@localhost /]# systemctl status httpd</span><br><span class="line">● httpd.service - The Apache HTTP Server</span><br><span class="line">Loaded: loaded (/usr/lib/systemd/system/httpd.service; disabled; vendor preset: disabled)</span><br><span class="line">Active: active (running) since Wed 2021-06-30 00:49:06 CST; 5s ago</span><br><span class="line">....output omitted....</span><br></pre></td></tr></table></figure><p>![[Pasted image 20230630203747.png]]<br>![[Pasted image 20230630203826.png]]</p><h4 id="同步阿里云的YUM源到本地"><a href="#同步阿里云的YUM源到本地" class="headerlink" title="同步阿里云的YUM源到本地"></a>同步阿里云的YUM源到本地</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"> #</span><span class="language-bash">reposync根据之前下载的repo文件下载rpm包到指定文件夹,</span> </span><br><span class="line"><span class="meta prompt_"> #</span><span class="language-bash">这个服务器home文件夹下的空间比较多，同步下载到 /home路径下，<span class="built_in">mkdir</span> /phm/yum</span></span><br><span class="line">[root@localhost /]# cd /home/phm/yum</span><br><span class="line">[root@localhost yum]# reposync -r base </span><br><span class="line">[root@localhost yum]# reposync -r extras</span><br><span class="line">[root@localhost yum]# reposync -r updates</span><br><span class="line">[root@localhost yum]# reposync -r epel</span><br></pre></td></tr></table></figure><p>![[Pasted image 20230701115938.png]]<br>![[Pasted image 20230701120738.png]]</p><h4 id="创建本地YUM仓库"><a href="#创建本地YUM仓库" class="headerlink" title="创建本地YUM仓库"></a>创建本地YUM仓库</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">为本地yum仓库，生成新的repo文件</span></span><br><span class="line">[root@localhost yum]# cd /home/phm/yum/base</span><br><span class="line">[root@localhost yum]# createrepo ./</span><br><span class="line">[root@localhost yum]# cd /home/phm/yum/extras</span><br><span class="line">[root@localhost yum]# createrepo ./</span><br><span class="line">[root@localhost yum]# cd /home/phm/yum/updates</span><br><span class="line">[root@localhost yum]# createrepo ./</span><br><span class="line">[root@localhost yum]# cd /home/phm/yum/epel</span><br><span class="line">[root@localhost yum]# createrepo ./</span><br></pre></td></tr></table></figure><p>![[Pasted image 20230702202732.png]]<br>![[Pasted image 20230702202807.png]]</p><h4 id="更新YUM仓库"><a href="#更新YUM仓库" class="headerlink" title="更新YUM仓库"></a>更新YUM仓库</h4><p>到此本地YUM仓库搭建完成，<strong>如果在对应的仓库中加入新的软件包时</strong>，需要更新仓库。本文中不涉及。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ]#  createrepo --update /var/www/html/yum/epel</span><br></pre></td></tr></table></figure><h4 id="离线YUM仓库的使用"><a href="#离线YUM仓库的使用" class="headerlink" title="离线YUM仓库的使用"></a>离线YUM仓库的使用</h4><p>将本机&#x2F;var&#x2F;www&#x2F;html&#x2F;yum目录通过sftp等客户端工具保存到移动硬盘，之后再上传到现有的yum源主机对应目录中。更新客户端主机的yum源配置，指向新的YUM仓库目录，即可正常使用该YUM仓库。配置维护域ip地址，确保与其他主机网络正常通信。配置客户端主机的yum源配置文件，将YUM仓库指向该主机，即可正常使用该YUM仓库。客户端yum源配置文件如下：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3种文件获取方式</span></span><br><span class="line"><span class="comment"># 1. HTTP</span></span><br><span class="line">baseurl=http:<span class="regexp">//</span><span class="number">10.16</span>.<span class="number">9.34</span><span class="regexp">/yum/</span>base</span><br><span class="line"><span class="comment"># 2. 本地文件获取</span></span><br><span class="line">baseurl=file:<span class="regexp">//</span><span class="regexp">/home/yum</span>/base</span><br><span class="line"><span class="comment"># 3. FTP</span></span><br><span class="line">baseurl=ftp:<span class="regexp">//</span><span class="number">10.16</span>.<span class="number">9.34</span><span class="regexp">/yum/</span>base</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">[root@client yum.repos.d]# cd /etc/yum.repos.d</span><br><span class="line">[root@client yum.repos.d]# mkdir bak ; mv * bak</span><br><span class="line">[root@client yum.repos.d]# vi http.repo</span><br><span class="line">[base]</span><br><span class="line">name=RHEL- - Base - http</span><br><span class="line">baseurl=http://10.16.9.34/centos/yum/base</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br><span class="line"></span><br><span class="line">[updates]</span><br><span class="line">name=RHEL- - updates - http</span><br><span class="line">baseurl=http://10.16.9.34/centos/yum/updates</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br><span class="line"></span><br><span class="line">[epel]</span><br><span class="line">name=RHEL- - epel - http</span><br><span class="line">baseurl=http://10.16.9.34/centos/yum/epel</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br><span class="line"></span><br><span class="line">[extras]</span><br><span class="line">name=RHEL- - extras - http</span><br><span class="line">baseurl=http://10.16.9.34/centos/yum/extras</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br><span class="line"></span><br><span class="line">[root@client yum.repos.d]# yum clean all</span><br><span class="line">[root@client yum.repos.d]# yum makecache</span><br><span class="line">[root@client yum.repos.d]# yum list | grep kernel</span><br><span class="line">kernel.x86_64                            3.10.0-1160.el7               @anaconda</span><br><span class="line">kernel-tools.x86_64                      3.10.0-1160.el7               @anaconda</span><br><span class="line">abrt-addon-kerneloops.x86_64             2.1.11-60.el7.centos          base</span><br><span class="line">kernel-debug-devel.x86_64                3.10.0-1160.31.1.el7          updates</span><br><span class="line">kernel-devel.x86_64                      3.10.0-1160.31.1.el7          updates</span><br><span class="line">[root@client yum.repos.d]# yum install kernel</span><br><span class="line">Loaded plugins: fastestmirror</span><br><span class="line">Loading mirror speeds from cached hostfile</span><br><span class="line">Resolving Dependencies</span><br><span class="line"><span class="meta prompt_">--&gt; </span><span class="language-bash">Running transaction check</span></span><br><span class="line"><span class="meta prompt_">---&gt; </span><span class="language-bash">Package kernel.x86_64 0:3.10.0-1160.31.1.el7 will be installed</span></span><br><span class="line"><span class="meta prompt_">--&gt; </span><span class="language-bash">Finished Dependency Resolution</span></span><br><span class="line">.....output ommitted.........</span><br><span class="line">Transaction test succeeded</span><br><span class="line">Running transaction</span><br><span class="line">  Installing : kernel-3.10.0-1160.31.1.el7.x86_64                              </span><br><span class="line">         </span><br><span class="line">Complete!</span><br><span class="line">客户端测试安装kernel软件包，成功安装。</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>![[Pasted image 20230712131018.png]]</p><h3 id="超融合主机磁盘I-O"><a href="#超融合主机磁盘I-O" class="headerlink" title="超融合主机磁盘I&#x2F;O"></a>超融合主机磁盘I&#x2F;O</h3><p>60G 的yum文件，从9:20 – 12:55<br>![[Pasted image 20230712131354.png]]</p>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python环境离线安装</title>
      <link href="/2024/01/10/Python%20%E7%8E%AF%E5%A2%83%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85/"/>
      <url>/2024/01/10/Python%20%E7%8E%AF%E5%A2%83%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85/</url>
      
        <content type="html"><![CDATA[<h1 id="pip-通过-requirements-文件，批量下载python包，批量离线安装python包"><a href="#pip-通过-requirements-文件，批量下载python包，批量离线安装python包" class="headerlink" title="pip 通过 requirements 文件，批量下载python包，批量离线安装python包"></a>pip 通过 requirements 文件，批量下载python包，批量离线安装python包</h1><h4 id="1、首先，在开发项目环境中分析出所有依赖的库"><a href="#1、首先，在开发项目环境中分析出所有依赖的库" class="headerlink" title="1、首先，在开发项目环境中分析出所有依赖的库"></a>1、首先，在开发项目环境中分析出所有依赖的库</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip freeze &gt; requirements.txt # 该方法仅可以使用在虚拟环境中，会将python 解释器下的所有包都导出</span><br><span class="line">pipreqs ./ --encoding=utf-8 --force # 表示覆盖该原有requirements.txt</span><br></pre></td></tr></table></figure><h4 id="2-在有网络的目标环境中，将所有包下载到DIR这个目录中"><a href="#2-在有网络的目标环境中，将所有包下载到DIR这个目录中" class="headerlink" title="2. 在有网络的目标环境中，将所有包下载到DIR这个目录中"></a>2. 在有网络的目标环境中，将所有包下载到DIR这个目录中</h4><p>切记，不要在 windows 下载包，然后放到 Linux 上进行安装，这样基本装不上</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip download -d DIR -r requirements.txt</span><br><span class="line">pip wheel -w DIR -r requirements.txt</span><br></pre></td></tr></table></figure><ul><li>这两条命令的区别在于wheel 方式下载会将下载的包放入wheel 缓存，但缺点是wheel 不可以下载源码包</li><li>download 命令会查看wheel缓存，然后再去PyPI下载库，但download命令下载的包不会进入wheel缓存，download 的优点是可以下载源码包</li><li>需要注意，使用wheel 方式安装可能会报错，因为有些包是源码包，不能被打包成wheel 格式</li><li>download 方法下载的包，不会对依赖性进行检查，这意味着，如果下载 Flask-PyMongo 这个包，只会下载该包，而不会下载 pymongo，经试验发现，download 适合补充wheel不可下载的包，两者搭配使用，才能将requirements文件的库完整的下载</li></ul><h4 id="3-将文件打包后放到离线服务器上，并进行解压缩"><a href="#3-将文件打包后放到离线服务器上，并进行解压缩" class="headerlink" title="3. 将文件打包后放到离线服务器上，并进行解压缩"></a>3. 将文件打包后放到离线服务器上，并进行解压缩</h4><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install --<span class="keyword">no</span>-<span class="built_in">index</span> --<span class="keyword">find</span>-links=DIR -r requirements.txt</span><br></pre></td></tr></table></figure><p>命令说明</p><ul><li>freeze 将依赖关系分析出来并 使用管道符导入到该文件中</li><li>download 分析 requirements 文件，将所有包进行下载，通过 d 选项导入 DIR 文件夹</li><li>wheel 分析requirements 文件，并将所有包及其依赖包下载为 wheel 格式，通过 w 选项导入 DIR 文件夹中</li><li>–find-links 指定离线安装的文件夹DIR，也就是你下载好的包</li><li>注意: –no-index 必须搭配 –find-links 使用</li></ul><h2 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h2><ol><li>安装时报错，不能找到相应的包，打开文件夹后却发现有这个库<ol><li>原因可能是该库是3.10 版本，而服务器是3.9 版本，导致安装不上</li><li>使用以下命令选择合适的版本，注意 <code>--no-deps</code> 不可缺少， <figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">  pip download </span><br><span class="line"><span class="attr">--no-deps</span></span><br><span class="line">  <span class="attr">--platform</span> linux_x86_64</span><br><span class="line">  <span class="attr">--python-version</span> <span class="number">36</span></span><br><span class="line">  <span class="attr">--implementation</span> cp  </span><br><span class="line">  <span class="attr">--abi</span> cp36m    </span><br><span class="line">   -r requirements<span class="selector-class">.txt</span> -d pk</span><br></pre></td></tr></table></figure></li></ol></li></ol>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Docker 本地部署Hadoop、Spark、Zookeeper、Hive集群</title>
      <link href="/2024/01/08/Docker%20%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2Hadoop%EF%BC%8CSpark%EF%BC%8CZookeeper%EF%BC%8CHive%E9%9B%86%E7%BE%A4/"/>
      <url>/2024/01/08/Docker%20%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2Hadoop%EF%BC%8CSpark%EF%BC%8CZookeeper%EF%BC%8CHive%E9%9B%86%E7%BE%A4/</url>
      
        <content type="html"><![CDATA[<h2 id="Mysql、Redis、Tdengine-省略…-直接docker容器拉取"><a href="#Mysql、Redis、Tdengine-省略…-直接docker容器拉取" class="headerlink" title="Mysql、Redis、Tdengine 省略… 直接docker容器拉取"></a>Mysql、Redis、Tdengine 省略… 直接docker容器拉取</h2><h2 id="拉取各节点centos系统镜像"><a href="#拉取各节点centos系统镜像" class="headerlink" title="拉取各节点centos系统镜像"></a>拉取各节点centos系统镜像</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 设置docker网桥，用于分配固定IP</span></span></span><br><span class="line">docker network create --subnet=172.15.0.0/16 netgroup</span><br><span class="line"></span><br><span class="line">docker  pull  centos</span><br><span class="line"></span><br><span class="line">docker run -d --privileged -ti -v D:\WorkStation\Bigdatadevelop\BigdataTools\master:/opt --name cluster-master -h cluster-master --net netgroup --ip 172.15.0.2 centos:centos7 /usr/sbin/init</span><br><span class="line"></span><br><span class="line">docker run -d --privileged -ti -v D:\WorkStation\Bigdatadevelop\BigdataTools\slave1:/opt --name cluster-slave1-h cluster-slave1--net netgroup --ip 172.15.0.3 centos:centos7 /usr/sbin/init </span><br><span class="line"></span><br><span class="line">docker exec -ti 946556c3ae6c /bin/bash</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">改hostname</span></span><br><span class="line">vi /etc/hosts</span><br><span class="line">service network restart</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">改yum源</span></span><br><span class="line">mkdir bak</span><br><span class="line">mv * bak</span><br><span class="line">mv Centos-7.repo /etc/yum.repos.d/</span><br><span class="line">mv epel-7.repo /etc/yum.repos.d/</span><br><span class="line">yum clean all</span><br><span class="line">yum makecache</span><br><span class="line">yum repolist</span><br><span class="line"></span><br><span class="line">yum install httpd</span><br><span class="line">yum install net-tools</span><br><span class="line"></span><br><span class="line">yum -y install openssh-clients</span><br><span class="line">yum install openssh-server</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">免密配置</span></span><br><span class="line">systemctl status sshd</span><br><span class="line"></span><br><span class="line">passwd root</span><br><span class="line">ssh-keygen -t rsa</span><br><span class="line"></span><br><span class="line">ssh-copy-id  -f -i ~/.ssh/id_rsa.pub cluster-master</span><br><span class="line">ssh-copy-id  -f -i ~/.ssh/id_rsa.pub cluster-slave1</span><br><span class="line">ssh-copy-id  -f -i ~/.ssh/id_rsa.pub cluster-slave2</span><br><span class="line">ssh-copy-id  -f -i ~/.ssh/id_rsa.pub cluster-slave3</span><br><span class="line"></span><br><span class="line">Pzszh@062</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">容器保存成镜像</span></span><br><span class="line">docker commit -m &#x27;提交文字说明&#x27; -a &#x27;作者&#x27; 容器名 提交后的镜像名:提交后的镜像tag名</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">给需要推送的镜像打标签</span></span><br><span class="line">docker tag 镜像id 要推入的仓库的用户名/要推入的仓库名:新定义的tag</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">推送镜像到仓库</span></span><br><span class="line">docker push 要推入的仓库的用户名/要推入的仓库名:镜像标签</span><br></pre></td></tr></table></figure><h2 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">cd /opt</span><br><span class="line">tar -xzvf zookeeper-3.4.10.tar.gz</span><br><span class="line">mv zookeeper-3.4.10 /usr/local/zookeeper3.4.10</span><br><span class="line"> </span><br><span class="line">mkdir /usr/local/opt/zookeeper3.4.10/data        # 创建data目录</span><br><span class="line">mkdir /usr/local/opt/zookeeper3.4.10/dataLog     # 创建dataLog目录</span><br><span class="line"></span><br><span class="line">cd /opt/zookeeper3.4.10/data</span><br><span class="line">vi myid     # 输入数字1，然后保存，第二个节点输入2，第三个节点输入3</span><br><span class="line"> </span><br><span class="line">cd /opt/zookeeper3.4.10/conf</span><br><span class="line">cp zoo_sample.cfg zoo.cfg</span><br><span class="line">vi zoo.cfg             #在文件末尾添加如下内容</span><br><span class="line"></span><br><span class="line">dataDir=/opt/zookeeper3.4.10/data  </span><br><span class="line">dataLogDir=/opt/zookeeper3.4.10/dataLog  </span><br><span class="line">server.1=hadoop0:2888:3888  </span><br><span class="line">server.2=hadoop1:2888:3888  </span><br><span class="line">server.3=hadoop2:2888:3888  </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">注hadoop0，hadoop1，hadoop2为三个节点的主机名！</span></span><br><span class="line"></span><br><span class="line">cd /usr/local</span><br><span class="line">scp -r zookeeper3.4.10  root@hadoop1:/usr/local</span><br><span class="line">scp -r zookeeper3.4.10  root@hadoop2:/usr/local</span><br><span class="line"></span><br><span class="line">分别在三台服务器上运行如下命令</span><br><span class="line">zkServer.sh start</span><br></pre></td></tr></table></figure><h3 id="添加-Hadoop-用户"><a href="#添加-Hadoop-用户" class="headerlink" title="添加 Hadoop 用户"></a>添加 Hadoop 用户</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">useradd -m hadoop -s /bin/bash</span><br><span class="line">passwd hadoop</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">添加 root权限</span></span><br><span class="line">vi /etc/passwd </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">hadoop:X:0:1000::/home/hadoop:/bin/bash</span></span><br><span class="line">su - hadoop</span><br></pre></td></tr></table></figure><h2 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tar -zxf hadoop-3.1.3.tar.gz -C /usr/local/</span><br><span class="line">cd /usr/local/</span><br><span class="line">mv ./hadoop-3.1.3/ ./hadoop</span><br><span class="line">chown -R hadoop ./hadoop</span><br><span class="line"></span><br><span class="line">vi ~/.bashrc</span><br><span class="line">export PATH=$PATH:/usr/local/hadoop/bin:/usr/local/hadoop/sbin</span><br></pre></td></tr></table></figure><p>在配置集群&#x2F;分布式模式时，需要修改“&#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;etc&#x2F;hadoop”目录下的配置文件，这里仅设置正常启动所必须的设置项，包括workers 、core-site.xml、hdfs-site.xml、mapred-site.xml、yarn-site.xml共5个文件，更多设置项可查看官方说明。</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## workers</span></span><br><span class="line">需要把所有数据节点的主机名写入该文件，每行一个，默认为 localhost（即把本机作为数据节点），在进行分布式配置时，可以保留localhost，让<span class="literal">Master</span>节点同时充当名称节点和数据节点，或者也可以删掉localhost这行，让<span class="literal">Master</span>节点仅作为名称节点使用。</span><br><span class="line">localhost</span><br><span class="line">cluster-<span class="keyword">master</span></span><br><span class="line"><span class="title">cluster-slave1</span></span><br><span class="line">cluster-slave2</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># core-site.xml</span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://cluster-master:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/usr/local/hadoop/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">description</span>&gt;</span>Abase for other temporary directories.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">## hdfs-site.xml</span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>cluster-master:50090<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>13/value&gt;</span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/usr/local/hadoop/tmp/dfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/usr/local/hadoop/tmp/dfs/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">## mapred-site.xml</span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>cluster-master:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>cluster-master:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.app.mapreduce.am.env<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>HADOOP_MAPRED_HOME=/usr/local/hadoop<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.env<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>HADOOP_MAPRED_HOME=/usr/local/hadoop<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.env<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>HADOOP_MAPRED_HOME=/usr/local/hadoop<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">## yarn-site.xml</span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>cluster-master<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">master</span></span><br><span class="line">cd /usr/local</span><br><span class="line">rm -r ./hadoop/tmp # 删除 Hadoop 临时文件</span><br><span class="line">rm -r ./hadoop/logs/* # 删除日志文件</span><br><span class="line">tar -zcf ~/hadoop.master.tar.gz ./hadoop # 先压缩再复制</span><br><span class="line">cd ~</span><br><span class="line">scp ./hadoop.master.tar.gz Slave1:/home/hadoop</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">slave</span></span><br><span class="line">rm -r /usr/local/hadoop # 删掉旧的（如果存在）</span><br><span class="line">tar -zxf ~/hadoop.master.tar.gz -C /usr/local</span><br><span class="line">chown -R hadoop /usr/local/hadoop</span><br></pre></td></tr></table></figure><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 切换 hadoop用户</span></span><br><span class="line"><span class="comment"># 首次启动Hadoop集群时，需要先在Master节点执行名称节点的格式化（只需要执行这一次，后面再启动Hadoop时，不要再次格式化名称节点），命令如下：</span></span><br><span class="line">hdfs namenode -<span class="built_in">format</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动Hadoop，启动需要在Master节点上进行，执行如下命令：</span></span><br><span class="line"><span class="built_in">start</span>-dfs.sh</span><br><span class="line"><span class="built_in">start</span>-yarn.sh </span><br><span class="line">mr-jobhistory-daemon.sh <span class="built_in">start</span> historyserver</span><br></pre></td></tr></table></figure><h2 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tar -zxf ~/下载/spark-2.4.0-bin-without-hadoop.tgz -C /usr/local/</span><br><span class="line">mv ./spark-2.4.0-bin-without-hadoop/ ./spark</span><br><span class="line">chown -R hadoop:hadoop ./spark</span><br><span class="line"></span><br><span class="line">cd /usr/local/spark</span><br><span class="line">cp ./conf/spark-env.sh.template ./conf/spark-env.sh</span><br><span class="line"></span><br><span class="line">vi ./conf/spark-env.sh</span><br><span class="line">export SPARK_DIST_CLASSPATH=$(/usr/local/hadoop/bin/hadoop classpath)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">SPARK_LOCAL_DIRS</span>=/usr/local/spark/</span><br><span class="line"><span class="attribute">HADOOP_CONF_DIR</span>=/usr/local/hadoop/etc/hadoop</span><br><span class="line"><span class="attribute">YARN_CONF_DIR</span>=//usr/local/hadoop/etc/hadoop</span><br><span class="line"><span class="attribute">JAVA_HOME</span>=/usr/local/java/jdk1.8.0_162</span><br><span class="line"><span class="built_in">export</span> <span class="attribute">SPARK_MASTER_IP</span>=cluster-master</span><br><span class="line"><span class="built_in">export</span> <span class="attribute">SPARK_DAEMON_JAVA_OPTS</span>=<span class="string">&quot;</span></span><br><span class="line"><span class="string">-Dspark.deploy.recoveryMode=ZOOKEEPER</span></span><br><span class="line"><span class="string">-Dspark.deploy.zookeeper.url=172.15.0.2:2181</span></span><br><span class="line"><span class="string">-Dspark.deploy.zookeeper.dir=/sparkmaster&quot;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="slaves"><a href="#slaves" class="headerlink" title="slaves"></a>slaves</h3><figure class="highlight mel"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cluster</span>-slave1 </span><br><span class="line"><span class="keyword">cluster</span>-slave2</span><br></pre></td></tr></table></figure><h3 id="spark-default-conf"><a href="#spark-default-conf" class="headerlink" title="spark-default.conf"></a>spark-default.conf</h3><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">spark<span class="selector-class">.eventLog</span><span class="selector-class">.enabled</span>          true</span><br><span class="line">spark<span class="selector-class">.eventLog</span><span class="selector-class">.dir</span>              hdfs:<span class="comment">//jinbill/spark/eventLog</span></span><br><span class="line">spark<span class="selector-class">.history</span><span class="selector-class">.fs</span><span class="selector-class">.logDirectory</span>   hdfs:<span class="comment">//jinbill/spark/eventLog</span></span><br><span class="line">spark<span class="selector-class">.eventLog</span><span class="selector-class">.compress</span>         true</span><br></pre></td></tr></table></figure><h2 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf apache-hive-3.1.2-bin.tar.gz</span><br><span class="line">mv apache-hive-3.1.2-bin /usr/local/hive</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">新建一个日志目录</span></span><br><span class="line">mkdir /usr/local/hive/iotmp</span><br></pre></td></tr></table></figure><h3 id="设置环境变量"><a href="#设置环境变量" class="headerlink" title="设置环境变量"></a>设置环境变量</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">vi ~/.bashrc</span><br><span class="line"></span><br><span class="line">export HIVE_HOME=/usr/local/hive</span><br><span class="line">export PATH=$HIVE_HOME/bin:$PATH</span><br><span class="line"></span><br><span class="line">source ~/.bashrc</span><br></pre></td></tr></table></figure><h3 id="设置Hive-HDFS文件夹"><a href="#设置Hive-HDFS文件夹" class="headerlink" title="设置Hive HDFS文件夹"></a>设置Hive HDFS文件夹</h3><h3 id="解决guava库问题"><a href="#解决guava库问题" class="headerlink" title="解决guava库问题"></a>解决guava库问题</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看hadoop下guava 版本</span></span><br><span class="line">cd /usr/local/hadoop/share/hadoop/common/lib/</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">guava-27.0-jre.jar</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看hive下guava 版本</span></span><br><span class="line">cd /usr/local/hive/lib</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">guava-19.0.jar</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">高版本替换低版本</span></span><br></pre></td></tr></table></figure><h3 id="MySQL-中-建立hive数据库"><a href="#MySQL-中-建立hive数据库" class="headerlink" title="MySQL 中 建立hive数据库"></a>MySQL 中 建立hive数据库</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> DATABASE hive;</span><br></pre></td></tr></table></figure><h3 id="jdbc依赖导入"><a href="#jdbc依赖导入" class="headerlink" title="jdbc依赖导入"></a>jdbc依赖导入</h3><h4 id="将hive的jline包替换到hadoop的yarn下"><a href="#将hive的jline包替换到hadoop的yarn下" class="headerlink" title="将hive的jline包替换到hadoop的yarn下"></a>将hive的jline包替换到hadoop的yarn下</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">`mv /opt/hive/apache-hive-3.1.2-bin/lib/jline-2.12.jar /opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/`</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">JDBC 依赖放入 /usr/local/hive/lib</span></span><br><span class="line">mv mysql-connector-java-5.1.47.jar /usr/local/hive/lib/</span><br></pre></td></tr></table></figure><h3 id="修改master节点配置文件"><a href="#修改master节点配置文件" class="headerlink" title="修改master节点配置文件"></a>修改master节点配置文件</h3><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#### VI编辑器替换命令</span><br><span class="line"><span class="meta">:%s/$&#123;system:java.io.tmpdir&#125;/\/opt\/hive\/iotmp/g</span>  </span><br><span class="line"><span class="meta">:%s/$&#123;system:user.name&#125;/huan/g</span></span><br></pre></td></tr></table></figure><h4 id="使用mysql替换默认的derby存放元数据"><a href="#使用mysql替换默认的derby存放元数据" class="headerlink" title="使用mysql替换默认的derby存放元数据"></a>使用mysql替换默认的derby存放元数据</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--元数据库修改为MySQL--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.db.type<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">      Expects one of [derby, oracle, mysql, mssql, postgres].</span><br><span class="line">      Type of database used by the metastore. Information schema <span class="symbol">&amp;amp;</span> JDBCStorageHandler depend on it.</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--MySQL 驱动--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Driver class name for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--MySQL URL--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://10.20.89.80:3306/hive?createDatabaseIfNotExist=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">      JDBC connect string for a JDBC metastore.</span><br><span class="line">      To use SSL to encrypt/authenticate the connection, provide database-specific SSL flag in the connection URL.</span><br><span class="line">      For example, jdbc:postgresql://myhost/db?ssl=true for postgres database.</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--MySQL 用户名--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Username to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--MySQL 密码--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>123456<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>password to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="设置解析引擎为spark"><a href="#设置解析引擎为spark" class="headerlink" title="设置解析引擎为spark"></a>设置解析引擎为spark</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.execution.engine<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>spark<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">      Expects one of [mr, tez, spark].</span><br><span class="line">      Chooses execution engine. Options are: mr (Map reduce, default), tez, spark. While MR</span><br><span class="line">      remains the default engine for historical reasons, it is itself a historical engine</span><br><span class="line">      and is deprecated in Hive 2 line. It may be removed without further warning.</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="自动初始化元数据"><a href="#自动初始化元数据" class="headerlink" title="自动初始化元数据"></a>自动初始化元数据</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>datanucleus.schema.autoCreateAll<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Auto creates necessary schema on a startup if one doesn&#x27;t exist. Set this to false, after creating it once.To enable auto create also set hive.metastore.schema.verification=false. Auto creation is not recommended for production use cases, run schematool command instead.</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="关闭校验"><a href="#关闭校验" class="headerlink" title="关闭校验"></a>关闭校验</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--听说是JDK版本使用1.8的问题。。--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.schema.verification<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">     Enforce metastore schema version consistency.</span><br><span class="line">     True: Verify that version information stored in is compatible with one from Hive jars.  Also disable automatic</span><br><span class="line">           schema migration attempt. Users are required to manually migrate schema after Hive upgrade which ensures</span><br><span class="line">           proper metastore schema migration. (Default)</span><br><span class="line">     False: Warn if the version information stored in metastore doesn&#x27;t match with one from in Hive jars.</span><br><span class="line">   <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.conf.validation<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">description</span>&gt;</span>Enables type checking for registered Hive configurations<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="删除-description-中的-8，这个解析会报错"><a href="#删除-description-中的-8，这个解析会报错" class="headerlink" title="删除 description 中的 &amp;#8，这个解析会报错"></a>删除 description 中的 <code>&amp;#8</code>，这个解析会报错</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.txn.xlock.iow<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">     Ensures commands with OVERWRITE (such as INSERT OVERWRITE) acquire Exclusive locks for<span class="symbol">&amp;#8;</span>transactional tables.  This ensures that inserts (w/o overwrite) running concurrently</span><br><span class="line">     are not hidden by the INSERT OVERWRITE.</span><br><span class="line">   <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="Spark任务提交"><a href="#Spark任务提交" class="headerlink" title="Spark任务提交"></a>Spark任务提交</h2><p>打包python依赖，镜像<br>进入到虚拟环境下，如&#x2F;home&#x2F;hadoop&#x2F;anaconda3&#x2F;envs，使用以下命令将虚拟环境进行打包：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zip -r conda_env.zip conda_env # 虚拟环境为conda_env, 打包为conda_env.zip 文件</span><br></pre></td></tr></table></figure><h4 id="spark-submit参数设置"><a href="#spark-submit参数设置" class="headerlink" title="spark-submit参数设置"></a>spark-submit参数设置</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">spark_conf = SparkConf().loadPropertiesFile(<span class="string">&quot;spark_config.properties&quot;</span>)</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder \</span><br><span class="line">    .appName(spark_conf.get(<span class="string">&quot;spark.app.name&quot;</span>)) \</span><br><span class="line">    .master(spark_conf.get(<span class="string">&quot;spark.master&quot;</span>)) \</span><br><span class="line">    .config(<span class="string">&quot;spark.executor.memory&quot;</span>, spark_conf.get(<span class="string">&quot;spark.executor.memory&quot;</span>)) \</span><br><span class="line">    .config(<span class="string">&quot;spark.driver.memory&quot;</span>, spark_conf.get(<span class="string">&quot;spark.driver.memory&quot;</span>)) \</span><br><span class="line">    .getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用SparkSession进行数据处理和分析</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$&#123;SPARK_PATH&#125;</span>/bin/spark-submit \</span><br><span class="line"> --master yarn \</span><br><span class="line"> --name <span class="string">&quot;spark_demo_lr&quot;</span> \</span><br><span class="line"> --queue <span class="variable">$&#123;YARN_QUEUE&#125;</span> \</span><br><span class="line"> --deploy-mode <span class="variable">$&#123;DEPLOY_MODE&#125;</span> \</span><br><span class="line"> --driver-memory 6g \</span><br><span class="line"> --driver-cores 4 \</span><br><span class="line"> --executor-memory 12g \</span><br><span class="line"> --executor-cores 15 \</span><br><span class="line"> --num-executors 10 \</span><br><span class="line"> --archives ./source/py27.zip<span class="comment">#python_env \</span></span><br><span class="line"> --conf spark.default.parallelism=150 \</span><br><span class="line"> --conf spark.executor.memoryOverhead=4g \</span><br><span class="line"> --conf spark.driver.memoryOverhead=2g \</span><br><span class="line"> --conf spark.yarn.maxAppAttempts=3 \</span><br><span class="line"> --conf spark.yarn.submit.waitAppCompletion=<span class="literal">true</span> \</span><br><span class="line"> --conf spark.pyspark.driver.python=./source/py27/bin/python2 \</span><br><span class="line"> --conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=./python_env/py27/bin/python2 \</span><br><span class="line"> --conf spark.pyspark.python=./python_env/py27/bin/python2 \</span><br><span class="line"> ./<span class="variable">$&#123;ModelType&#125;</span>.py <span class="variable">$input_path_train</span> <span class="variable">$input_path_test</span> <span class="variable">$output_path</span></span><br></pre></td></tr></table></figure><h4 id="pyspark-传入配置文件参数"><a href="#pyspark-传入配置文件参数" class="headerlink" title="pyspark 传入配置文件参数"></a>pyspark 传入配置文件参数</h4><p>首先，可以使用 <code>--files</code> 参数将配置文件传递给 <code>spark-submit</code> 命令。这将确保配置文件在集群中的每个节点上都可用。以下是一个示例：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-submit --master yarn --deploy-mode cluster --files /home/sys_user/ask/conf/config.ini test.py</span><br></pre></td></tr></table></figure><p>在 PySpark 脚本中，可以使用 <code>SparkFiles.get()</code> 方法来读取传入的配置文件</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkFiles</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(SparkFiles.get(<span class="string">&#x27;config.ini&#x27;</span>)) <span class="keyword">as</span> config_file:</span><br><span class="line">    <span class="built_in">print</span>(config_file.read())</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 SparkSession</span></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">&quot;ConfigExample&quot;</span>).getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载配置参数</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;config.properties&quot;</span>, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">        line = line.strip()</span><br><span class="line">        <span class="keyword">if</span> line <span class="keyword">and</span> <span class="keyword">not</span> line.startswith(<span class="string">&quot;#&quot;</span>):</span><br><span class="line">            key, value = line.split(<span class="string">&quot;=&quot;</span>)</span><br><span class="line">            spark.conf.<span class="built_in">set</span>(key, value)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印配置参数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;spark.sql.shuffle.partitions:&quot;</span>, spark.conf.get(<span class="string">&quot;spark.sql.shuffle.partitions&quot;</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;spark.sql.autoBroadcastJoinThreshold:&quot;</span>, spark.conf.get(<span class="string">&quot;spark.sql.autoBroadcastJoinThreshold&quot;</span>))</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./spark-submit --master yarn --deploy-mode cluster --files /opt/bigdata/config_batch.properties --executor-memory 2g --executor-cores 1 --num-executors 5 --driver-memory 2G  --py-files /usr/local/miniconda3/envs/spark_env.zip --jars /usr/local/spark/jars/mysql-connector-java-5.1.47.jar,/usr/local/spark/jars/taos-jdbcdriver-2.0.34.jar,/usr/local/spark/jars/spark-redis-2.4.0-jar-with-dependencies.jar,/usr/local/spark/jars/fastjson-1.2.73.jar --conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=/usr/local/miniconda3/envs/spark_env/bin/python --conf spark.pyspark.python=/usr/local/miniconda3/envs/spark_env/bin/python /opt/bigdata/HealthScoreHFL2.py</span><br></pre></td></tr></table></figure><p>        .master(“local”)<br>        .enableHiveSupport()<br>        .config(‘spark.executor.extraClassPath’, “&#x2F;usr&#x2F;local&#x2F;spark&#x2F;jars&#x2F;mysql-connector-java-5.1.47.jar”)\</p><p>        .config(‘spark.driver.extraClassPath’, “&#x2F;usr&#x2F;local&#x2F;spark&#x2F;jars&#x2F;mysql-connector-java-5.1.47.jar”)\</p><p>        .config(“spark.jars”, “&#x2F;usr&#x2F;local&#x2F;spark&#x2F;jars&#x2F;mysql-connector-java-5.1.47.jar,&#x2F;usr&#x2F;local&#x2F;spark&#x2F;jars&#x2F;taos-jdbcdriver-2.0.34.jar,&#x2F;usr&#x2F;local&#x2F;spark&#x2F;jars&#x2F;spark-redis-2.4.0-jar-with-dependencies.jar,&#x2F;usr&#x2F;local&#x2F;spark&#x2F;jars&#x2F;fastjson-1.2.73.jar”)\</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./spark-submit --master yarn --deploy-mode cluster --files /opt/bigdata/batch/config_batch.properties --executor-memory 2g --executor-cores 1 --num-executors 5 --driver-memory 2G  --py-files /opt/bigdata/batch/spark3.1_env.zip --jars /usr/local/spark-yarn/jars/mysql-connector-java-5.1.47.jar,/usr/local/spark-yarn/jars/taos-jdbcdriver-2.0.34.jar,/usr/local/spark-yarn/jars/spark-redis-2.4.0-jar-with-dependencies.jar,/usr/local/spark-yarn/jars/fastjson-1.2.73.jar --conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=/usr/local/miniconda3/bin/python --conf spark.pyspark.python=/usr/local/miniconda3/bin/python /opt/bigdata/batch/HealthScoreHFL2.py</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> bigdata </category>
          
          <category> docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> bigdata </tag>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2024/01/03/hello-world/"/>
      <url>/2024/01/03/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
