<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Docker 本地部署Hadoop、Spark、Zookeeper、Hive集群</title>
      <link href="/2024/01/08/Docker%20%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2Hadoop%EF%BC%8CSpark%EF%BC%8CZookeeper%EF%BC%8CHive%E9%9B%86%E7%BE%A4/"/>
      <url>/2024/01/08/Docker%20%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2Hadoop%EF%BC%8CSpark%EF%BC%8CZookeeper%EF%BC%8CHive%E9%9B%86%E7%BE%A4/</url>
      
        <content type="html"><![CDATA[<h2 id="Mysql、Redis、Tdengine-省略…-直接docker容器拉取"><a href="#Mysql、Redis、Tdengine-省略…-直接docker容器拉取" class="headerlink" title="Mysql、Redis、Tdengine 省略… 直接docker容器拉取"></a>Mysql、Redis、Tdengine 省略… 直接docker容器拉取</h2><h2 id="拉取各节点centos系统镜像"><a href="#拉取各节点centos系统镜像" class="headerlink" title="拉取各节点centos系统镜像"></a>拉取各节点centos系统镜像</h2><pre><code class="shell">## 设置docker网桥，用于分配固定IPdocker network create --subnet=172.15.0.0/16 netgroupdocker  pull  centosdocker run -d --privileged -ti -v D:\WorkStation\Bigdatadevelop\BigdataTools\master:/opt --name cluster-master -h cluster-master --net netgroup --ip 172.15.0.2 centos:centos7 /usr/sbin/initdocker run -d --privileged -ti -v D:\WorkStation\Bigdatadevelop\BigdataTools\slave1:/opt --name cluster-slave1-h cluster-slave1--net netgroup --ip 172.15.0.3 centos:centos7 /usr/sbin/init docker exec -ti 946556c3ae6c /bin/bash#改hostnamevi /etc/hostsservice network restart# 改yum源mkdir bakmv * bakmv Centos-7.repo /etc/yum.repos.d/mv epel-7.repo /etc/yum.repos.d/yum clean allyum makecacheyum repolistyum install httpdyum install net-toolsyum -y install openssh-clientsyum install openssh-server#免密配置systemctl status sshdpasswd rootssh-keygen -t rsassh-copy-id  -f -i ~/.ssh/id_rsa.pub cluster-masterssh-copy-id  -f -i ~/.ssh/id_rsa.pub cluster-slave1ssh-copy-id  -f -i ~/.ssh/id_rsa.pub cluster-slave2ssh-copy-id  -f -i ~/.ssh/id_rsa.pub cluster-slave3Pzszh@062# 容器保存成镜像docker commit -m &#39;提交文字说明&#39; -a &#39;作者&#39; 容器名 提交后的镜像名:提交后的镜像tag名# 给需要推送的镜像打标签docker tag 镜像id 要推入的仓库的用户名/要推入的仓库名:新定义的tag# 推送镜像到仓库docker push 要推入的仓库的用户名/要推入的仓库名:镜像标签</code></pre><h2 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h2><pre><code class="shell">cd /opttar -xzvf zookeeper-3.4.10.tar.gzmv zookeeper-3.4.10 /usr/local/zookeeper3.4.10 mkdir /usr/local/opt/zookeeper3.4.10/data        # 创建data目录mkdir /usr/local/opt/zookeeper3.4.10/dataLog     # 创建dataLog目录cd /opt/zookeeper3.4.10/datavi myid     # 输入数字1，然后保存，第二个节点输入2，第三个节点输入3 cd /opt/zookeeper3.4.10/confcp zoo_sample.cfg zoo.cfgvi zoo.cfg             #在文件末尾添加如下内容dataDir=/opt/zookeeper3.4.10/data  dataLogDir=/opt/zookeeper3.4.10/dataLog  server.1=hadoop0:2888:3888  server.2=hadoop1:2888:3888  server.3=hadoop2:2888:3888  # 注hadoop0，hadoop1，hadoop2为三个节点的主机名！cd /usr/localscp -r zookeeper3.4.10  root@hadoop1:/usr/localscp -r zookeeper3.4.10  root@hadoop2:/usr/local分别在三台服务器上运行如下命令zkServer.sh start</code></pre><h3 id="添加-Hadoop-用户"><a href="#添加-Hadoop-用户" class="headerlink" title="添加 Hadoop 用户"></a>添加 Hadoop 用户</h3><pre><code class="shell">useradd -m hadoop -s /bin/bashpasswd hadoop# 添加 root权限vi /etc/passwd # hadoop:X:0:1000::/home/hadoop:/bin/bashsu - hadoop</code></pre><h2 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h2><pre><code class="shell">tar -zxf hadoop-3.1.3.tar.gz -C /usr/local/cd /usr/local/mv ./hadoop-3.1.3/ ./hadoopchown -R hadoop ./hadoopvi ~/.bashrcexport PATH=$PATH:/usr/local/hadoop/bin:/usr/local/hadoop/sbin</code></pre><p>在配置集群&#x2F;分布式模式时，需要修改“&#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;etc&#x2F;hadoop”目录下的配置文件，这里仅设置正常启动所必须的设置项，包括workers 、core-site.xml、hdfs-site.xml、mapred-site.xml、yarn-site.xml共5个文件，更多设置项可查看官方说明。</p><pre><code>## workers需要把所有数据节点的主机名写入该文件，每行一个，默认为 localhost（即把本机作为数据节点），在进行分布式配置时，可以保留localhost，让Master节点同时充当名称节点和数据节点，或者也可以删掉localhost这行，让Master节点仅作为名称节点使用。localhostcluster-mastercluster-slave1cluster-slave2</code></pre><pre><code class="xml"># core-site.xml&lt;configuration&gt;        &lt;property&gt;                &lt;name&gt;fs.defaultFS&lt;/name&gt;                &lt;value&gt;hdfs://cluster-master:9000&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;                &lt;value&gt;file:/usr/local/hadoop/tmp&lt;/value&gt;                &lt;description&gt;Abase for other temporary directories.&lt;/description&gt;        &lt;/property&gt;&lt;/configuration&gt;</code></pre><pre><code class="xml">## hdfs-site.xml&lt;configuration&gt;        &lt;property&gt;                &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;                &lt;value&gt;cluster-master:50090&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;dfs.replication&lt;/name&gt;                &lt;value&gt;13/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;                &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/name&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;                &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/data&lt;/value&gt;        &lt;/property&gt;&lt;/configuration&gt;</code></pre><pre><code class="xml">## mapred-site.xml&lt;configuration&gt;        &lt;property&gt;                &lt;name&gt;mapreduce.framework.name&lt;/name&gt;                &lt;value&gt;yarn&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;                &lt;value&gt;cluster-master:10020&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;                &lt;value&gt;cluster-master:19888&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt;                &lt;value&gt;HADOOP_MAPRED_HOME=/usr/local/hadoop&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;mapreduce.map.env&lt;/name&gt;                &lt;value&gt;HADOOP_MAPRED_HOME=/usr/local/hadoop&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;mapreduce.reduce.env&lt;/name&gt;                &lt;value&gt;HADOOP_MAPRED_HOME=/usr/local/hadoop&lt;/value&gt;        &lt;/property&gt; &lt;/configuration&gt;</code></pre><pre><code class="xml">## yarn-site.xml&lt;configuration&gt;        &lt;property&gt;                &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;                &lt;value&gt;cluster-master&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;                &lt;value&gt;mapreduce_shuffle&lt;/value&gt;        &lt;/property&gt;&lt;/configuration&gt;</code></pre><pre><code class="shell"># mastercd /usr/localrm -r ./hadoop/tmp # 删除 Hadoop 临时文件rm -r ./hadoop/logs/* # 删除日志文件tar -zcf ~/hadoop.master.tar.gz ./hadoop # 先压缩再复制cd ~scp ./hadoop.master.tar.gz Slave1:/home/hadoop</code></pre><pre><code class="shell"># slaverm -r /usr/local/hadoop # 删掉旧的（如果存在）tar -zxf ~/hadoop.master.tar.gz -C /usr/localchown -R hadoop /usr/local/hadoop</code></pre><pre><code># 切换 hadoop用户# 首次启动Hadoop集群时，需要先在Master节点执行名称节点的格式化（只需要执行这一次，后面再启动Hadoop时，不要再次格式化名称节点），命令如下：hdfs namenode -format# 启动Hadoop，启动需要在Master节点上进行，执行如下命令：start-dfs.shstart-yarn.sh mr-jobhistory-daemon.sh start historyserver</code></pre><h2 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h2><pre><code class="shell">tar -zxf ~/下载/spark-2.4.0-bin-without-hadoop.tgz -C /usr/local/mv ./spark-2.4.0-bin-without-hadoop/ ./sparkchown -R hadoop:hadoop ./sparkcd /usr/local/sparkcp ./conf/spark-env.sh.template ./conf/spark-env.shvi ./conf/spark-env.shexport SPARK_DIST_CLASSPATH=$(/usr/local/hadoop/bin/hadoop classpath)</code></pre><pre><code>SPARK_LOCAL_DIRS=/usr/local/spark/HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoopYARN_CONF_DIR=//usr/local/hadoop/etc/hadoopJAVA_HOME=/usr/local/java/jdk1.8.0_162export SPARK_MASTER_IP=cluster-masterexport SPARK_DAEMON_JAVA_OPTS=&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER-Dspark.deploy.zookeeper.url=172.15.0.2:2181-Dspark.deploy.zookeeper.dir=/sparkmaster&quot;</code></pre><h3 id="slaves"><a href="#slaves" class="headerlink" title="slaves"></a>slaves</h3><pre><code>cluster-slave1 cluster-slave2</code></pre><h3 id="spark-default-conf"><a href="#spark-default-conf" class="headerlink" title="spark-default.conf"></a>spark-default.conf</h3><pre><code>spark.eventLog.enabled          truespark.eventLog.dir              hdfs://jinbill/spark/eventLogspark.history.fs.logDirectory   hdfs://jinbill/spark/eventLogspark.eventLog.compress         true</code></pre><h2 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h2><pre><code class="shell">tar -zxvf apache-hive-3.1.2-bin.tar.gzmv apache-hive-3.1.2-bin /usr/local/hive# 新建一个日志目录mkdir /usr/local/hive/iotmp</code></pre><h3 id="设置环境变量"><a href="#设置环境变量" class="headerlink" title="设置环境变量"></a>设置环境变量</h3><pre><code class="shell">vi ~/.bashrcexport HIVE_HOME=/usr/local/hiveexport PATH=$HIVE_HOME/bin:$PATHsource ~/.bashrc</code></pre><h3 id="设置Hive-HDFS文件夹"><a href="#设置Hive-HDFS文件夹" class="headerlink" title="设置Hive HDFS文件夹"></a>设置Hive HDFS文件夹</h3><h3 id="解决guava库问题"><a href="#解决guava库问题" class="headerlink" title="解决guava库问题"></a>解决guava库问题</h3><pre><code class="shell"># 查看hadoop下guava 版本cd /usr/local/hadoop/share/hadoop/common/lib/# guava-27.0-jre.jar# 查看hive下guava 版本cd /usr/local/hive/lib# guava-19.0.jar# 高版本替换低版本</code></pre><h3 id="MySQL-中-建立hive数据库"><a href="#MySQL-中-建立hive数据库" class="headerlink" title="MySQL 中 建立hive数据库"></a>MySQL 中 建立hive数据库</h3><pre><code class="sql">CREATE DATABASE hive;</code></pre><h3 id="jdbc依赖导入"><a href="#jdbc依赖导入" class="headerlink" title="jdbc依赖导入"></a>jdbc依赖导入</h3><h4 id="将hive的jline包替换到hadoop的yarn下"><a href="#将hive的jline包替换到hadoop的yarn下" class="headerlink" title="将hive的jline包替换到hadoop的yarn下"></a>将hive的jline包替换到hadoop的yarn下</h4><pre><code class="shell">`mv /opt/hive/apache-hive-3.1.2-bin/lib/jline-2.12.jar /opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/`# JDBC 依赖放入 /usr/local/hive/libmv mysql-connector-java-5.1.47.jar /usr/local/hive/lib/</code></pre><h3 id="修改master节点配置文件"><a href="#修改master节点配置文件" class="headerlink" title="修改master节点配置文件"></a>修改master节点配置文件</h3><pre><code>#### VI编辑器替换命令:%s/$&#123;system:java.io.tmpdir&#125;/\/opt\/hive\/iotmp/g  :%s/$&#123;system:user.name&#125;/huan/g</code></pre><h4 id="使用mysql替换默认的derby存放元数据"><a href="#使用mysql替换默认的derby存放元数据" class="headerlink" title="使用mysql替换默认的derby存放元数据"></a>使用mysql替换默认的derby存放元数据</h4><pre><code class="xml">&lt;!--元数据库修改为MySQL--&gt;&lt;property&gt;    &lt;name&gt;hive.metastore.db.type&lt;/name&gt;    &lt;value&gt;mysql&lt;/value&gt;    &lt;description&gt;      Expects one of [derby, oracle, mysql, mssql, postgres].      Type of database used by the metastore. Information schema &amp;amp; JDBCStorageHandler depend on it.    &lt;/description&gt;&lt;/property&gt;&lt;!--MySQL 驱动--&gt;&lt;property&gt;    &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;    &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;    &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;&lt;/property&gt;&lt;!--MySQL URL--&gt;&lt;property&gt;    &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;    &lt;value&gt;jdbc:mysql://10.20.89.80:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt;    &lt;description&gt;      JDBC connect string for a JDBC metastore.      To use SSL to encrypt/authenticate the connection, provide database-specific SSL flag in the connection URL.      For example, jdbc:postgresql://myhost/db?ssl=true for postgres database.    &lt;/description&gt;&lt;/property&gt;&lt;!--MySQL 用户名--&gt;&lt;property&gt;  &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;  &lt;value&gt;root&lt;/value&gt;  &lt;description&gt;Username to use against metastore database&lt;/description&gt;&lt;/property&gt;&lt;!--MySQL 密码--&gt;&lt;property&gt;    &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;    &lt;value&gt;123456&lt;/value&gt;    &lt;description&gt;password to use against metastore database&lt;/description&gt;&lt;/property&gt;</code></pre><h4 id="设置解析引擎为spark"><a href="#设置解析引擎为spark" class="headerlink" title="设置解析引擎为spark"></a>设置解析引擎为spark</h4><pre><code class="xml">&lt;property&gt;    &lt;name&gt;hive.execution.engine&lt;/name&gt;    &lt;value&gt;spark&lt;/value&gt;    &lt;description&gt;      Expects one of [mr, tez, spark].      Chooses execution engine. Options are: mr (Map reduce, default), tez, spark. While MR      remains the default engine for historical reasons, it is itself a historical engine      and is deprecated in Hive 2 line. It may be removed without further warning.    &lt;/description&gt;&lt;/property&gt;</code></pre><h4 id="自动初始化元数据"><a href="#自动初始化元数据" class="headerlink" title="自动初始化元数据"></a>自动初始化元数据</h4><pre><code class="xml">&lt;property&gt;    &lt;name&gt;datanucleus.schema.autoCreateAll&lt;/name&gt;    &lt;value&gt;true&lt;/value&gt;    &lt;description&gt;Auto creates necessary schema on a startup if one doesn&#39;t exist. Set this to false, after creating it once.To enable auto create also set hive.metastore.schema.verification=false. Auto creation is not recommended for production use cases, run schematool command instead.    &lt;/description&gt;&lt;/property&gt;</code></pre><h4 id="关闭校验"><a href="#关闭校验" class="headerlink" title="关闭校验"></a>关闭校验</h4><pre><code class="xml">&lt;!--听说是JDK版本使用1.8的问题。。--&gt;&lt;property&gt;   &lt;name&gt;hive.metastore.schema.verification&lt;/name&gt;   &lt;value&gt;false&lt;/value&gt;   &lt;description&gt;     Enforce metastore schema version consistency.     True: Verify that version information stored in is compatible with one from Hive jars.  Also disable automatic           schema migration attempt. Users are required to manually migrate schema after Hive upgrade which ensures           proper metastore schema migration. (Default)     False: Warn if the version information stored in metastore doesn&#39;t match with one from in Hive jars.   &lt;/description&gt;&lt;/property&gt;&lt;property&gt;   &lt;name&gt;hive.conf.validation&lt;/name&gt;   &lt;value&gt;false&lt;/value&gt;   &lt;description&gt;Enables type checking for registered Hive configurations&lt;/description&gt; &lt;/property&gt;</code></pre><h4 id="删除-description-中的-8，这个解析会报错"><a href="#删除-description-中的-8，这个解析会报错" class="headerlink" title="删除 description 中的 &amp;#8，这个解析会报错"></a>删除 description 中的 <code>&amp;#8</code>，这个解析会报错</h4><pre><code class="xml">&lt;property&gt;   &lt;name&gt;hive.txn.xlock.iow&lt;/name&gt;   &lt;value&gt;true&lt;/value&gt;   &lt;description&gt;     Ensures commands with OVERWRITE (such as INSERT OVERWRITE) acquire Exclusive locks for&amp;#8;transactional tables.  This ensures that inserts (w/o overwrite) running concurrently     are not hidden by the INSERT OVERWRITE.   &lt;/description&gt;&lt;/property&gt;</code></pre><h2 id="Spark任务提交"><a href="#Spark任务提交" class="headerlink" title="Spark任务提交"></a>Spark任务提交</h2><p>打包python依赖，镜像<br>进入到虚拟环境下，如&#x2F;home&#x2F;hadoop&#x2F;anaconda3&#x2F;envs，使用以下命令将虚拟环境进行打包：</p><pre><code class="shell">zip -r conda_env.zip conda_env # 虚拟环境为conda_env, 打包为conda_env.zip 文件</code></pre><h4 id="spark-submit参数设置"><a href="#spark-submit参数设置" class="headerlink" title="spark-submit参数设置"></a>spark-submit参数设置</h4><pre><code class="python">from pyspark.sql import SparkSessionspark_conf = SparkConf().loadPropertiesFile(&quot;spark_config.properties&quot;)spark = SparkSession.builder \    .appName(spark_conf.get(&quot;spark.app.name&quot;)) \    .master(spark_conf.get(&quot;spark.master&quot;)) \    .config(&quot;spark.executor.memory&quot;, spark_conf.get(&quot;spark.executor.memory&quot;)) \    .config(&quot;spark.driver.memory&quot;, spark_conf.get(&quot;spark.driver.memory&quot;)) \    .getOrCreate()# 使用SparkSession进行数据处理和分析</code></pre><pre><code class="bash">$&#123;SPARK_PATH&#125;/bin/spark-submit \ --master yarn \ --name &quot;spark_demo_lr&quot; \ --queue $&#123;YARN_QUEUE&#125; \ --deploy-mode $&#123;DEPLOY_MODE&#125; \ --driver-memory 6g \ --driver-cores 4 \ --executor-memory 12g \ --executor-cores 15 \ --num-executors 10 \ --archives ./source/py27.zip#python_env \ --conf spark.default.parallelism=150 \ --conf spark.executor.memoryOverhead=4g \ --conf spark.driver.memoryOverhead=2g \ --conf spark.yarn.maxAppAttempts=3 \ --conf spark.yarn.submit.waitAppCompletion=true \ --conf spark.pyspark.driver.python=./source/py27/bin/python2 \ --conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=./python_env/py27/bin/python2 \ --conf spark.pyspark.python=./python_env/py27/bin/python2 \ ./$&#123;ModelType&#125;.py $input_path_train $input_path_test $output_path</code></pre><h4 id="pyspark-传入配置文件参数"><a href="#pyspark-传入配置文件参数" class="headerlink" title="pyspark 传入配置文件参数"></a>pyspark 传入配置文件参数</h4><p>首先，可以使用 <code>--files</code> 参数将配置文件传递给 <code>spark-submit</code> 命令。这将确保配置文件在集群中的每个节点上都可用。以下是一个示例：</p><pre><code class="shell">spark-submit --master yarn --deploy-mode cluster --files /home/sys_user/ask/conf/config.ini test.py</code></pre><p>在 PySpark 脚本中，可以使用 <code>SparkFiles.get()</code> 方法来读取传入的配置文件</p><pre><code class="python">from pyspark import SparkFileswith open(SparkFiles.get(&#39;config.ini&#39;)) as config_file:    print(config_file.read())</code></pre><pre><code class="python">from pyspark.sql import SparkSession# 创建 SparkSessionspark = SparkSession.builder.appName(&quot;ConfigExample&quot;).getOrCreate()# 加载配置参数with open(&quot;config.properties&quot;, &quot;r&quot;) as f:    for line in f:        line = line.strip()        if line and not line.startswith(&quot;#&quot;):            key, value = line.split(&quot;=&quot;)            spark.conf.set(key, value)# 打印配置参数print(&quot;spark.sql.shuffle.partitions:&quot;, spark.conf.get(&quot;spark.sql.shuffle.partitions&quot;))print(&quot;spark.sql.autoBroadcastJoinThreshold:&quot;, spark.conf.get(&quot;spark.sql.autoBroadcastJoinThreshold&quot;))</code></pre><pre><code class="shell">./spark-submit --master yarn --deploy-mode cluster --files /opt/bigdata/config_batch.properties --executor-memory 2g --executor-cores 1 --num-executors 5 --driver-memory 2G  --py-files /usr/local/miniconda3/envs/spark_env.zip --jars /usr/local/spark/jars/mysql-connector-java-5.1.47.jar,/usr/local/spark/jars/taos-jdbcdriver-2.0.34.jar,/usr/local/spark/jars/spark-redis-2.4.0-jar-with-dependencies.jar,/usr/local/spark/jars/fastjson-1.2.73.jar --conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=/usr/local/miniconda3/envs/spark_env/bin/python --conf spark.pyspark.python=/usr/local/miniconda3/envs/spark_env/bin/python /opt/bigdata/HealthScoreHFL2.py</code></pre><p>        .master(“local”)<br>        .enableHiveSupport()<br>        .config(‘spark.executor.extraClassPath’, “&#x2F;usr&#x2F;local&#x2F;spark&#x2F;jars&#x2F;mysql-connector-java-5.1.47.jar”)\</p><p>        .config(‘spark.driver.extraClassPath’, “&#x2F;usr&#x2F;local&#x2F;spark&#x2F;jars&#x2F;mysql-connector-java-5.1.47.jar”)\</p><p>        .config(“spark.jars”, “&#x2F;usr&#x2F;local&#x2F;spark&#x2F;jars&#x2F;mysql-connector-java-5.1.47.jar,&#x2F;usr&#x2F;local&#x2F;spark&#x2F;jars&#x2F;taos-jdbcdriver-2.0.34.jar,&#x2F;usr&#x2F;local&#x2F;spark&#x2F;jars&#x2F;spark-redis-2.4.0-jar-with-dependencies.jar,&#x2F;usr&#x2F;local&#x2F;spark&#x2F;jars&#x2F;fastjson-1.2.73.jar”)\</p><pre><code class="shell">./spark-submit --master yarn --deploy-mode cluster --files /opt/bigdata/batch/config_batch.properties --executor-memory 2g --executor-cores 1 --num-executors 5 --driver-memory 2G  --py-files /opt/bigdata/batch/spark3.1_env.zip --jars /usr/local/spark-yarn/jars/mysql-connector-java-5.1.47.jar,/usr/local/spark-yarn/jars/taos-jdbcdriver-2.0.34.jar,/usr/local/spark-yarn/jars/spark-redis-2.4.0-jar-with-dependencies.jar,/usr/local/spark-yarn/jars/fastjson-1.2.73.jar --conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=/usr/local/miniconda3/bin/python --conf spark.pyspark.python=/usr/local/miniconda3/bin/python /opt/bigdata/batch/HealthScoreHFL2.py</code></pre>]]></content>
      
      
      <categories>
          
          <category> bigdata </category>
          
          <category> docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> bigdata </tag>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2024/01/03/hello-world/"/>
      <url>/2024/01/03/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre><code class="bash">$ hexo new &quot;My New Post&quot;</code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre><code class="bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre><code class="bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre><code class="bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
