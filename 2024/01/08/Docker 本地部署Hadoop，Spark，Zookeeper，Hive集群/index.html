<!DOCTYPE html><html lang="zh-TW" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Docker 本地部署Hadoop、Spark、Zookeeper、Hive集群 | 飞驰的细狗</title><meta name="author" content="auhjin_ai8"><meta name="copyright" content="auhjin_ai8"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="Mysql、Redis、Tdengine 省略… 直接docker容器拉取拉取各节点centos系统镜像 ## 设置docker网桥，用于分配固定IP docker network create --subnet&#x3D;172.15.0.0&#x2F;16 netgroup  docker  pull  centos  docker run -d --privileged -ti -v D:\WorkStatio">
<meta property="og:type" content="article">
<meta property="og:title" content="Docker 本地部署Hadoop、Spark、Zookeeper、Hive集群">
<meta property="og:url" content="http://auhjin.github.io/2024/01/08/Docker%20%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2Hadoop%EF%BC%8CSpark%EF%BC%8CZookeeper%EF%BC%8CHive%E9%9B%86%E7%BE%A4/index.html">
<meta property="og:site_name" content="飞驰的细狗">
<meta property="og:description" content="Mysql、Redis、Tdengine 省略… 直接docker容器拉取拉取各节点centos系统镜像 ## 设置docker网桥，用于分配固定IP docker network create --subnet&#x3D;172.15.0.0&#x2F;16 netgroup  docker  pull  centos  docker run -d --privileged -ti -v D:\WorkStatio">
<meta property="og:locale" content="zh_TW">
<meta property="og:image" content="http://auhjin.github.io/img/logo.png">
<meta property="article:published_time" content="2024-01-08T08:30:00.000Z">
<meta property="article:modified_time" content="2024-01-08T08:16:42.732Z">
<meta property="article:author" content="auhjin_ai8">
<meta property="article:tag" content="bigdata">
<meta property="article:tag" content="docker">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://auhjin.github.io/img/logo.png"><link rel="shortcut icon" href="/img/logo.png"><link rel="canonical" href="http://auhjin.github.io/2024/01/08/Docker%20%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2Hadoop%EF%BC%8CSpark%EF%BC%8CZookeeper%EF%BC%8CHive%E9%9B%86%E7%BE%A4/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?830c22da3f3d148d0977a09fcbcd2610";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><link rel="stylesheet" href="/%E9%A3%9E%E9%A9%B0%E7%9A%84%E7%BB%86%E7%8B%97" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: undefined,
  copy: {
    success: '複製成功',
    error: '複製錯誤',
    noSupport: '瀏覽器不支援'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '剛剛',
    min: '分鐘前',
    hour: '小時前',
    day: '天前',
    month: '個月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js',
    buttonText: '載入更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Docker 本地部署Hadoop、Spark、Zookeeper、Hive集群',
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isToc: true,
  postUpdate: '2024-01-08 16:16:42'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/logo.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">2</div></a><a href="/tags/"><div class="headline">標籤</div><div class="length-num">2</div></a><a href="/categories/"><div class="headline">分類</div><div class="length-num">2</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/weekly/"><i class="fa-fw fas fa-newspaper"></i><span> 周报</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/top.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="飞驰的细狗"><span class="site-name">飞驰的细狗</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/weekly/"><i class="fa-fw fas fa-newspaper"></i><span> 周报</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Docker 本地部署Hadoop、Spark、Zookeeper、Hive集群</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">發表於</span><time class="post-meta-date-created" datetime="2024-01-08T08:30:00.000Z" title="發表於 2024-01-08 16:30:00">2024-01-08</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新於</span><time class="post-meta-date-updated" datetime="2024-01-08T08:16:42.732Z" title="更新於 2024-01-08 16:16:42">2024-01-08</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/bigdata/">bigdata</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/bigdata/docker/">docker</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字數總計:</span><span class="word-count">3.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">閱讀時長:</span><span>16分鐘</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Docker 本地部署Hadoop、Spark、Zookeeper、Hive集群"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">閱讀量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="Mysql、Redis、Tdengine-省略…-直接docker容器拉取"><a href="#Mysql、Redis、Tdengine-省略…-直接docker容器拉取" class="headerlink" title="Mysql、Redis、Tdengine 省略… 直接docker容器拉取"></a>Mysql、Redis、Tdengine 省略… 直接docker容器拉取</h2><h2 id="拉取各节点centos系统镜像"><a href="#拉取各节点centos系统镜像" class="headerlink" title="拉取各节点centos系统镜像"></a>拉取各节点centos系统镜像</h2><pre><code class="shell">
## 设置docker网桥，用于分配固定IP
docker network create --subnet=172.15.0.0/16 netgroup

docker  pull  centos

docker run -d --privileged -ti -v D:\WorkStation\Bigdatadevelop\BigdataTools\master:/opt --name cluster-master -h cluster-master --net netgroup --ip 172.15.0.2 centos:centos7 /usr/sbin/init

docker run -d --privileged -ti -v D:\WorkStation\Bigdatadevelop\BigdataTools\slave1:/opt --name cluster-slave1-h cluster-slave1--net netgroup --ip 172.15.0.3 centos:centos7 /usr/sbin/init 

docker exec -ti 946556c3ae6c /bin/bash

#改hostname
vi /etc/hosts
service network restart
# 改yum源
mkdir bak
mv * bak
mv Centos-7.repo /etc/yum.repos.d/
mv epel-7.repo /etc/yum.repos.d/
yum clean all
yum makecache
yum repolist

yum install httpd
yum install net-tools

yum -y install openssh-clients
yum install openssh-server
#免密配置
systemctl status sshd

passwd root
ssh-keygen -t rsa

ssh-copy-id  -f -i ~/.ssh/id_rsa.pub cluster-master
ssh-copy-id  -f -i ~/.ssh/id_rsa.pub cluster-slave1
ssh-copy-id  -f -i ~/.ssh/id_rsa.pub cluster-slave2
ssh-copy-id  -f -i ~/.ssh/id_rsa.pub cluster-slave3

Pzszh@062

# 容器保存成镜像
docker commit -m &#39;提交文字说明&#39; -a &#39;作者&#39; 容器名 提交后的镜像名:提交后的镜像tag名
# 给需要推送的镜像打标签
docker tag 镜像id 要推入的仓库的用户名/要推入的仓库名:新定义的tag
# 推送镜像到仓库
docker push 要推入的仓库的用户名/要推入的仓库名:镜像标签
</code></pre>
<h2 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h2><pre><code class="shell">cd /opt
tar -xzvf zookeeper-3.4.10.tar.gz
mv zookeeper-3.4.10 /usr/local/zookeeper3.4.10
 
mkdir /usr/local/opt/zookeeper3.4.10/data        # 创建data目录
mkdir /usr/local/opt/zookeeper3.4.10/dataLog     # 创建dataLog目录

cd /opt/zookeeper3.4.10/data
vi myid     # 输入数字1，然后保存，第二个节点输入2，第三个节点输入3
 
cd /opt/zookeeper3.4.10/conf
cp zoo_sample.cfg zoo.cfg
vi zoo.cfg             #在文件末尾添加如下内容

dataDir=/opt/zookeeper3.4.10/data  
dataLogDir=/opt/zookeeper3.4.10/dataLog  
server.1=hadoop0:2888:3888  
server.2=hadoop1:2888:3888  
server.3=hadoop2:2888:3888  
# 注hadoop0，hadoop1，hadoop2为三个节点的主机名！

cd /usr/local
scp -r zookeeper3.4.10  root@hadoop1:/usr/local
scp -r zookeeper3.4.10  root@hadoop2:/usr/local

分别在三台服务器上运行如下命令
zkServer.sh start
</code></pre>
<h3 id="添加-Hadoop-用户"><a href="#添加-Hadoop-用户" class="headerlink" title="添加 Hadoop 用户"></a>添加 Hadoop 用户</h3><pre><code class="shell">useradd -m hadoop -s /bin/bash
passwd hadoop
# 添加 root权限
vi /etc/passwd 
# hadoop:X:0:1000::/home/hadoop:/bin/bash
su - hadoop
</code></pre>
<h2 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h2><pre><code class="shell">tar -zxf hadoop-3.1.3.tar.gz -C /usr/local/
cd /usr/local/
mv ./hadoop-3.1.3/ ./hadoop
chown -R hadoop ./hadoop

vi ~/.bashrc
export PATH=$PATH:/usr/local/hadoop/bin:/usr/local/hadoop/sbin
</code></pre>
<p>在配置集群&#x2F;分布式模式时，需要修改“&#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;etc&#x2F;hadoop”目录下的配置文件，这里仅设置正常启动所必须的设置项，包括workers 、core-site.xml、hdfs-site.xml、mapred-site.xml、yarn-site.xml共5个文件，更多设置项可查看官方说明。</p>
<pre><code>## workers
需要把所有数据节点的主机名写入该文件，每行一个，默认为 localhost（即把本机作为数据节点），在进行分布式配置时，可以保留localhost，让Master节点同时充当名称节点和数据节点，或者也可以删掉localhost这行，让Master节点仅作为名称节点使用。
localhost
cluster-master
cluster-slave1
cluster-slave2
</code></pre>
<pre><code class="xml"># core-site.xml
&lt;configuration&gt;
        &lt;property&gt;
                &lt;name&gt;fs.defaultFS&lt;/name&gt;
                &lt;value&gt;hdfs://cluster-master:9000&lt;/value&gt;
        &lt;/property&gt;
        &lt;property&gt;
                &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
                &lt;value&gt;file:/usr/local/hadoop/tmp&lt;/value&gt;
                &lt;description&gt;Abase for other temporary directories.&lt;/description&gt;
        &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>
<pre><code class="xml">## hdfs-site.xml
&lt;configuration&gt;
        &lt;property&gt;
                &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;
                &lt;value&gt;cluster-master:50090&lt;/value&gt;
        &lt;/property&gt;
        &lt;property&gt;
                &lt;name&gt;dfs.replication&lt;/name&gt;
                &lt;value&gt;13/value&gt;
        &lt;/property&gt;
        &lt;property&gt;
                &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
                &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/name&lt;/value&gt;
        &lt;/property&gt;
        &lt;property&gt;
                &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
                &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/data&lt;/value&gt;
        &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>
<pre><code class="xml">## mapred-site.xml
&lt;configuration&gt;
        &lt;property&gt;
                &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
                &lt;value&gt;yarn&lt;/value&gt;
        &lt;/property&gt;
        &lt;property&gt;
                &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;
                &lt;value&gt;cluster-master:10020&lt;/value&gt;
        &lt;/property&gt;
        &lt;property&gt;
                &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;
                &lt;value&gt;cluster-master:19888&lt;/value&gt;
        &lt;/property&gt;
        &lt;property&gt;
                &lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt;
                &lt;value&gt;HADOOP_MAPRED_HOME=/usr/local/hadoop&lt;/value&gt;
        &lt;/property&gt;
        &lt;property&gt;
                &lt;name&gt;mapreduce.map.env&lt;/name&gt;
                &lt;value&gt;HADOOP_MAPRED_HOME=/usr/local/hadoop&lt;/value&gt;
        &lt;/property&gt;
        &lt;property&gt;
                &lt;name&gt;mapreduce.reduce.env&lt;/name&gt;
                &lt;value&gt;HADOOP_MAPRED_HOME=/usr/local/hadoop&lt;/value&gt;
        &lt;/property&gt; 
&lt;/configuration&gt;
</code></pre>
<pre><code class="xml">## yarn-site.xml
&lt;configuration&gt;
        &lt;property&gt;
                &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;
                &lt;value&gt;cluster-master&lt;/value&gt;
        &lt;/property&gt;
        &lt;property&gt;
                &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
                &lt;value&gt;mapreduce_shuffle&lt;/value&gt;
        &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>
<pre><code class="shell"># master
cd /usr/local
rm -r ./hadoop/tmp # 删除 Hadoop 临时文件
rm -r ./hadoop/logs/* # 删除日志文件
tar -zcf ~/hadoop.master.tar.gz ./hadoop # 先压缩再复制
cd ~
scp ./hadoop.master.tar.gz Slave1:/home/hadoop
</code></pre>
<pre><code class="shell"># slave
rm -r /usr/local/hadoop # 删掉旧的（如果存在）
tar -zxf ~/hadoop.master.tar.gz -C /usr/local
chown -R hadoop /usr/local/hadoop
</code></pre>
<pre><code># 切换 hadoop用户
# 首次启动Hadoop集群时，需要先在Master节点执行名称节点的格式化（只需要执行这一次，后面再启动Hadoop时，不要再次格式化名称节点），命令如下：
hdfs namenode -format

# 启动Hadoop，启动需要在Master节点上进行，执行如下命令：
start-dfs.sh
start-yarn.sh 
mr-jobhistory-daemon.sh start historyserver
</code></pre>
<h2 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h2><pre><code class="shell">tar -zxf ~/下载/spark-2.4.0-bin-without-hadoop.tgz -C /usr/local/
mv ./spark-2.4.0-bin-without-hadoop/ ./spark
chown -R hadoop:hadoop ./spark

cd /usr/local/spark
cp ./conf/spark-env.sh.template ./conf/spark-env.sh

vi ./conf/spark-env.sh
export SPARK_DIST_CLASSPATH=$(/usr/local/hadoop/bin/hadoop classpath)
</code></pre>
<pre><code>SPARK_LOCAL_DIRS=/usr/local/spark/
HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop
YARN_CONF_DIR=//usr/local/hadoop/etc/hadoop
JAVA_HOME=/usr/local/java/jdk1.8.0_162
export SPARK_MASTER_IP=cluster-master
export SPARK_DAEMON_JAVA_OPTS=&quot;
-Dspark.deploy.recoveryMode=ZOOKEEPER
-Dspark.deploy.zookeeper.url=172.15.0.2:2181
-Dspark.deploy.zookeeper.dir=/sparkmaster&quot;
</code></pre>
<h3 id="slaves"><a href="#slaves" class="headerlink" title="slaves"></a>slaves</h3><pre><code>cluster-slave1 
cluster-slave2
</code></pre>
<h3 id="spark-default-conf"><a href="#spark-default-conf" class="headerlink" title="spark-default.conf"></a>spark-default.conf</h3><pre><code>spark.eventLog.enabled          true
spark.eventLog.dir              hdfs://jinbill/spark/eventLog
spark.history.fs.logDirectory   hdfs://jinbill/spark/eventLog
spark.eventLog.compress         true
</code></pre>
<h2 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h2><pre><code class="shell">tar -zxvf apache-hive-3.1.2-bin.tar.gz
mv apache-hive-3.1.2-bin /usr/local/hive

# 新建一个日志目录
mkdir /usr/local/hive/iotmp
</code></pre>
<h3 id="设置环境变量"><a href="#设置环境变量" class="headerlink" title="设置环境变量"></a>设置环境变量</h3><pre><code class="shell">vi ~/.bashrc

export HIVE_HOME=/usr/local/hive
export PATH=$HIVE_HOME/bin:$PATH

source ~/.bashrc
</code></pre>
<h3 id="设置Hive-HDFS文件夹"><a href="#设置Hive-HDFS文件夹" class="headerlink" title="设置Hive HDFS文件夹"></a>设置Hive HDFS文件夹</h3><h3 id="解决guava库问题"><a href="#解决guava库问题" class="headerlink" title="解决guava库问题"></a>解决guava库问题</h3><pre><code class="shell"># 查看hadoop下guava 版本
cd /usr/local/hadoop/share/hadoop/common/lib/
# guava-27.0-jre.jar

# 查看hive下guava 版本
cd /usr/local/hive/lib
# guava-19.0.jar
# 高版本替换低版本
</code></pre>
<h3 id="MySQL-中-建立hive数据库"><a href="#MySQL-中-建立hive数据库" class="headerlink" title="MySQL 中 建立hive数据库"></a>MySQL 中 建立hive数据库</h3><pre><code class="sql">CREATE DATABASE hive;
</code></pre>
<h3 id="jdbc依赖导入"><a href="#jdbc依赖导入" class="headerlink" title="jdbc依赖导入"></a>jdbc依赖导入</h3><h4 id="将hive的jline包替换到hadoop的yarn下"><a href="#将hive的jline包替换到hadoop的yarn下" class="headerlink" title="将hive的jline包替换到hadoop的yarn下"></a>将hive的jline包替换到hadoop的yarn下</h4><pre><code class="shell">`mv /opt/hive/apache-hive-3.1.2-bin/lib/jline-2.12.jar /opt/hadoop/hadoop-2.7.7/share/hadoop/yarn/`
# JDBC 依赖放入 /usr/local/hive/lib
mv mysql-connector-java-5.1.47.jar /usr/local/hive/lib/
</code></pre>
<h3 id="修改master节点配置文件"><a href="#修改master节点配置文件" class="headerlink" title="修改master节点配置文件"></a>修改master节点配置文件</h3><pre><code>#### VI编辑器替换命令
:%s/$&#123;system:java.io.tmpdir&#125;/\/opt\/hive\/iotmp/g  
:%s/$&#123;system:user.name&#125;/huan/g
</code></pre>
<h4 id="使用mysql替换默认的derby存放元数据"><a href="#使用mysql替换默认的derby存放元数据" class="headerlink" title="使用mysql替换默认的derby存放元数据"></a>使用mysql替换默认的derby存放元数据</h4><pre><code class="xml">&lt;!--元数据库修改为MySQL--&gt;
&lt;property&gt;
    &lt;name&gt;hive.metastore.db.type&lt;/name&gt;
    &lt;value&gt;mysql&lt;/value&gt;
    &lt;description&gt;
      Expects one of [derby, oracle, mysql, mssql, postgres].
      Type of database used by the metastore. Information schema &amp;amp; JDBCStorageHandler depend on it.
    &lt;/description&gt;
&lt;/property&gt;
&lt;!--MySQL 驱动--&gt;
&lt;property&gt;
    &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;
    &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;
    &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;
&lt;/property&gt;
&lt;!--MySQL URL--&gt;
&lt;property&gt;
    &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;
    &lt;value&gt;jdbc:mysql://10.20.89.80:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt;
    &lt;description&gt;
      JDBC connect string for a JDBC metastore.
      To use SSL to encrypt/authenticate the connection, provide database-specific SSL flag in the connection URL.
      For example, jdbc:postgresql://myhost/db?ssl=true for postgres database.
    &lt;/description&gt;
&lt;/property&gt;
&lt;!--MySQL 用户名--&gt;
&lt;property&gt;
  &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;
  &lt;value&gt;root&lt;/value&gt;
  &lt;description&gt;Username to use against metastore database&lt;/description&gt;
&lt;/property&gt;
&lt;!--MySQL 密码--&gt;
&lt;property&gt;
    &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;
    &lt;value&gt;123456&lt;/value&gt;
    &lt;description&gt;password to use against metastore database&lt;/description&gt;
&lt;/property&gt;
</code></pre>
<h4 id="设置解析引擎为spark"><a href="#设置解析引擎为spark" class="headerlink" title="设置解析引擎为spark"></a>设置解析引擎为spark</h4><pre><code class="xml">&lt;property&gt;
    &lt;name&gt;hive.execution.engine&lt;/name&gt;
    &lt;value&gt;spark&lt;/value&gt;
    &lt;description&gt;
      Expects one of [mr, tez, spark].
      Chooses execution engine. Options are: mr (Map reduce, default), tez, spark. While MR
      remains the default engine for historical reasons, it is itself a historical engine
      and is deprecated in Hive 2 line. It may be removed without further warning.
    &lt;/description&gt;
&lt;/property&gt;
</code></pre>
<h4 id="自动初始化元数据"><a href="#自动初始化元数据" class="headerlink" title="自动初始化元数据"></a>自动初始化元数据</h4><pre><code class="xml">&lt;property&gt;
    &lt;name&gt;datanucleus.schema.autoCreateAll&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
    &lt;description&gt;Auto creates necessary schema on a startup if one doesn&#39;t exist. Set this to false, after creating it once.To enable auto create also set hive.metastore.schema.verification=false. Auto creation is not recommended for production use cases, run schematool command instead.
    &lt;/description&gt;
&lt;/property&gt;
</code></pre>
<h4 id="关闭校验"><a href="#关闭校验" class="headerlink" title="关闭校验"></a>关闭校验</h4><pre><code class="xml">&lt;!--听说是JDK版本使用1.8的问题。。--&gt;
&lt;property&gt;
   &lt;name&gt;hive.metastore.schema.verification&lt;/name&gt;
   &lt;value&gt;false&lt;/value&gt;
   &lt;description&gt;
     Enforce metastore schema version consistency.
     True: Verify that version information stored in is compatible with one from Hive jars.  Also disable automatic
           schema migration attempt. Users are required to manually migrate schema after Hive upgrade which ensures
           proper metastore schema migration. (Default)
     False: Warn if the version information stored in metastore doesn&#39;t match with one from in Hive jars.
   &lt;/description&gt;
&lt;/property&gt;
&lt;property&gt;
   &lt;name&gt;hive.conf.validation&lt;/name&gt;
   &lt;value&gt;false&lt;/value&gt;
   &lt;description&gt;Enables type checking for registered Hive configurations&lt;/description&gt;
 &lt;/property&gt;
</code></pre>
<h4 id="删除-description-中的-8，这个解析会报错"><a href="#删除-description-中的-8，这个解析会报错" class="headerlink" title="删除 description 中的 &amp;#8，这个解析会报错"></a>删除 description 中的 <code>&amp;#8</code>，这个解析会报错</h4><pre><code class="xml">&lt;property&gt;
   &lt;name&gt;hive.txn.xlock.iow&lt;/name&gt;
   &lt;value&gt;true&lt;/value&gt;
   &lt;description&gt;
     Ensures commands with OVERWRITE (such as INSERT OVERWRITE) acquire Exclusive locks for&amp;#8;transactional tables.  This ensures that inserts (w/o overwrite) running concurrently
     are not hidden by the INSERT OVERWRITE.
   &lt;/description&gt;
&lt;/property&gt;
</code></pre>
<h2 id="Spark任务提交"><a href="#Spark任务提交" class="headerlink" title="Spark任务提交"></a>Spark任务提交</h2><p>打包python依赖，镜像<br>进入到虚拟环境下，如&#x2F;home&#x2F;hadoop&#x2F;anaconda3&#x2F;envs，使用以下命令将虚拟环境进行打包：</p>
<pre><code class="shell">zip -r conda_env.zip conda_env # 虚拟环境为conda_env, 打包为conda_env.zip 文件
</code></pre>
<h4 id="spark-submit参数设置"><a href="#spark-submit参数设置" class="headerlink" title="spark-submit参数设置"></a>spark-submit参数设置</h4><pre><code class="python">from pyspark.sql import SparkSession

spark_conf = SparkConf().loadPropertiesFile(&quot;spark_config.properties&quot;)

spark = SparkSession.builder \
    .appName(spark_conf.get(&quot;spark.app.name&quot;)) \
    .master(spark_conf.get(&quot;spark.master&quot;)) \
    .config(&quot;spark.executor.memory&quot;, spark_conf.get(&quot;spark.executor.memory&quot;)) \
    .config(&quot;spark.driver.memory&quot;, spark_conf.get(&quot;spark.driver.memory&quot;)) \
    .getOrCreate()

# 使用SparkSession进行数据处理和分析
</code></pre>
<pre><code class="bash">$&#123;SPARK_PATH&#125;/bin/spark-submit \
 --master yarn \
 --name &quot;spark_demo_lr&quot; \
 --queue $&#123;YARN_QUEUE&#125; \
 --deploy-mode $&#123;DEPLOY_MODE&#125; \
 --driver-memory 6g \
 --driver-cores 4 \
 --executor-memory 12g \
 --executor-cores 15 \
 --num-executors 10 \
 --archives ./source/py27.zip#python_env \
 --conf spark.default.parallelism=150 \
 --conf spark.executor.memoryOverhead=4g \
 --conf spark.driver.memoryOverhead=2g \
 --conf spark.yarn.maxAppAttempts=3 \
 --conf spark.yarn.submit.waitAppCompletion=true \
 --conf spark.pyspark.driver.python=./source/py27/bin/python2 \
 --conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=./python_env/py27/bin/python2 \
 --conf spark.pyspark.python=./python_env/py27/bin/python2 \
 ./$&#123;ModelType&#125;.py $input_path_train $input_path_test $output_path
</code></pre>
<h4 id="pyspark-传入配置文件参数"><a href="#pyspark-传入配置文件参数" class="headerlink" title="pyspark 传入配置文件参数"></a>pyspark 传入配置文件参数</h4><p>首先，可以使用 <code>--files</code> 参数将配置文件传递给 <code>spark-submit</code> 命令。这将确保配置文件在集群中的每个节点上都可用。以下是一个示例：</p>
<pre><code class="shell">spark-submit --master yarn --deploy-mode cluster --files /home/sys_user/ask/conf/config.ini test.py
</code></pre>
<p>在 PySpark 脚本中，可以使用 <code>SparkFiles.get()</code> 方法来读取传入的配置文件</p>
<pre><code class="python">from pyspark import SparkFiles

with open(SparkFiles.get(&#39;config.ini&#39;)) as config_file:
    print(config_file.read())
</code></pre>
<pre><code class="python">from pyspark.sql import SparkSession

# 创建 SparkSession
spark = SparkSession.builder.appName(&quot;ConfigExample&quot;).getOrCreate()

# 加载配置参数
with open(&quot;config.properties&quot;, &quot;r&quot;) as f:
    for line in f:
        line = line.strip()
        if line and not line.startswith(&quot;#&quot;):
            key, value = line.split(&quot;=&quot;)
            spark.conf.set(key, value)

# 打印配置参数
print(&quot;spark.sql.shuffle.partitions:&quot;, spark.conf.get(&quot;spark.sql.shuffle.partitions&quot;))
print(&quot;spark.sql.autoBroadcastJoinThreshold:&quot;, spark.conf.get(&quot;spark.sql.autoBroadcastJoinThreshold&quot;))
</code></pre>
<pre><code class="shell">./spark-submit --master yarn --deploy-mode cluster --files /opt/bigdata/config_batch.properties --executor-memory 2g --executor-cores 1 --num-executors 5 --driver-memory 2G  --py-files /usr/local/miniconda3/envs/spark_env.zip --jars /usr/local/spark/jars/mysql-connector-java-5.1.47.jar,/usr/local/spark/jars/taos-jdbcdriver-2.0.34.jar,/usr/local/spark/jars/spark-redis-2.4.0-jar-with-dependencies.jar,/usr/local/spark/jars/fastjson-1.2.73.jar --conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=/usr/local/miniconda3/envs/spark_env/bin/python --conf spark.pyspark.python=/usr/local/miniconda3/envs/spark_env/bin/python /opt/bigdata/HealthScoreHFL2.py
</code></pre>
<p>        .master(“local”)<br>        .enableHiveSupport()<br>        .config(‘spark.executor.extraClassPath’, “&#x2F;usr&#x2F;local&#x2F;spark&#x2F;jars&#x2F;mysql-connector-java-5.1.47.jar”)\</p>
<p>        .config(‘spark.driver.extraClassPath’, “&#x2F;usr&#x2F;local&#x2F;spark&#x2F;jars&#x2F;mysql-connector-java-5.1.47.jar”)\</p>
<p>        .config(“spark.jars”, “&#x2F;usr&#x2F;local&#x2F;spark&#x2F;jars&#x2F;mysql-connector-java-5.1.47.jar,&#x2F;usr&#x2F;local&#x2F;spark&#x2F;jars&#x2F;taos-jdbcdriver-2.0.34.jar,&#x2F;usr&#x2F;local&#x2F;spark&#x2F;jars&#x2F;spark-redis-2.4.0-jar-with-dependencies.jar,&#x2F;usr&#x2F;local&#x2F;spark&#x2F;jars&#x2F;fastjson-1.2.73.jar”)\</p>
<pre><code class="shell">./spark-submit --master yarn --deploy-mode cluster --files /opt/bigdata/batch/config_batch.properties --executor-memory 2g --executor-cores 1 --num-executors 5 --driver-memory 2G  --py-files /opt/bigdata/batch/spark3.1_env.zip --jars /usr/local/spark-yarn/jars/mysql-connector-java-5.1.47.jar,/usr/local/spark-yarn/jars/taos-jdbcdriver-2.0.34.jar,/usr/local/spark-yarn/jars/spark-redis-2.4.0-jar-with-dependencies.jar,/usr/local/spark-yarn/jars/fastjson-1.2.73.jar --conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=/usr/local/miniconda3/bin/python --conf spark.pyspark.python=/usr/local/miniconda3/bin/python /opt/bigdata/batch/HealthScoreHFL2.py
</code></pre>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="auhjin.github.io">auhjin_ai8</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章連結: </span><span class="post-copyright-info"><a href="http://auhjin.github.io/2024/01/08/Docker%20%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2Hadoop%EF%BC%8CSpark%EF%BC%8CZookeeper%EF%BC%8CHive%E9%9B%86%E7%BE%A4/">http://auhjin.github.io/2024/01/08/Docker%20%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2Hadoop%EF%BC%8CSpark%EF%BC%8CZookeeper%EF%BC%8CHive%E9%9B%86%E7%BE%A4/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版權聲明: </span><span class="post-copyright-info">本部落格所有文章除特別聲明外，均採用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 許可協議。轉載請註明來自 <a href="http://auhjin.github.io" target="_blank">飞驰的细狗</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/bigdata/">bigdata</a><a class="post-meta__tags" href="/tags/docker/">docker</a></div><div class="post_share"><div class="social-share" data-image="/img/logo.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2024/01/03/hello-world/" title="Hello World"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Hello World</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/logo.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">auhjin_ai8</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">2</div></a><a href="/tags/"><div class="headline">標籤</div><div class="length-num">2</div></a><a href="/categories/"><div class="headline">分類</div><div class="length-num">2</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/auhjin"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/auhjin" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:auhjin.ai8@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目錄</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Mysql%E3%80%81Redis%E3%80%81Tdengine-%E7%9C%81%E7%95%A5%E2%80%A6-%E7%9B%B4%E6%8E%A5docker%E5%AE%B9%E5%99%A8%E6%8B%89%E5%8F%96"><span class="toc-number">1.</span> <span class="toc-text">Mysql、Redis、Tdengine 省略… 直接docker容器拉取</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8B%89%E5%8F%96%E5%90%84%E8%8A%82%E7%82%B9centos%E7%B3%BB%E7%BB%9F%E9%95%9C%E5%83%8F"><span class="toc-number">2.</span> <span class="toc-text">拉取各节点centos系统镜像</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Zookeeper"><span class="toc-number">3.</span> <span class="toc-text">Zookeeper</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B7%BB%E5%8A%A0-Hadoop-%E7%94%A8%E6%88%B7"><span class="toc-number">3.1.</span> <span class="toc-text">添加 Hadoop 用户</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hadoop"><span class="toc-number">4.</span> <span class="toc-text">Hadoop</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark"><span class="toc-number">5.</span> <span class="toc-text">Spark</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#slaves"><span class="toc-number">5.1.</span> <span class="toc-text">slaves</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#spark-default-conf"><span class="toc-number">5.2.</span> <span class="toc-text">spark-default.conf</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hive"><span class="toc-number">6.</span> <span class="toc-text">Hive</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%BE%E7%BD%AE%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F"><span class="toc-number">6.1.</span> <span class="toc-text">设置环境变量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%BE%E7%BD%AEHive-HDFS%E6%96%87%E4%BB%B6%E5%A4%B9"><span class="toc-number">6.2.</span> <span class="toc-text">设置Hive HDFS文件夹</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%A3%E5%86%B3guava%E5%BA%93%E9%97%AE%E9%A2%98"><span class="toc-number">6.3.</span> <span class="toc-text">解决guava库问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MySQL-%E4%B8%AD-%E5%BB%BA%E7%AB%8Bhive%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-number">6.4.</span> <span class="toc-text">MySQL 中 建立hive数据库</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#jdbc%E4%BE%9D%E8%B5%96%E5%AF%BC%E5%85%A5"><span class="toc-number">6.5.</span> <span class="toc-text">jdbc依赖导入</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B0%86hive%E7%9A%84jline%E5%8C%85%E6%9B%BF%E6%8D%A2%E5%88%B0hadoop%E7%9A%84yarn%E4%B8%8B"><span class="toc-number">6.5.1.</span> <span class="toc-text">将hive的jline包替换到hadoop的yarn下</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BF%AE%E6%94%B9master%E8%8A%82%E7%82%B9%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="toc-number">6.6.</span> <span class="toc-text">修改master节点配置文件</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8mysql%E6%9B%BF%E6%8D%A2%E9%BB%98%E8%AE%A4%E7%9A%84derby%E5%AD%98%E6%94%BE%E5%85%83%E6%95%B0%E6%8D%AE"><span class="toc-number">6.6.1.</span> <span class="toc-text">使用mysql替换默认的derby存放元数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%BE%E7%BD%AE%E8%A7%A3%E6%9E%90%E5%BC%95%E6%93%8E%E4%B8%BAspark"><span class="toc-number">6.6.2.</span> <span class="toc-text">设置解析引擎为spark</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E5%88%9D%E5%A7%8B%E5%8C%96%E5%85%83%E6%95%B0%E6%8D%AE"><span class="toc-number">6.6.3.</span> <span class="toc-text">自动初始化元数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B3%E9%97%AD%E6%A0%A1%E9%AA%8C"><span class="toc-number">6.6.4.</span> <span class="toc-text">关闭校验</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%A0%E9%99%A4-description-%E4%B8%AD%E7%9A%84-8%EF%BC%8C%E8%BF%99%E4%B8%AA%E8%A7%A3%E6%9E%90%E4%BC%9A%E6%8A%A5%E9%94%99"><span class="toc-number">6.6.5.</span> <span class="toc-text">删除 description 中的 &amp;#8，这个解析会报错</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark%E4%BB%BB%E5%8A%A1%E6%8F%90%E4%BA%A4"><span class="toc-number">7.</span> <span class="toc-text">Spark任务提交</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#spark-submit%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE"><span class="toc-number">7.0.1.</span> <span class="toc-text">spark-submit参数设置</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#pyspark-%E4%BC%A0%E5%85%A5%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E5%8F%82%E6%95%B0"><span class="toc-number">7.0.2.</span> <span class="toc-text">pyspark 传入配置文件参数</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/01/08/Docker%20%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2Hadoop%EF%BC%8CSpark%EF%BC%8CZookeeper%EF%BC%8CHive%E9%9B%86%E7%BE%A4/" title="Docker 本地部署Hadoop、Spark、Zookeeper、Hive集群">Docker 本地部署Hadoop、Spark、Zookeeper、Hive集群</a><time datetime="2024-01-08T08:30:00.000Z" title="發表於 2024-01-08 16:30:00">2024-01-08</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/01/03/hello-world/" title="Hello World">Hello World</a><time datetime="2024-01-03T14:16:45.838Z" title="發表於 2024-01-03 22:16:45">2024-01-03</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2024 By auhjin_ai8</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主題 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="閱讀模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="淺色和深色模式轉換"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="單欄和雙欄切換"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="設定"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目錄"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="返回頂部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/fireworks.min.js"></script><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-fluttering-ribbon.min.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>